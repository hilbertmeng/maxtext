{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3864c933-93cd-4fe7-b35a-fd2fedfed681",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-18 07:50:18.994389: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:479] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-12-18 07:50:19.007909: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:10575] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-12-18 07:50:19.007932: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1442] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-18 07:50:19.656298: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating keys from env and command line: []\n",
      "Running Model: default\n",
      "Updating keys from model: []\n",
      "System Information: Jax Version: 0.4.35\n",
      "System Information: Jaxlib Version: 0.4.35\n",
      "System Information: Jax Backend: PJRT C API\n",
      "TFRT TPU v6 lite\n",
      "Built on Oct 21 2024 00:24:02 (1729495442) cl/687888698\n",
      "Not using emergency checkpoint, ignoring local_checkpoint_directory and local_checkpoint_period\n",
      "Config param activations_in_float32: False\n",
      "Config param adam_b1: 0.9\n",
      "Config param adam_b2: 0.95\n",
      "Config param adam_eps: 1e-08\n",
      "Config param adam_eps_root: 0.0\n",
      "Config param adam_weight_decay: 0.1\n",
      "Config param allow_split_physical_axes: False\n",
      "Config param ar_cache_axis_order: 1,2,0,3\n",
      "Config param async_checkpointing: True\n",
      "Config param attention: dot_product\n",
      "Config param autoregressive_decode_assert: \n",
      "Config param aux_loss_coef: 0.0\n",
      "Config param base_emb_dim: 4096\n",
      "Config param base_mlp_dim: 5632\n",
      "Config param base_num_decoder_layers: 48\n",
      "Config param base_num_kv_heads: 32\n",
      "Config param base_num_query_heads: 32\n",
      "Config param base_output_directory: \n",
      "Config param checkpoint_dir: ./checkpoints\n",
      "Config param checkpoint_period: 250\n",
      "Config param collect_stack_trace: False\n",
      "Config param compile_topology: \n",
      "Config param compile_topology_num_slices: -1\n",
      "Config param compiled_trainstep_file: \n",
      "Config param compute_axis_order: 0,1,2,3\n",
      "Config param cosine_learning_rate_final_fraction: 0.1\n",
      "Config param data_sharding: (('data', 'stage', 'fsdp', 'fsdp_transpose', 'sequence', 'tensor', 'expert', 'autoregressive'),)\n",
      "Config param data_shuffle_seed: 9876\n",
      "Config param dataset_path: \n",
      "Config param dataset_type: pretrain_4k\n",
      "Config param dcn_autoregressive_parallelism: 1\n",
      "Config param dcn_data_parallelism: -1\n",
      "Config param dcn_expert_parallelism: 1\n",
      "Config param dcn_fsdp_parallelism: 1\n",
      "Config param dcn_fsdp_transpose_parallelism: 1\n",
      "Config param dcn_pipeline_parallelism: 1\n",
      "Config param dcn_sequence_parallelism: 1\n",
      "Config param dcn_tensor_parallelism: 1\n",
      "Config param decode_sampling_nucleus_p: -1\n",
      "Config param decode_sampling_strategy: greedy\n",
      "Config param decode_sampling_temperature: 1.0\n",
      "Config param decode_sampling_top_k: 0\n",
      "Config param decoder_block: dcformer\n",
      "Config param dropout_rate: 0\n",
      "Config param dtype: bfloat16\n",
      "Config param emb_dim: 4096\n",
      "Config param enable_background_delete: True\n",
      "Config param enable_checkpoint_cloud_logger: False\n",
      "Config param enable_checkpoint_standard_logger: False\n",
      "Config param enable_checkpointing: False\n",
      "Config param enable_data_shuffling: False\n",
      "Config param enable_dropout: False\n",
      "Config param enable_emergency_checkpoint: False\n",
      "Config param enable_goodput_recording: False\n",
      "Config param enable_jax_profiler: False\n",
      "Config param enable_single_controller: False\n",
      "Config param enable_single_replica_ckpt_restoring: False\n",
      "Config param epoch: 1\n",
      "Config param eval_interval: 500\n",
      "Config param eval_loop_num_batches: 82\n",
      "Config param eval_per_device_batch_size: 1.0\n",
      "Config param eval_shuffle_buffer_size: None\n",
      "Config param eval_split: valid\n",
      "Config param eval_start_step: False\n",
      "Config param expansion_factor_real_data: -1\n",
      "Config param expert_capacity_factor: 1.375\n",
      "Config param expert_chunk_size: 1\n",
      "Config param force_unroll: False\n",
      "Config param fused_mlp: False\n",
      "Config param fused_qkv: False\n",
      "Config param gate_noise_coef: 1.0\n",
      "Config param gcs_metrics: False\n",
      "Config param global_batch_size_to_load: 4\n",
      "Config param global_batch_size_to_train_on: 4\n",
      "Config param global_parameter_scale: 1\n",
      "Config param gradient_accumulation_steps: 1\n",
      "Config param gradient_clipping_threshold: 1.0\n",
      "Config param grain_worker_count: 4\n",
      "Config param hardware: tpu\n",
      "Config param head_dim: 128\n",
      "Config param ici_autoregressive_parallelism: 1\n",
      "Config param ici_data_parallelism: 1\n",
      "Config param ici_expert_parallelism: 1\n",
      "Config param ici_fsdp_parallelism: -1\n",
      "Config param ici_fsdp_transpose_parallelism: 1\n",
      "Config param ici_pipeline_parallelism: 1\n",
      "Config param ici_sequence_parallelism: 1\n",
      "Config param ici_tensor_parallelism: 1\n",
      "Config param inference_metadata_file: \n",
      "Config param inference_microbenchmark_log_file_path: \n",
      "Config param inference_microbenchmark_loop_iters: 10\n",
      "Config param inference_microbenchmark_prefill_lengths: 64,128,256,512,1024\n",
      "Config param inference_microbenchmark_stages: prefill,generate\n",
      "Config param init_weights_seed: 9876\n",
      "Config param insert_moe_divisor: 1\n",
      "Config param intermediate_dropout_rate: 0\n",
      "Config param iter_file_nums: 20\n",
      "Config param jax_cache_dir: ~/jax_cache\n",
      "Config param jax_profiler_port: 9999\n",
      "Config param keep_period: 1000\n",
      "Config param key_wise: True\n",
      "Config param kv_quant_axis: heads_and_dkv\n",
      "Config param learning_rate: 0.0001\n",
      "Config param learning_rate_schedule_steps: 200000\n",
      "Config param load_from_prefill_dir: False\n",
      "Config param load_full_state_path: \n",
      "Config param load_ocdbt: True\n",
      "Config param load_parameters_path: \n",
      "Config param log_period: 10\n",
      "Config param logical_axis_rules: (('activation_batch', ('data', 'fsdp', 'fsdp_transpose', 'expert')), ('activation_batch_no_exp', ('data', 'fsdp', 'fsdp_transpose')), ('activation_embed_and_logits_batch', ('data', 'stage', 'fsdp', 'fsdp_transpose', 'expert')), ('activation_heads', ('tensor', 'sequence')), ('activation_kv_heads', ('tensor', 'sequence')), ('activation_length', 'sequence'), ('activation_embed', 'tensor'), ('activation_mlp', 'tensor'), ('activation_kv', 'tensor'), ('activation_kv_batch', ('data', 'fsdp', 'fsdp_transpose', 'expert')), ('activation_kv_head_dim', 'tensor'), ('activation_vocab', ('tensor', 'sequence')), ('activation_vocab', 'tensor'), ('activation_vocab', 'sequence'), ('activation_stage', 'stage'), ('activation_exp', 'expert'), ('mlp', ('fsdp_transpose', 'tensor', 'autoregressive')), ('vocab', ('tensor', 'autoregressive')), ('embed', ('fsdp', 'fsdp_transpose', 'sequence', 'expert')), ('embed', ('fsdp', 'sequence', 'expert')), ('embed_no_exp', ('fsdp', 'fsdp_transpose', 'sequence')), ('embed_no_exp', ('fsdp', 'sequence')), ('norm', 'tensor'), ('heads', ('tensor', 'autoregressive')), ('layers', 'stage'), ('kv', ()), ('kv_heads', ('tensor', 'autoregressive')), ('kv_head_dim', ()), ('cache_batch', ()), ('cache_heads', ('autoregressive', 'tensor')), ('cache_kv', ()), ('cache_sequence', ()), ('exp', 'expert'))\n",
      "Config param logits_dot_in_fp32: False\n",
      "Config param logits_via_embedding: False\n",
      "Config param matmul_precision: default\n",
      "Config param max_checkify: False\n",
      "Config param max_corpus_chars: 10000000\n",
      "Config param max_prefill_predict_length: 64\n",
      "Config param max_target_length: 4097\n",
      "Config param max_to_keep: 4\n",
      "Config param megablox: True\n",
      "Config param megablox_chunks: 1\n",
      "Config param mesh_axes: ['data', 'stage', 'fsdp', 'fsdp_transpose', 'sequence', 'tensor', 'expert', 'autoregressive']\n",
      "Config param metrics_dir: ./metrics\n",
      "Config param metrics_file: \n",
      "Config param mgate: True\n",
      "Config param mgate_dim: 44\n",
      "Config param min_group_size: 1\n",
      "Config param mlp_activations: ['silu', 'linear']\n",
      "Config param mlp_dim: 5632\n",
      "Config param mlp_residual_dropout_rate: 0\n",
      "Config param model_name: default\n",
      "Config param moe_type: unmistral\n",
      "Config param n_shared_experts: 0\n",
      "Config param noam_down_end_lr: 3e-5\n",
      "Config param noam_down_lr_step: 10000\n",
      "Config param noam_transition_steps: 1500\n",
      "Config param noam_warmup_steps: 100\n",
      "Config param normalization_layer_epsilon: 1e-05\n",
      "Config param normalize_embedding_logits: True\n",
      "Config param num_decoder_layers: 48\n",
      "Config param num_experts: 8\n",
      "Config param num_experts_per_tok: 2\n",
      "Config param num_groups: 4\n",
      "Config param num_kv_heads: 32\n",
      "Config param num_layers_per_block: 4\n",
      "Config param num_query_heads: 32\n",
      "Config param num_slices: 1\n",
      "Config param only_eval: False\n",
      "Config param opt_type: adam_pax\n",
      "Config param param_scan_axis: 1\n",
      "Config param per_device_batch_size: 1.0\n",
      "Config param post_compose: True\n",
      "Config param pre_compose: True\n",
      "Config param prefill_cache_axis_order: 1,2,0,3\n",
      "Config param prefill_cache_dir: \n",
      "Config param profiler: \n",
      "Config param profiler_steps: 5\n",
      "Config param prometheus_port: 0\n",
      "Config param prompt: I love to\n",
      "Config param qk_norm: True\n",
      "Config param quantization: \n",
      "Config param quantization_local_shard_count: 1\n",
      "Config param quantize_kvcache: False\n",
      "Config param query_chunk_size: None\n",
      "Config param query_wise: True\n",
      "Config param record_internal_nn_metrics: 1\n",
      "Config param remat_policy: full\n",
      "Config param reset_for_eval: True\n",
      "Config param reshape_q: False\n",
      "Config param reuse_example_batch: 0\n",
      "Config param rope_max_timescale: 10000\n",
      "Config param rope_min_timescale: 1\n",
      "Config param router_z_loss_coef: 0.0\n",
      "Config param run_name: ./\n",
      "Config param save_config_to_gcs: False\n",
      "Config param save_ocdbt: True\n",
      "Config param scan_layers: True\n",
      "Config param set_mask_by_eos: False\n",
      "Config param sfm_after_topn: True\n",
      "Config param shared_mlp_scale: 1\n",
      "Config param skip_first_n_steps_for_profiler: 1\n",
      "Config param stack_trace_interval_seconds: 600\n",
      "Config param stack_trace_to_cloud: False\n",
      "Config param static_proj: False\n",
      "Config param steps: 10000000\n",
      "Config param target_eval_loss: 0.0\n",
      "Config param task_features: ['input_ids']\n",
      "Config param tensorboard_dir: gs://llm_base_models_us-east5/maxtext_tensorboard_dir/.\n",
      "Config param tokenizer_path: assets/tokenizer.llama2\n",
      "Config param train_shuffle_buffer_size: None\n",
      "Config param trainable_position_size: -1\n",
      "Config param training_num_batches_to_skip: None\n",
      "Config param unshared_mlp_dropout_rate: 0\n",
      "Config param unshared_mlp_scale: 1\n",
      "Config param upload_all_profiler_results: False\n",
      "Config param use_iota_embed: False\n",
      "Config param use_noam_schedule: False\n",
      "Config param use_untrainable_positional_embedding: False\n",
      "Config param use_vertex_tensorboard: False\n",
      "Config param using_pipeline_parallelism: False\n",
      "Config param vertex_tensorboard_project: \n",
      "Config param vertex_tensorboard_region: \n",
      "Config param vocab_size: 152064\n",
      "Config param warmup_steps_fraction: 0.0005\n",
      "Config param weight_dtype: float32\n",
      "Config param window_size: [256, 4096, 256, 256]\n",
      "Config param zero_loss: True\n",
      "Mesh: [[[[[[[[TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)]]]]]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "   [[[[[TpuDevice(id=1, process_index=0, coords=(1,0,0), core_on_chip=0)]]]]]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "   [[[[[TpuDevice(id=2, process_index=0, coords=(0,1,0), core_on_chip=0)]]]]]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "   [[[[[TpuDevice(id=3, process_index=0, coords=(1,1,0), core_on_chip=0)]]]]]]]] Num_devices: 4, shape (1, 1, 4, 1, 1, 1, 1, 1) \n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "    \n",
    "import os\n",
    "import sys\n",
    "import yaml\n",
    "import json\n",
    "import base64\n",
    "from collections import defaultdict\n",
    "from typing import Tuple\n",
    "import functools\n",
    "\n",
    "sys.path.append('/home/lishengping/projects/maxtext/MaxText')\n",
    "os.environ['HARDWARE'] = 'tpu'\n",
    "\n",
    "from layers import models\n",
    "import max_utils\n",
    "import jax\n",
    "import orbax\n",
    "import jax.numpy as jnp\n",
    "from jax.sharding import Mesh\n",
    "from flax.traverse_util import flatten_dict, unflatten_dict\n",
    "from flax import linen as nn\n",
    "from transformers import AutoTokenizer\n",
    "from etils import epath\n",
    "\n",
    "import pyconfig\n",
    "from jax.sharding import PartitionSpec\n",
    "from flax.linen import partitioning as nn_partitioning\n",
    "\n",
    "\n",
    "TOKENIZER_PATH = '/home/lishengping/tokenizer'\n",
    "if not os.path.exists(TOKENIZER_PATH):\n",
    "    !gsutil cp -r gs://llm_base_models_us-east5/qwen/tokenizer /home/lishengping/\n",
    "tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_PATH, use_fast=True, trust_remote_code=True)\n",
    "\n",
    "read_dir = \"gs://llm_base_models_europe-west4/v5p_256/7B/xm_E8T2S0x7B_WithMgate_openmoe_RouterR_1218/checkpoints/\"\n",
    "read_dir = epath.Path(read_dir)\n",
    "\n",
    "# config_name = '/home/lishengping/projects/maxtext/MaxText/configs/dcformer_pp_405m.yml'\n",
    "config_name = '/home/lishengping/projects/maxtext/MaxText/configs/dc_forward.yml'\n",
    "\n",
    "argv = [None, config_name]\n",
    "pyconfig.initialize(argv)\n",
    "config = pyconfig.config\n",
    "# validate_train_config(config)\n",
    "devices_array = max_utils.create_device_mesh(config)\n",
    "mesh = Mesh(devices_array, config.mesh_axes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "305f45e1-63a3-4a4c-b678-17222d18df14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_base64(encoded_str):\n",
    "    decoded_bytes = base64.b64decode(encoded_str)\n",
    "    decoded_str = decoded_bytes.decode('utf-8')\n",
    "    return decoded_str\n",
    "\n",
    "\n",
    "def mesh_shard_rules(mesh, rules, remove_keys=[]):\n",
    "    _sharding_dict = {}\n",
    "    for name, rule in rules.items():\n",
    "        if isinstance(rule, str):\n",
    "            rule = json.loads(rule)\n",
    "        name = decode_base64(name)\n",
    "        param_key = tuple(name.split('.'))\n",
    "        remove = any([1 if key in param_key else 0 for key in remove_keys])\n",
    "        if remove: continue\n",
    "        prule = [tuple(r) if isinstance(r, list) else r for r in rule['partition_spec'] ]\n",
    "        spec = jax.sharding.PartitionSpec(*prule)\n",
    "        _sharding_dict[param_key] = jax.sharding.NamedSharding(mesh, spec)\n",
    "    return _sharding_dict\n",
    "\n",
    "\n",
    "def rewrite_bucket_sharding(mesh, old_sharding, save_path):\n",
    "    cur_machine_sharding = {}\n",
    "    for k, v in old_sharding.items():\n",
    "        if isinstance(v, str):\n",
    "            v = json.loads(v)\n",
    "        v['shape'] = mesh.device_ids.shape\n",
    "        cur_machine_sharding[k] = v\n",
    "    save_path = epath.Path(save_path)\n",
    "    with save_path.open('w') as f:\n",
    "        json.dump(cur_machine_sharding, f)\n",
    "    \n",
    "load_step = 0\n",
    "_sharding_path = epath.Path('gs://llm_base_models_europe-west4/v5p_256/7B/xm3p5_7b_with_perlayer_moe_sharding')\n",
    "_metadata_path = read_dir / str(load_step) / 'state/_METADATA'\n",
    "\n",
    "_metadata_path = read_dir / str(load_step) / 'state/_METADATA'\n",
    "\n",
    "# delete file or dir\n",
    "# _sharding_path.unlink()\n",
    "\n",
    "remove_keys = ['opt_state', 'step']\n",
    "if _sharding_path.exists():\n",
    "    with _sharding_path.open('r') as f:\n",
    "        _sharding_rules = json.load(f)\n",
    "    # 重写_sharding文件\n",
    "    rewrite_bucket_sharding(mesh, _sharding_rules, _sharding_path)\n",
    "    _sharding_dict = mesh_shard_rules(mesh, _sharding_rules, remove_keys=remove_keys)\n",
    "    _sharding_dict = unflatten_dict(_sharding_dict)\n",
    "elif _metadata_path.exists():\n",
    "    _metadata_dict = {}\n",
    "    with _metadata_path.open('r') as f:\n",
    "        _metadata = json.load(f)\n",
    "    for param_key in _metadata['tree_metadata']:\n",
    "        if isinstance(param_key, str): param_key = eval(param_key)\n",
    "        remove = any([1 if key in param_key else 0 for key in remove_keys])\n",
    "        if remove: continue\n",
    "        _metadata_dict[param_key] = jax.ShapeDtypeStruct(shape=(), dtype=jnp.float32)\n",
    "    _metadata_dict = unflatten_dict(_metadata_dict)\n",
    "    \n",
    "else:\n",
    "    _sharding_dict = None\n",
    "    _metadata_dict = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd9671ce-b283-4de7-8b3a-ac4e0d07c737",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install tensorflow==2.16.1\n",
    "# pip install numpy==1.26.4\n",
    "# 运行2遍\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import asyncio\n",
    "import argparse\n",
    "from collections import defaultdict\n",
    "import time\n",
    "\n",
    "# os.environ[\"JAX_PLATFORMS\"] = \"cpu\"\n",
    "from etils import epath\n",
    "import json\n",
    "import base64\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import jax.numpy as jnp\n",
    "import jax\n",
    "import orbax\n",
    "import orbax.checkpoint as ocp\n",
    "from etils import epath\n",
    "from jax.sharding import PartitionSpec as PS\n",
    "from flax.traverse_util import flatten_dict, unflatten_dict\n",
    "\n",
    "metadata_path = _metadata_path\n",
    "METADATA_FILE = '_METADATA'\n",
    "_CHECKPOINT_FILE = 'checkpoint'\n",
    "\n",
    "read_dir = \"gs://llm_base_models_europe-west4/v5p_256/7B/xm_E8T2S0x7B_WithMgate_openmoe_RouterR_1218/checkpoints/0/state\"\n",
    "read_dir = epath.Path(read_dir)\n",
    "\n",
    "back_metadata_path = read_dir / f'{METADATA_FILE}.back'\n",
    "try:\n",
    "    metadata_path.rename(back_metadata_path)\n",
    "except:\n",
    "    pass\n",
    "metadata_path.unlink(missing_ok=True) # delete\n",
    "structure_path = read_dir / _CHECKPOINT_FILE\n",
    "msgpack = ocp.aggregate_handlers.MsgpackHandler(0)\n",
    "structure = msgpack.deserialize(structure_path)\n",
    "# backup original checkpoint fil\n",
    "back_structure_path = read_dir / 'checkpoint_back'\n",
    "back_structure = structure.copy()\n",
    "if not back_structure_path.exists():\n",
    "    asyncio.run(msgpack.serialize(back_structure_path, item=back_structure))\n",
    "print(f'Old structure file keys: {structure.keys()}')\n",
    "remove_keys = ['opt_state', 'step'] # select the weight name you don't want to load, all weight name: opt_state, step, params\n",
    "_ = [structure.pop(key) for key in remove_keys if key in structure]\n",
    "print(f'New structure file keys: {structure.keys()}')\n",
    "asyncio.run(msgpack.serialize(structure_path, item=structure))  # rewrite struct file\n",
    "\n",
    "# load model based struct, note: axes must same as training\n",
    "mesh_axes = ['data', 'stage', 'fsdp', 'fsdp_transpose', 'sequence', 'tensor', 'expert', 'autoregressive']\n",
    "axes = [1] * len(mesh_axes)\n",
    "axes[2] = 4\n",
    "devices = np.asarray(jax.devices()).reshape(axes)\n",
    "mesh = jax.sharding.Mesh(devices, mesh_axes)\n",
    "sharding = jax.sharding.NamedSharding(mesh, PS()) # Sharding is None because we use cpu to load weights\n",
    "weight_dtype = jnp.bfloat16 # set restore weights dtype\n",
    "restore_args = {}\n",
    "for k, v in flatten_dict(structure).items():\n",
    "    restore_args[k] =  ocp.ArrayRestoreArgs(restore_type=jax.Array, dtype=weight_dtype, sharding=sharding)\n",
    "restore_args = unflatten_dict(restore_args)\n",
    "\n",
    "# 新加的\n",
    "right_shard = flatten_dict(_sharding_dict)\n",
    "new_restore_args = {}\n",
    "for k, v in flatten_dict(restore_args).items():\n",
    "    print(k, v)\n",
    "    v.sharding = right_shard[k]\n",
    "    new_restore_args[k] = v\n",
    "restore_args = unflatten_dict(new_restore_args)\n",
    "\n",
    "ckptr = ocp.Checkpointer(ocp.PyTreeCheckpointHandler())\n",
    "params = ckptr.restore(read_dir, args=ocp.args.PyTreeRestore(restore_args=restore_args))\n",
    "structure_path = read_dir / _CHECKPOINT_FILE\n",
    "# rewrite struct file, otherwise occur error when continue training\n",
    "asyncio.run(msgpack.serialize(structure_path, item=back_structure))\n",
    "while 'params' in w:\n",
    "    params = params['params']\n",
    "# xm3p5_w = {'.'.join(k): np.array(v) for k, v in flatten_dict(w).items()}\n",
    "try:\n",
    "    back_metadata_path.rename(metadata_path)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cda6e24c-bb50-41bf-acd3-0f5b5fdffdcb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('params', 'params', 'decoder', 'decoder_norm', 'scale') {TpuDevice(id=1, process_index=0, coords=(1,0,0), core_on_chip=0), TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0), TpuDevice(id=2, process_index=0, coords=(0,1,0), core_on_chip=0), TpuDevice(id=3, process_index=0, coords=(1,1,0), core_on_chip=0)}\n"
     ]
    }
   ],
   "source": [
    "for k, v in flatten_dict(w).items():\n",
    "    print(k, v.devices())\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "032f0e9c-d65c-4590-a3b2-e14f16b960a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "quant = None\n",
    "\n",
    "Transformer = models.Transformer\n",
    "model = Transformer(config, mesh, quant=quant)\n",
    "is_train = False\n",
    "rng1, aqt_rng = jax.random.split(jax.random.key(9876))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bd213cd3-57db-4f6d-b944-2f63ba5a688c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据制作完成.\n"
     ]
    }
   ],
   "source": [
    "text = '<|extra_0|>飞流直下三千尺，疑是银河落九天。'\n",
    "x = tokenizer(text, return_tensor='pt')\n",
    "batch_size = 4\n",
    "input_ids = jnp.array(x['input_ids']).reshape(1, -1).repeat(batch_size, 0)\n",
    "data = {}\n",
    "data['inputs'] = input_ids[:, :-1]\n",
    "pos = jnp.arange(data['inputs'].shape[1]).reshape(1, -1).repeat(batch_size, 0)\n",
    "data[\"inputs_position\"] = jnp.broadcast_to(pos, (batch_size, pos.shape[-1]))\n",
    "data[\"inputs_segmentation\"] = jnp.ones_like(data['inputs'])\n",
    "data[\"targets\"] = input_ids[:, 1:]\n",
    "data = {k: v[:, :] for k, v in data.items()}\n",
    "print(f'数据制作完成.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fa3fd4a4-b4ca-4db4-b88b-8becd5fe3329",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "logits, intermediate_outputs = model.apply(\n",
    "          {'params':params},\n",
    "          data[\"inputs\"],\n",
    "          data[\"inputs_position\"],\n",
    "          decoder_segment_ids=data[\"inputs_segmentation\"],\n",
    "          enable_dropout=config.enable_dropout if is_train else False,\n",
    "          rngs={\"dropout\": rng1, \"params\": aqt_rng},\n",
    "          mutable=\"intermediates\",\n",
    "      )\n",
    "one_hot_targets = jax.nn.one_hot(data[\"targets\"], config.vocab_size)\n",
    "loss, _ = max_utils.cross_entropy_with_logits(logits, one_hot_targets, 0.0)\n",
    "\n",
    "print(f'loss shape: {loss.shape} mean: {loss.mean()}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
