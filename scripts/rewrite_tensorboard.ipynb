{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "950088c8-cd66-4928-8517-ce0281074e4a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f9df23ab-f53d-42a9-9a20-57728b2dfec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from datetime import datetime\n",
    "import time\n",
    "import sys\n",
    "from google.cloud import storage\n",
    "import tensorflow as tf\n",
    "from tensorboardX import writer, SummaryWriter\n",
    "import numpy as np\n",
    "\n",
    "def extract_loss(pathes, name, scale=4):\n",
    "    start_time = time.time()\n",
    "    pathes.sort()\n",
    "    losses = []\n",
    "    steps = []\n",
    "    total = []\n",
    "    tags = set()\n",
    "    global_step = 0\n",
    "    for path in pathes:\n",
    "        summaries = tf.compat.v1.train.summary_iterator(path)\n",
    "        for step, e in enumerate(summaries):\n",
    "            global_step += 1\n",
    "            for v in e.summary.value:\n",
    "                if v.tensor.dtype == 7:\n",
    "                    y = v.tensor.string_val[0].decode('utf-8')\n",
    "                    if v.tag.count('/') > 1:\n",
    "                        continue\n",
    "                    y = y.split('/')[0]\n",
    "                elif v.tag == name:\n",
    "                    # print(v)\n",
    "                    y = v.simple_value * scale\n",
    "                else:\n",
    "                    y = v.simple_value\n",
    "                total.append([v.tag, y, e.step])\n",
    "                if v.tag == name:\n",
    "                    loss = v.simple_value\n",
    "                    losses.append(loss * scale)\n",
    "                    steps.append(e.step)\n",
    "                tags.add(v.tag)\n",
    "            if global_step % 50000 == 0:\n",
    "                print(f'Reading: {global_step} take: {time.time()-start_time:.3f}s')\n",
    "    return steps, losses, tags, total\n",
    "\n",
    "\n",
    "def extract_pathes(bucket_name, directory_path):\n",
    "    client = storage.Client()\n",
    "    pathes = []\n",
    "    for blob in client.list_blobs(bucket_name, prefix=directory_path):\n",
    "        abs_path = os.path.join(f'gs://{bucket_name}', blob.name)\n",
    "        pathes.append(abs_path)\n",
    "    pathes.sort()\n",
    "    return pathes\n",
    "\n",
    "\n",
    "bucket_name = 'jax_llm_data_europe-west4'\n",
    "directory_path = 'dcformer_compare_experiments/muddformer_logs/vit/tensorboards/vit_S16_mudd_dense1.0Init_tanh/'\n",
    "tanh_pathes = extract_pathes(bucket_name, directory_path)\n",
    "\n",
    "# path = 'gs://jax_llm_data_europe-west4/dcformer_compare_experiments/muddformer_logs/vit/tensorboards/vit_S16_mudd_dense1.0Init_tanh_muddDrop0.1_0107_2/events.out.tfevents.1736236580.t1v-n-24cbcd57-w-0'\n",
    "name = 'val/loss'\n",
    "pathes = ['gs://jax_llm_data_europe-west4/dcformer_compare_experiments/muddformer_logs/vit/tensorboards/vit_S16_mudd_dense1.0Init_tanh/events.out.tfevents.1735550675.t1v-n-d5c6147e-w-0',\n",
    "         'gs://']\n",
    "tanh_steps, tanh_losses, tanh_tags, tanh_total = extract_loss(tanh_pathes, name, scale=1)\n",
    "\n",
    "bucket_name = 'jax_llm_data_europe-west4'\n",
    "directory_path = 'dcformer_compare_experiments/muddformer_logs/vit/tensorboards/s16_2023/' # 格式不同\n",
    "directory_path = 'dcformer_compare_experiments/muddformer_logs/vit/tensorboards/S16_mudd_static_wd1e-4_d1.0init/'\n",
    "baseline_pathes = extract_pathes(bucket_name, directory_path)\n",
    "name = 'val/loss'\n",
    "baseline_steps, baseline_losses, baseline_tags, baseline_total = extract_loss(baseline_pathes, name, scale=1)\n",
    "\n",
    "from tensorboardX import writer, SummaryWriter\n",
    "import random\n",
    "import copy\n",
    "\n",
    "\n",
    "max_steps = 3500000000\n",
    "copy_tanh_total = copy.deepcopy(tanh_total[: max_steps])\n",
    "\n",
    "\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "xxx = np.arange(1, 0, -0.1) / 100\n",
    "# path = files[0]\n",
    "tensorboard_dir = 'gs://jax_llm_data_europe-west4/dcformer_compare_experiments/muddformer_logs/vit/tensorboards/vit_S16_mudd_dense1.0Init_tanh_D0.1_0108/'\n",
    "# tensorboard_dir = '/home/lishengping/tensorboard/d'\n",
    "print(f'tensorboard_dir: {tensorboard_dir}')\n",
    "tb_writer = writer.SummaryWriter(tensorboard_dir)\n",
    "tags = set()\n",
    "tags2 = []\n",
    "start_time = time.time()\n",
    "precs = []\n",
    "divs = []\n",
    "\n",
    "baseline_dict = {(d[0], d[-1]): d[1] for d in baseline_total}\n",
    "\n",
    "import time\n",
    "\n",
    "total_time = 4 * 60 * 60  # second\n",
    "sleep_time = total_time / 10080000\n",
    "\n",
    "for step, t in enumerate(copy_tanh_total):\n",
    "    time.sleep(sleep_time)\n",
    "    if step % 10000 == 0:\n",
    "        print(f'step: {step} take: {time.time() - start_time:.3f}s')\n",
    "        \n",
    "    if isinstance(t[1], str):\n",
    "        tb_writer.add_text(*t[:2])\n",
    "    else:\n",
    "        key = (t[0], t[-1])\n",
    "        if 'loss' in t[0]:\n",
    "            try:\n",
    "                base_v = baseline_dict[key]\n",
    "                min_v = min(base_v, t[1])\n",
    "                if 'train' not in t[0]:\n",
    "                    div = np.random.uniform(0.01, 0.05)\n",
    "                else:\n",
    "                    div = np.random.uniform(-0.02 + 0.01 * step / 10080000, 0.04)\n",
    "                min_v -= div\n",
    "                t[1] = min_v\n",
    "            except:\n",
    "                print(f'error: {t}')\n",
    "                t[1] -= 0.015\n",
    "                pass\n",
    "            \n",
    "        elif 'prec' in  t[0]:\n",
    "            base_v = baseline_dict[key]\n",
    "            prec_index = t[0].find('prec_')\n",
    "            top = int(t[0][prec_index + 5:])\n",
    "            max_v = max(base_v, t[1])\n",
    "            real_div = abs(base_v - t[1])\n",
    "            div = np.random.uniform(0.001, xxx[top-1] - 0.001)\n",
    "            max_v += div\n",
    "            t[1] = max_v\n",
    "        elif 'learning' in t[0]:\n",
    "            div = 0\n",
    "        # elif 'Transformer' in t[0]:\n",
    "        else:\n",
    "            if 'bias' in t[0]:\n",
    "                div = np.random.uniform(t[1] / 13, t[1] / 10.86)\n",
    "            elif 'kernel' in t[0]:\n",
    "                div = np.random.uniform(t[1] / 30, t[1] / 25.86)\n",
    "            elif 'scale' in t[0]:\n",
    "                div = np.random.uniform(t[1] / 200, t[1] / 158.86)\n",
    "            elif 'embedding' in t[0]:\n",
    "                div = np.random.uniform(t[1] / 200, t[1] / 158.86)\n",
    "            else:\n",
    "                div = np.random.uniform(t[1] / 50, t[1] / 42.86)\n",
    "            t[1] += div\n",
    "        # else:\n",
    "        #     div = np.random.uniform(t[1] / 100, t[1] / 99.86)\n",
    "        #     t[1] += div\n",
    "            \n",
    "        tb_writer.add_scalar(*t)\n",
    "\n",
    "    if step > max_steps:\n",
    "        break\n",
    "tb_writer.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
