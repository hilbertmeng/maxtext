{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d40b0160",
   "metadata": {},
   "source": [
    "### 说明\n",
    "- 基于440000预训练的dense模型进行初始化，1个共享专家，8个非共享专家，共享和非共享专家均直接复制dense的mlp。\n",
    "- 其次采用的是openmoe的实现方案"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f932c95e-9958-4079-b1da-0d9245046e44",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-27 19:10:06.572276: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:479] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-11-27 19:10:06.588055: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:10575] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-11-27 19:10:06.588083: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1442] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-27 19:10:07.325663: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Old structure file keys: dict_keys(['params'])\n",
      "New structure file keys: dict_keys(['params'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1732734613.567408   29825 gcs_resource.cc:109] Using default AdmissionQueue with limit 32\n",
      "I0000 00:00:1732734613.571445   30404 google_auth_provider.cc:180] Running on GCE, using service account 887571727717-compute@developer.gserviceaccount.com\n"
     ]
    }
   ],
   "source": [
    "# !pip install tensorflow==2.16.1\n",
    "# pip install numpy==1.26.4\n",
    "# 运行2遍\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import asyncio\n",
    "import argparse\n",
    "from collections import defaultdict\n",
    "import time\n",
    "\n",
    "os.environ[\"JAX_PLATFORMS\"] = \"cpu\"\n",
    "from etils import epath\n",
    "import json\n",
    "import base64\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import jax.numpy as jnp\n",
    "import jax\n",
    "import orbax\n",
    "import orbax.checkpoint as ocp\n",
    "from etils import epath\n",
    "from jax.sharding import PartitionSpec as PS\n",
    "from flax.traverse_util import flatten_dict, unflatten_dict\n",
    "\n",
    "\n",
    "METADATA_FILE = '_METADATA'\n",
    "_CHECKPOINT_FILE = 'checkpoint'\n",
    "\n",
    "\n",
    "read_dir = 'gs://llm_base_models_europe-west4/v5p_256/7B/PileDCSlimLlama7B32Kx4x256x1v5p_0713/checkpoints/440000/state'\n",
    "save_dir = 'gs://llm_base_models_europe-west4/v5p_256/7B/xm_45x7B_moe_129/checkpoints/'\n",
    "\n",
    "read_dir = epath.Path(read_dir) \n",
    "save_dir = epath.Path(save_dir)\n",
    "\n",
    "metadata_path = read_dir / METADATA_FILE\n",
    "back_metadata_path = read_dir / f'{METADATA_FILE}.back'\n",
    "try:\n",
    "    metadata_path.rename(back_metadata_path)\n",
    "except:\n",
    "    pass\n",
    "metadata_path.unlink(missing_ok=True) # delete\n",
    "structure_path = read_dir / _CHECKPOINT_FILE\n",
    "msgpack = ocp.aggregate_handlers.MsgpackHandler(0)\n",
    "structure = msgpack.deserialize(structure_path)\n",
    "# backup original checkpoint fil\n",
    "back_structure_path = read_dir / 'checkpoint_back'\n",
    "back_structure = structure.copy()\n",
    "if not back_structure_path.exists():\n",
    "    asyncio.run(msgpack.serialize(back_structure_path, item=back_structure))\n",
    "print(f'Old structure file keys: {structure.keys()}')\n",
    "remove_keys = ['opt_state', 'step'] # select the weight name you don't want to load, all weight name: opt_state, step, params\n",
    "_ = [structure.pop(key) for key in remove_keys if key in structure]\n",
    "print(f'New structure file keys: {structure.keys()}')\n",
    "asyncio.run(msgpack.serialize(structure_path, item=structure))  # rewrite struct file\n",
    "\n",
    "# load model based struct, note: axes must same as training\n",
    "mesh_axes = ['data', 'stage', 'fsdp', 'fsdp_transpose', 'sequence', 'tensor', 'autoregressive']\n",
    "devices = np.asarray(jax.devices()).reshape([1] * len(mesh_axes))\n",
    "mesh = jax.sharding.Mesh(devices, mesh_axes)\n",
    "sharding = jax.sharding.NamedSharding(mesh, PS()) # Sharding is None because we use cpu to load weights\n",
    "weight_dtype = jnp.bfloat16 # set restore weights dtype\n",
    "restore_args = {}\n",
    "for k, v in flatten_dict(structure).items():\n",
    "    restore_args[k] =  ocp.ArrayRestoreArgs(restore_type=jax.Array, dtype=weight_dtype, sharding=sharding)\n",
    "restore_args = unflatten_dict(restore_args)\n",
    "ckptr = ocp.Checkpointer(ocp.PyTreeCheckpointHandler())\n",
    "w = ckptr.restore(read_dir, args=ocp.args.PyTreeRestore(restore_args=restore_args))\n",
    "structure_path = read_dir / _CHECKPOINT_FILE\n",
    "# rewrite struct file, otherwise occur error when continue training\n",
    "asyncio.run(msgpack.serialize(structure_path, item=back_structure))\n",
    "while 'params' in w:\n",
    "    w = w['params']\n",
    "xm3p5_w = {'.'.join(k): np.array(v) for k, v in flatten_dict(w).items()}\n",
    "\n",
    "try:\n",
    "    back_metadata_path.rename(metadata_path)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b96d15f-c5d8-42ee-9ea0-ffc501df4672",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 基于dense模型保存和moe相同名字的参数\n",
    "def convert_to_jnp(params, remove_keys=[]):\n",
    "    convert_params = {}\n",
    "    for k, v in params.items():\n",
    "        r = 0\n",
    "        for remove_key in remove_keys:\n",
    "            if remove_key in k: \n",
    "                r = 1\n",
    "                break\n",
    "        if r: continue\n",
    "        k = tuple(k.split('.'))\n",
    "        convert_params[k] = v\n",
    "        # convert_params[k] = jnp.array(v).astype(jnp.bfloat16)\n",
    "    for k, v in convert_params.items():\n",
    "        print(k, v.shape, v.dtype)\n",
    "    return convert_params\n",
    "\n",
    "\n",
    "def save_params(step, save_dir, params):\n",
    "    item = {\n",
    "            'state': orbax.checkpoint.AsyncCheckpointer(orbax.checkpoint.PyTreeCheckpointHandler(use_ocdbt=False)),\n",
    "                    }\n",
    "    # new_params = {tuple(k.split('.')): v for k,v in params.items()}\n",
    "    unflatten_params = unflatten_dict(params)\n",
    "    for k, v in params.items():\n",
    "        print(k, v.shape, v.dtype)\n",
    "    mngr = orbax.checkpoint.CheckpointManager(save_dir, item)\n",
    "    if 'params' not in unflatten_params: unflatten_params = {'params': unflatten_params}\n",
    "    mngr.save(step, items={'state': {'params': unflatten_params}})\n",
    "\n",
    "# convert_params = convert_to_jnp(xm3p5_w, remove_keys=[])\n",
    "# save_params(0, save_dir, convert_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "325abe21-6624-488e-b322-af6ee54f890c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k: decoder.decoder_norm.scale take: 0.050s\n",
      "k: decoder.layers.mlp_0.mgate.kernel take: 0.064s\n",
      "unshared_mlp: decoder.layers.unshared_mlp_0.mgate v: (4096, 12, 44) copy_w: (8, 12, 4096, 44)\n",
      "k: decoder.layers.mlp_0.wi_0.kernel take: 0.244s\n",
      "unshared_mlp: decoder.layers.unshared_mlp_0.wi_0 v: (4096, 12, 5632) copy_w: (8, 12, 4096, 5632)\n",
      "k: decoder.layers.mlp_0.wi_1.kernel take: 1.265s\n",
      "unshared_mlp: decoder.layers.unshared_mlp_0.wi_1 v: (4096, 12, 5632) copy_w: (8, 12, 4096, 5632)\n",
      "k: decoder.layers.mlp_0.wo.kernel take: 1.900s\n",
      "unshared_mlp: decoder.layers.unshared_mlp_0.wo v: (5632, 12, 4096) copy_w: (8, 12, 5632, 4096)\n",
      "k: decoder.layers.mlp_1.mgate.kernel take: 2.617s\n",
      "unshared_mlp: decoder.layers.unshared_mlp_1.mgate v: (4096, 12, 44) copy_w: (8, 12, 4096, 44)\n",
      "k: decoder.layers.mlp_1.wi_0.kernel take: 2.626s\n",
      "unshared_mlp: decoder.layers.unshared_mlp_1.wi_0 v: (4096, 12, 5632) copy_w: (8, 12, 4096, 5632)\n",
      "k: decoder.layers.mlp_1.wi_1.kernel take: 2.626s\n",
      "unshared_mlp: decoder.layers.unshared_mlp_1.wi_1 v: (4096, 12, 5632) copy_w: (8, 12, 4096, 5632)\n",
      "k: decoder.layers.mlp_1.wo.kernel take: 2.627s\n",
      "unshared_mlp: decoder.layers.unshared_mlp_1.wo v: (5632, 12, 4096) copy_w: (8, 12, 5632, 4096)\n",
      "k: decoder.layers.mlp_2.mgate.kernel take: 2.627s\n",
      "unshared_mlp: decoder.layers.unshared_mlp_2.mgate v: (4096, 12, 44) copy_w: (8, 12, 4096, 44)\n",
      "k: decoder.layers.mlp_2.wi_0.kernel take: 2.628s\n",
      "unshared_mlp: decoder.layers.unshared_mlp_2.wi_0 v: (4096, 12, 5632) copy_w: (8, 12, 4096, 5632)\n",
      "k: decoder.layers.mlp_2.wi_1.kernel take: 2.628s\n",
      "unshared_mlp: decoder.layers.unshared_mlp_2.wi_1 v: (4096, 12, 5632) copy_w: (8, 12, 4096, 5632)\n",
      "k: decoder.layers.mlp_2.wo.kernel take: 2.629s\n",
      "unshared_mlp: decoder.layers.unshared_mlp_2.wo v: (5632, 12, 4096) copy_w: (8, 12, 5632, 4096)\n",
      "k: decoder.layers.mlp_3.mgate.kernel take: 3.256s\n",
      "unshared_mlp: decoder.layers.unshared_mlp_3.mgate v: (4096, 12, 44) copy_w: (8, 12, 4096, 44)\n",
      "k: decoder.layers.mlp_3.wi_0.kernel take: 3.937s\n",
      "unshared_mlp: decoder.layers.unshared_mlp_3.wi_0 v: (4096, 12, 5632) copy_w: (8, 12, 4096, 5632)\n",
      "k: decoder.layers.mlp_3.wi_1.kernel take: 4.557s\n",
      "unshared_mlp: decoder.layers.unshared_mlp_3.wi_1 v: (4096, 12, 5632) copy_w: (8, 12, 4096, 5632)\n",
      "k: decoder.layers.mlp_3.wo.kernel take: 4.564s\n",
      "unshared_mlp: decoder.layers.unshared_mlp_3.wo v: (5632, 12, 4096) copy_w: (8, 12, 5632, 4096)\n",
      "k: decoder.layers.post_self_attention_layer_norm_0.scale take: 5.193s\n",
      "k: decoder.layers.post_self_attention_layer_norm_1.scale take: 5.231s\n",
      "k: decoder.layers.post_self_attention_layer_norm_2.scale take: 5.250s\n",
      "k: decoder.layers.post_self_attention_layer_norm_3.scale take: 5.261s\n",
      "k: decoder.layers.pre_self_attention_layer_norm_0.scale take: 5.389s\n",
      "k: decoder.layers.pre_self_attention_layer_norm_1.scale take: 5.810s\n",
      "k: decoder.layers.pre_self_attention_layer_norm_2.scale take: 5.821s\n",
      "k: decoder.layers.pre_self_attention_layer_norm_3.scale take: 5.837s\n",
      "k: decoder.layers.self_attention_0.AttentionOp_0.dyn_w_proj.dd.kernel take: 5.853s\n",
      "k: decoder.layers.self_attention_0.AttentionOp_0.dyn_w_proj.dw1.kernel take: 5.975s\n",
      "k: decoder.layers.self_attention_0.AttentionOp_0.dyn_w_proj.qkw take: 6.416s\n",
      "k: decoder.layers.self_attention_0.k_norm.scale take: 6.430s\n",
      "k: decoder.layers.self_attention_0.key.kernel take: 6.442s\n",
      "k: decoder.layers.self_attention_0.out.kernel take: 6.453s\n",
      "k: decoder.layers.self_attention_0.q_norm.scale take: 6.453s\n",
      "k: decoder.layers.self_attention_0.query.kernel take: 6.453s\n",
      "k: decoder.layers.self_attention_0.value.kernel take: 6.453s\n",
      "k: decoder.layers.self_attention_1.AttentionOp_0.dyn_w_proj.dd.kernel take: 6.477s\n",
      "k: decoder.layers.self_attention_1.AttentionOp_0.dyn_w_proj.dw1.kernel take: 6.493s\n",
      "k: decoder.layers.self_attention_1.AttentionOp_0.dyn_w_proj.qkw take: 6.504s\n",
      "k: decoder.layers.self_attention_1.k_norm.scale take: 6.635s\n",
      "k: decoder.layers.self_attention_1.key.kernel take: 7.063s\n",
      "k: decoder.layers.self_attention_1.out.kernel take: 7.072s\n",
      "k: decoder.layers.self_attention_1.q_norm.scale take: 7.091s\n",
      "k: decoder.layers.self_attention_1.query.kernel take: 7.102s\n",
      "k: decoder.layers.self_attention_1.value.kernel take: 7.230s\n",
      "k: decoder.layers.self_attention_2.AttentionOp_0.dyn_w_proj.dd.kernel take: 7.662s\n",
      "k: decoder.layers.self_attention_2.AttentionOp_0.dyn_w_proj.dw1.kernel take: 7.673s\n",
      "k: decoder.layers.self_attention_2.AttentionOp_0.dyn_w_proj.qkw take: 7.689s\n",
      "k: decoder.layers.self_attention_2.k_norm.scale take: 7.701s\n",
      "k: decoder.layers.self_attention_2.key.kernel take: 7.826s\n",
      "k: decoder.layers.self_attention_2.out.kernel take: 8.249s\n",
      "k: decoder.layers.self_attention_2.q_norm.scale take: 8.249s\n",
      "k: decoder.layers.self_attention_2.query.kernel take: 8.249s\n",
      "k: decoder.layers.self_attention_2.value.kernel take: 8.249s\n",
      "k: decoder.layers.self_attention_3.AttentionOp_0.dyn_w_proj.dd.kernel take: 8.249s\n",
      "k: decoder.layers.self_attention_3.AttentionOp_0.dyn_w_proj.dw1.kernel take: 8.249s\n",
      "k: decoder.layers.self_attention_3.AttentionOp_0.dyn_w_proj.qkw take: 8.250s\n",
      "k: decoder.layers.self_attention_3.k_norm.scale take: 8.250s\n",
      "k: decoder.layers.self_attention_3.key.kernel take: 8.250s\n",
      "k: decoder.layers.self_attention_3.out.kernel take: 8.252s\n",
      "k: decoder.layers.self_attention_3.q_norm.scale take: 8.257s\n",
      "k: decoder.layers.self_attention_3.query.kernel take: 8.258s\n",
      "k: decoder.layers.self_attention_3.value.kernel take: 8.258s\n",
      "k: decoder.logits_dense.kernel take: 8.274s\n",
      "k: token_embedder.embedding take: 8.287s\n"
     ]
    }
   ],
   "source": [
    "## moe部分的参数保存，保存后在bucket后台人工进行转移\n",
    "start_time = time.time()\n",
    "unshared_experts = 8\n",
    "\n",
    "scale = 1\n",
    "mlp_dim = 5632 // scale\n",
    "model_dim = 4096 // scale\n",
    "# fp16_dtype = np.dtype('float16')\n",
    "# 4个子层\n",
    "total_moe_params = [{} for i in range(4)]\n",
    "for k, v in xm3p5_w.items():\n",
    "    v = jnp.array(v).astype(jnp.bfloat16)\n",
    "    print(f'k: {k} take: {time.time() - start_time:.3f}s')\n",
    "    if 'decoder.layers.mlp_' in k:\n",
    "        mlp_inx = k.find('mlp_')\n",
    "        l = k[mlp_inx+4: mlp_inx+5]\n",
    "        # if int(l) % 2 != 0: continue\n",
    "        moe_params = total_moe_params[int(l)]\n",
    "        unshared_mlp = k.replace('decoder.layers.mlp_', 'decoder.layers.unshared_mlp_')\n",
    "        unshared_mlp = unshared_mlp.replace('.kernel', '')\n",
    "        copy_w = v.transpose(1, 0, 2)[None].repeat(unshared_experts, 0)\n",
    "        print(f'unshared_mlp: {unshared_mlp} v: {v.shape} copy_w: {copy_w.shape}')\n",
    "        moe_params[unshared_mlp] = copy_w\n",
    "        if 'mgate' in unshared_mlp:\n",
    "            router_gate = unshared_mlp.replace('mgate', 'router_gate.kernel')\n",
    "            moe_params[router_gate] = v[...,:unshared_experts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b81ed971-203a-42ff-a101-7d7f1287ddf0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Configured `CheckpointManager` using deprecated legacy API. Please follow the instructions at https://orbax.readthedocs.io/en/latest/api_refactor.html to migrate by August 1st, 2024.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save step: 1\n",
      "('decoder', 'layers', 'unshared_mlp_0', 'mgate') (8, 12, 4096, 44)\n",
      "('decoder', 'layers', 'unshared_mlp_0', 'router_gate', 'kernel') (4096, 12, 8)\n",
      "('decoder', 'layers', 'unshared_mlp_0', 'wi_0') (8, 12, 4096, 5632)\n",
      "('decoder', 'layers', 'unshared_mlp_0', 'wi_1') (8, 12, 4096, 5632)\n",
      "('decoder', 'layers', 'unshared_mlp_0', 'wo') (8, 12, 5632, 4096)\n",
      "('decoder', 'layers', 'unshared_mlp_0', 'mgate') (8, 12, 4096, 44) bfloat16\n",
      "('decoder', 'layers', 'unshared_mlp_0', 'router_gate', 'kernel') (4096, 12, 8) bfloat16\n",
      "('decoder', 'layers', 'unshared_mlp_0', 'wi_0') (8, 12, 4096, 5632) bfloat16\n",
      "('decoder', 'layers', 'unshared_mlp_0', 'wi_1') (8, 12, 4096, 5632) bfloat16\n",
      "('decoder', 'layers', 'unshared_mlp_0', 'wo') (8, 12, 5632, 4096) bfloat16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Configured `CheckpointManager` using deprecated legacy API. Please follow the instructions at https://orbax.readthedocs.io/en/latest/api_refactor.html to migrate by August 1st, 2024.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save step: 2\n",
      "('decoder', 'layers', 'unshared_mlp_1', 'mgate') (8, 12, 4096, 44)\n",
      "('decoder', 'layers', 'unshared_mlp_1', 'router_gate', 'kernel') (4096, 12, 8)\n",
      "('decoder', 'layers', 'unshared_mlp_1', 'wi_0') (8, 12, 4096, 5632)\n",
      "('decoder', 'layers', 'unshared_mlp_1', 'wi_1') (8, 12, 4096, 5632)\n",
      "('decoder', 'layers', 'unshared_mlp_1', 'wo') (8, 12, 5632, 4096)\n",
      "('decoder', 'layers', 'unshared_mlp_1', 'mgate') (8, 12, 4096, 44) bfloat16\n",
      "('decoder', 'layers', 'unshared_mlp_1', 'router_gate', 'kernel') (4096, 12, 8) bfloat16\n",
      "('decoder', 'layers', 'unshared_mlp_1', 'wi_0') (8, 12, 4096, 5632) bfloat16\n",
      "('decoder', 'layers', 'unshared_mlp_1', 'wi_1') (8, 12, 4096, 5632) bfloat16\n",
      "('decoder', 'layers', 'unshared_mlp_1', 'wo') (8, 12, 5632, 4096) bfloat16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Configured `CheckpointManager` using deprecated legacy API. Please follow the instructions at https://orbax.readthedocs.io/en/latest/api_refactor.html to migrate by August 1st, 2024.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save step: 3\n",
      "('decoder', 'layers', 'unshared_mlp_2', 'mgate') (8, 12, 4096, 44)\n",
      "('decoder', 'layers', 'unshared_mlp_2', 'router_gate', 'kernel') (4096, 12, 8)\n",
      "('decoder', 'layers', 'unshared_mlp_2', 'wi_0') (8, 12, 4096, 5632)\n",
      "('decoder', 'layers', 'unshared_mlp_2', 'wi_1') (8, 12, 4096, 5632)\n",
      "('decoder', 'layers', 'unshared_mlp_2', 'wo') (8, 12, 5632, 4096)\n",
      "('decoder', 'layers', 'unshared_mlp_2', 'mgate') (8, 12, 4096, 44) bfloat16\n",
      "('decoder', 'layers', 'unshared_mlp_2', 'router_gate', 'kernel') (4096, 12, 8) bfloat16\n",
      "('decoder', 'layers', 'unshared_mlp_2', 'wi_0') (8, 12, 4096, 5632) bfloat16\n",
      "('decoder', 'layers', 'unshared_mlp_2', 'wi_1') (8, 12, 4096, 5632) bfloat16\n",
      "('decoder', 'layers', 'unshared_mlp_2', 'wo') (8, 12, 5632, 4096) bfloat16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Configured `CheckpointManager` using deprecated legacy API. Please follow the instructions at https://orbax.readthedocs.io/en/latest/api_refactor.html to migrate by August 1st, 2024.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save step: 4\n",
      "('decoder', 'layers', 'unshared_mlp_3', 'mgate') (8, 12, 4096, 44)\n",
      "('decoder', 'layers', 'unshared_mlp_3', 'router_gate', 'kernel') (4096, 12, 8)\n",
      "('decoder', 'layers', 'unshared_mlp_3', 'wi_0') (8, 12, 4096, 5632)\n",
      "('decoder', 'layers', 'unshared_mlp_3', 'wi_1') (8, 12, 4096, 5632)\n",
      "('decoder', 'layers', 'unshared_mlp_3', 'wo') (8, 12, 5632, 4096)\n",
      "('decoder', 'layers', 'unshared_mlp_3', 'mgate') (8, 12, 4096, 44) bfloat16\n",
      "('decoder', 'layers', 'unshared_mlp_3', 'router_gate', 'kernel') (4096, 12, 8) bfloat16\n",
      "('decoder', 'layers', 'unshared_mlp_3', 'wi_0') (8, 12, 4096, 5632) bfloat16\n",
      "('decoder', 'layers', 'unshared_mlp_3', 'wi_1') (8, 12, 4096, 5632) bfloat16\n",
      "('decoder', 'layers', 'unshared_mlp_3', 'wo') (8, 12, 5632, 4096) bfloat16\n"
     ]
    }
   ],
   "source": [
    "for i, params in enumerate(total_moe_params):\n",
    "    if not params: continue\n",
    "    print(f'Save step: {i+1}')\n",
    "    # print(params.keys(), '\\n')\n",
    "    save_moe_params = {}\n",
    "    for k, v in params.items():\n",
    "        newk = tuple(k.split('.'))\n",
    "        save_moe_params[newk] = v\n",
    "        print(newk, v.shape)\n",
    "    save_params(i + 1, save_dir, save_moe_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7de7c1f5-bbd1-4909-99ca-a32d363e70a2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Configured `CheckpointManager` using deprecated legacy API. Please follow the instructions at https://orbax.readthedocs.io/en/latest/api_refactor.html to migrate by August 1st, 2024.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k: decoder.decoder_norm.scale take: 0.001s\n",
      "k: decoder.layers.mlp_0.mgate.kernel take: 0.002s\n",
      "k: decoder.layers.mlp_0.wi_0.kernel take: 0.002s\n",
      "k: decoder.layers.mlp_0.wi_1.kernel take: 0.002s\n",
      "k: decoder.layers.mlp_0.wo.kernel take: 0.002s\n",
      "k: decoder.layers.mlp_1.mgate.kernel take: 0.002s\n",
      "k: decoder.layers.mlp_1.wi_0.kernel take: 0.003s\n",
      "k: decoder.layers.mlp_1.wi_1.kernel take: 0.003s\n",
      "k: decoder.layers.mlp_1.wo.kernel take: 0.003s\n",
      "k: decoder.layers.mlp_2.mgate.kernel take: 0.003s\n",
      "k: decoder.layers.mlp_2.wi_0.kernel take: 0.003s\n",
      "k: decoder.layers.mlp_2.wi_1.kernel take: 0.003s\n",
      "k: decoder.layers.mlp_2.wo.kernel take: 0.004s\n",
      "k: decoder.layers.mlp_3.mgate.kernel take: 0.004s\n",
      "k: decoder.layers.mlp_3.wi_0.kernel take: 0.004s\n",
      "k: decoder.layers.mlp_3.wi_1.kernel take: 0.004s\n",
      "k: decoder.layers.mlp_3.wo.kernel take: 0.004s\n",
      "k: decoder.layers.post_self_attention_layer_norm_0.scale take: 0.004s\n",
      "k: decoder.layers.post_self_attention_layer_norm_1.scale take: 0.004s\n",
      "k: decoder.layers.post_self_attention_layer_norm_2.scale take: 0.005s\n",
      "k: decoder.layers.post_self_attention_layer_norm_3.scale take: 0.005s\n",
      "k: decoder.layers.pre_self_attention_layer_norm_0.scale take: 0.005s\n",
      "k: decoder.layers.pre_self_attention_layer_norm_1.scale take: 0.005s\n",
      "k: decoder.layers.pre_self_attention_layer_norm_2.scale take: 0.005s\n",
      "k: decoder.layers.pre_self_attention_layer_norm_3.scale take: 0.005s\n",
      "k: decoder.layers.self_attention_0.AttentionOp_0.dyn_w_proj.dd.kernel take: 0.005s\n",
      "k: decoder.layers.self_attention_0.AttentionOp_0.dyn_w_proj.dw1.kernel take: 0.006s\n",
      "k: decoder.layers.self_attention_0.AttentionOp_0.dyn_w_proj.qkw take: 0.006s\n",
      "k: decoder.layers.self_attention_0.k_norm.scale take: 0.006s\n",
      "k: decoder.layers.self_attention_0.key.kernel take: 0.006s\n",
      "k: decoder.layers.self_attention_0.out.kernel take: 0.006s\n",
      "k: decoder.layers.self_attention_0.q_norm.scale take: 0.006s\n",
      "k: decoder.layers.self_attention_0.query.kernel take: 0.006s\n",
      "k: decoder.layers.self_attention_0.value.kernel take: 0.007s\n",
      "k: decoder.layers.self_attention_1.AttentionOp_0.dyn_w_proj.dd.kernel take: 0.007s\n",
      "k: decoder.layers.self_attention_1.AttentionOp_0.dyn_w_proj.dw1.kernel take: 0.007s\n",
      "k: decoder.layers.self_attention_1.AttentionOp_0.dyn_w_proj.qkw take: 0.007s\n",
      "k: decoder.layers.self_attention_1.k_norm.scale take: 0.007s\n",
      "k: decoder.layers.self_attention_1.key.kernel take: 0.007s\n",
      "k: decoder.layers.self_attention_1.out.kernel take: 0.007s\n",
      "k: decoder.layers.self_attention_1.q_norm.scale take: 0.007s\n",
      "k: decoder.layers.self_attention_1.query.kernel take: 0.008s\n",
      "k: decoder.layers.self_attention_1.value.kernel take: 0.008s\n",
      "k: decoder.layers.self_attention_2.AttentionOp_0.dyn_w_proj.dd.kernel take: 0.008s\n",
      "k: decoder.layers.self_attention_2.AttentionOp_0.dyn_w_proj.dw1.kernel take: 0.008s\n",
      "k: decoder.layers.self_attention_2.AttentionOp_0.dyn_w_proj.qkw take: 0.008s\n",
      "k: decoder.layers.self_attention_2.k_norm.scale take: 0.008s\n",
      "k: decoder.layers.self_attention_2.key.kernel take: 0.008s\n",
      "k: decoder.layers.self_attention_2.out.kernel take: 0.009s\n",
      "k: decoder.layers.self_attention_2.q_norm.scale take: 0.009s\n",
      "k: decoder.layers.self_attention_2.query.kernel take: 0.009s\n",
      "k: decoder.layers.self_attention_2.value.kernel take: 0.009s\n",
      "k: decoder.layers.self_attention_3.AttentionOp_0.dyn_w_proj.dd.kernel take: 0.009s\n",
      "k: decoder.layers.self_attention_3.AttentionOp_0.dyn_w_proj.dw1.kernel take: 0.009s\n",
      "k: decoder.layers.self_attention_3.AttentionOp_0.dyn_w_proj.qkw take: 0.009s\n",
      "k: decoder.layers.self_attention_3.k_norm.scale take: 0.010s\n",
      "k: decoder.layers.self_attention_3.key.kernel take: 0.010s\n",
      "k: decoder.layers.self_attention_3.out.kernel take: 0.010s\n",
      "k: decoder.layers.self_attention_3.q_norm.scale take: 0.010s\n",
      "k: decoder.layers.self_attention_3.query.kernel take: 0.010s\n",
      "k: decoder.layers.self_attention_3.value.kernel take: 0.010s\n",
      "k: decoder.logits_dense.kernel take: 0.010s\n",
      "k: token_embedder.embedding take: 0.011s\n",
      "('decoder', 'decoder_norm', 'scale') (1,) bfloat16\n",
      "('decoder', 'layers', 'mlp_0', 'mgate', 'kernel') (1,) bfloat16\n",
      "('decoder', 'layers', 'unshared_mlp_0', 'mgate') (1,) bfloat16\n",
      "('decoder', 'layers', 'unshared_mlp_0', 'router_gate', 'kernel') (1,) bfloat16\n",
      "('decoder', 'layers', 'mlp_0', 'wi_0', 'kernel') (1,) bfloat16\n",
      "('decoder', 'layers', 'unshared_mlp_0', 'wi_0') (1,) bfloat16\n",
      "('decoder', 'layers', 'mlp_0', 'wi_1', 'kernel') (1,) bfloat16\n",
      "('decoder', 'layers', 'unshared_mlp_0', 'wi_1') (1,) bfloat16\n",
      "('decoder', 'layers', 'mlp_0', 'wo', 'kernel') (1,) bfloat16\n",
      "('decoder', 'layers', 'unshared_mlp_0', 'wo') (1,) bfloat16\n",
      "('decoder', 'layers', 'mlp_1', 'mgate', 'kernel') (1,) bfloat16\n",
      "('decoder', 'layers', 'unshared_mlp_1', 'mgate') (1,) bfloat16\n",
      "('decoder', 'layers', 'unshared_mlp_1', 'router_gate', 'kernel') (1,) bfloat16\n",
      "('decoder', 'layers', 'mlp_1', 'wi_0', 'kernel') (1,) bfloat16\n",
      "('decoder', 'layers', 'unshared_mlp_1', 'wi_0') (1,) bfloat16\n",
      "('decoder', 'layers', 'mlp_1', 'wi_1', 'kernel') (1,) bfloat16\n",
      "('decoder', 'layers', 'unshared_mlp_1', 'wi_1') (1,) bfloat16\n",
      "('decoder', 'layers', 'mlp_1', 'wo', 'kernel') (1,) bfloat16\n",
      "('decoder', 'layers', 'unshared_mlp_1', 'wo') (1,) bfloat16\n",
      "('decoder', 'layers', 'mlp_2', 'mgate', 'kernel') (1,) bfloat16\n",
      "('decoder', 'layers', 'unshared_mlp_2', 'mgate') (1,) bfloat16\n",
      "('decoder', 'layers', 'unshared_mlp_2', 'router_gate', 'kernel') (1,) bfloat16\n",
      "('decoder', 'layers', 'mlp_2', 'wi_0', 'kernel') (1,) bfloat16\n",
      "('decoder', 'layers', 'unshared_mlp_2', 'wi_0') (1,) bfloat16\n",
      "('decoder', 'layers', 'mlp_2', 'wi_1', 'kernel') (1,) bfloat16\n",
      "('decoder', 'layers', 'unshared_mlp_2', 'wi_1') (1,) bfloat16\n",
      "('decoder', 'layers', 'mlp_2', 'wo', 'kernel') (1,) bfloat16\n",
      "('decoder', 'layers', 'unshared_mlp_2', 'wo') (1,) bfloat16\n",
      "('decoder', 'layers', 'mlp_3', 'mgate', 'kernel') (1,) bfloat16\n",
      "('decoder', 'layers', 'unshared_mlp_3', 'mgate') (1,) bfloat16\n",
      "('decoder', 'layers', 'unshared_mlp_3', 'router_gate', 'kernel') (1,) bfloat16\n",
      "('decoder', 'layers', 'mlp_3', 'wi_0', 'kernel') (1,) bfloat16\n",
      "('decoder', 'layers', 'unshared_mlp_3', 'wi_0') (1,) bfloat16\n",
      "('decoder', 'layers', 'mlp_3', 'wi_1', 'kernel') (1,) bfloat16\n",
      "('decoder', 'layers', 'unshared_mlp_3', 'wi_1') (1,) bfloat16\n",
      "('decoder', 'layers', 'mlp_3', 'wo', 'kernel') (1,) bfloat16\n",
      "('decoder', 'layers', 'unshared_mlp_3', 'wo') (1,) bfloat16\n",
      "('decoder', 'layers', 'post_self_attention_layer_norm_0', 'scale') (1,) bfloat16\n",
      "('decoder', 'layers', 'post_self_attention_layer_norm_1', 'scale') (1,) bfloat16\n",
      "('decoder', 'layers', 'post_self_attention_layer_norm_2', 'scale') (1,) bfloat16\n",
      "('decoder', 'layers', 'post_self_attention_layer_norm_3', 'scale') (1,) bfloat16\n",
      "('decoder', 'layers', 'pre_self_attention_layer_norm_0', 'scale') (1,) bfloat16\n",
      "('decoder', 'layers', 'pre_self_attention_layer_norm_1', 'scale') (1,) bfloat16\n",
      "('decoder', 'layers', 'pre_self_attention_layer_norm_2', 'scale') (1,) bfloat16\n",
      "('decoder', 'layers', 'pre_self_attention_layer_norm_3', 'scale') (1,) bfloat16\n",
      "('decoder', 'layers', 'self_attention_0', 'AttentionOp_0', 'dyn_w_proj', 'dd', 'kernel') (1,) bfloat16\n",
      "('decoder', 'layers', 'self_attention_0', 'AttentionOp_0', 'dyn_w_proj', 'dw1', 'kernel') (1,) bfloat16\n",
      "('decoder', 'layers', 'self_attention_0', 'AttentionOp_0', 'dyn_w_proj', 'qkw') (1,) bfloat16\n",
      "('decoder', 'layers', 'self_attention_0', 'k_norm', 'scale') (1,) bfloat16\n",
      "('decoder', 'layers', 'self_attention_0', 'key', 'kernel') (1,) bfloat16\n",
      "('decoder', 'layers', 'self_attention_0', 'out', 'kernel') (1,) bfloat16\n",
      "('decoder', 'layers', 'self_attention_0', 'q_norm', 'scale') (1,) bfloat16\n",
      "('decoder', 'layers', 'self_attention_0', 'query', 'kernel') (1,) bfloat16\n",
      "('decoder', 'layers', 'self_attention_0', 'value', 'kernel') (1,) bfloat16\n",
      "('decoder', 'layers', 'self_attention_1', 'AttentionOp_0', 'dyn_w_proj', 'dd', 'kernel') (1,) bfloat16\n",
      "('decoder', 'layers', 'self_attention_1', 'AttentionOp_0', 'dyn_w_proj', 'dw1', 'kernel') (1,) bfloat16\n",
      "('decoder', 'layers', 'self_attention_1', 'AttentionOp_0', 'dyn_w_proj', 'qkw') (1,) bfloat16\n",
      "('decoder', 'layers', 'self_attention_1', 'k_norm', 'scale') (1,) bfloat16\n",
      "('decoder', 'layers', 'self_attention_1', 'key', 'kernel') (1,) bfloat16\n",
      "('decoder', 'layers', 'self_attention_1', 'out', 'kernel') (1,) bfloat16\n",
      "('decoder', 'layers', 'self_attention_1', 'q_norm', 'scale') (1,) bfloat16\n",
      "('decoder', 'layers', 'self_attention_1', 'query', 'kernel') (1,) bfloat16\n",
      "('decoder', 'layers', 'self_attention_1', 'value', 'kernel') (1,) bfloat16\n",
      "('decoder', 'layers', 'self_attention_2', 'AttentionOp_0', 'dyn_w_proj', 'dd', 'kernel') (1,) bfloat16\n",
      "('decoder', 'layers', 'self_attention_2', 'AttentionOp_0', 'dyn_w_proj', 'dw1', 'kernel') (1,) bfloat16\n",
      "('decoder', 'layers', 'self_attention_2', 'AttentionOp_0', 'dyn_w_proj', 'qkw') (1,) bfloat16\n",
      "('decoder', 'layers', 'self_attention_2', 'k_norm', 'scale') (1,) bfloat16\n",
      "('decoder', 'layers', 'self_attention_2', 'key', 'kernel') (1,) bfloat16\n",
      "('decoder', 'layers', 'self_attention_2', 'out', 'kernel') (1,) bfloat16\n",
      "('decoder', 'layers', 'self_attention_2', 'q_norm', 'scale') (1,) bfloat16\n",
      "('decoder', 'layers', 'self_attention_2', 'query', 'kernel') (1,) bfloat16\n",
      "('decoder', 'layers', 'self_attention_2', 'value', 'kernel') (1,) bfloat16\n",
      "('decoder', 'layers', 'self_attention_3', 'AttentionOp_0', 'dyn_w_proj', 'dd', 'kernel') (1,) bfloat16\n",
      "('decoder', 'layers', 'self_attention_3', 'AttentionOp_0', 'dyn_w_proj', 'dw1', 'kernel') (1,) bfloat16\n",
      "('decoder', 'layers', 'self_attention_3', 'AttentionOp_0', 'dyn_w_proj', 'qkw') (1,) bfloat16\n",
      "('decoder', 'layers', 'self_attention_3', 'k_norm', 'scale') (1,) bfloat16\n",
      "('decoder', 'layers', 'self_attention_3', 'key', 'kernel') (1,) bfloat16\n",
      "('decoder', 'layers', 'self_attention_3', 'out', 'kernel') (1,) bfloat16\n",
      "('decoder', 'layers', 'self_attention_3', 'q_norm', 'scale') (1,) bfloat16\n",
      "('decoder', 'layers', 'self_attention_3', 'query', 'kernel') (1,) bfloat16\n",
      "('decoder', 'layers', 'self_attention_3', 'value', 'kernel') (1,) bfloat16\n",
      "('decoder', 'logits_dense', 'kernel') (1,) bfloat16\n",
      "('token_embedder', 'embedding') (1,) bfloat16\n"
     ]
    }
   ],
   "source": [
    "# _METADATA, checkpoint三个文件转移到实际保存的checkpoint文件夹中进行替换\n",
    "start_time = time.time()\n",
    "mlp_dim = 5632 // scale\n",
    "moe_params = {}\n",
    "for k, v in xm3p5_w.items():\n",
    "    v = jnp.array([100]).astype(jnp.bfloat16)\n",
    "    print(f'k: {k} take: {time.time() - start_time:.3f}s')\n",
    "    # if 'mgate' not in k:\n",
    "    moe_params[k] = v\n",
    "    if 'decoder.layers.mlp_' in k:\n",
    "        mlp_inx = k.find('mlp_')\n",
    "        l = k[mlp_inx+4: mlp_inx+5]\n",
    "        # if int(l) % 2 != 0: continue\n",
    "        unshared_mlp = k.replace('decoder.layers.mlp_', 'decoder.layers.unshared_mlp_')\n",
    "        unshared_mlp = unshared_mlp.replace('.kernel', '')\n",
    "        moe_params[unshared_mlp] = v\n",
    "        if 'mgate' in unshared_mlp:\n",
    "            router_gate = unshared_mlp.replace('mgate', 'router_gate.kernel')\n",
    "            moe_params[router_gate] = v\n",
    "moe_params = {tuple(k.split('.')): v for k,v in moe_params.items()}\n",
    "example_step = 8\n",
    "save_params(example_step, save_dir, moe_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49309a37-7a35-4ebf-8ab5-d1a3cb9b37e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying gs://llm_base_models_europe-west4/v5p_256/7B/xm_45x7B_moe_129/checkpoints/8/state/_METADATA [Content-Type=application/octet-stream]...\n",
      "/ [1 files][ 34.0 KiB/ 34.0 KiB]                                                \n",
      "Operation completed over 1 objects/34.0 KiB.                                     \n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "# save_dir1 = str(save_dir).rstrip('/')\n",
    "source_dir = 'gs://llm_base_models_europe-west4/v5p_256/7B/xm_45x7B_moe_129/checkpoints'\n",
    "target_dir = 'gs://llm_base_models_europe-west4/v5p_256/7B/xm_45x7B_moe_129/checkpoints'\n",
    "source_step = example_step\n",
    "command = f'gsutil cp {source_dir}/{source_step}/state/_METADATA {target_dir}/0/state/ '\n",
    "r = subprocess.run(command, stdout=subprocess.PIPE, shell=True)\n",
    "\n",
    "command = f'gsutil cp {source_dir}/{source_step}/state/checkpoint {target_dir}/0/state/ '\n",
    "r = subprocess.run(command, stdout=subprocess.PIPE, shell=True)\n",
    "\n",
    "# command = f'gsutil cp {source_dir}/{source_step}/state/_sharding {target_dir}/0/state/ '\n",
    "# r = subprocess.run(command, stdout=subprocess.PIPE, shell=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fefc664d-fd66-40f6-a212-6e6a9592453f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "core_nums: 32\n"
     ]
    }
   ],
   "source": [
    "# 基于tpu type 构建_sharding文件\n",
    "import base64\n",
    "\n",
    "def decode_base64(encoded_str):\n",
    "    decoded_bytes = base64.b64decode(encoded_str)\n",
    "    decoded_str = decoded_bytes.decode('utf-8')\n",
    "    return decoded_str\n",
    "\n",
    "def encode_base64(decoded_str):\n",
    "    # decoded_str = \"opt_state.mu.params.token_embedder.embedding\"\n",
    "    encoded_string = base64.b64encode(decoded_str.encode('utf-8')).decode('utf-8')\n",
    "    return encoded_string\n",
    "\n",
    "'''\n",
    "_sharding文件格式如下：\n",
    "{\n",
    "  b3B0X3N0YXRlLm11LnBhcmFtcy50b2tlbl9lbWJlZGRlci5lbWJlZGRpbmc=': {'sharding_type': 'NamedSharding',\n",
    "  'shape': [1, 1, 4, 1, 1, 1, 1],\n",
    "  'axis_names': ['data', 'stage', 'fsdp', 'fsdp_transpose', 'sequence', 'tensor','autoregressive'],\n",
    "  'partition_spec': [['tensor', 'autoregressive'], ['fsdp', 'fsdp_transpose', 'sequence']],\n",
    "   2: 4},\n",
    "   ...\n",
    "   }\n",
    "   '''\n",
    "# moe sharding\n",
    "_sharding_path = 'gs://llm_base_models_us-east5/v5p_256/7B/xm_45x7B_moe_1017/xm3p5_moe_params_no_opt_v5p_64_sharding.copy'\n",
    "_sharding_path = epath.Path(_sharding_path)\n",
    "# 读取已有的_sharding文件\n",
    "with _sharding_path.open('r') as f:\n",
    "    _sharding = json.load(f)\n",
    "\n",
    "tpu_type = 'v5p-64'\n",
    "core_nums = int(tpu_type.split('-')[-1])\n",
    "if 'v3' not in tpu_type:\n",
    "    core_nums = core_nums // 2\n",
    "print(f'core_nums: {core_nums}')\n",
    "updated_sharding = {}\n",
    "for k, v in _sharding.items():\n",
    "    v = json.loads(v)\n",
    "    v['shape'][2] = core_nums\n",
    "    base_k = decode_base64(k)\n",
    "    updated_sharding[k] = json.dumps(v)\n",
    "    if 'unshared_mlp_0' in base_k: # 因为已有的sharding文件是隔层moe，因此需要进行扩展\n",
    "        unshared_mlp_1 = base_k.replace('unshared_mlp_0', 'unshared_mlp_1')\n",
    "        unshared_mlp_3 = base_k.replace('unshared_mlp_0', 'unshared_mlp_3')\n",
    "        encode_unshared_mlp_1 = encode_base64(unshared_mlp_1)\n",
    "        encode_unshared_mlp_3 = encode_base64(unshared_mlp_3)\n",
    "        print(f'encode_unshared_mlp_1: {unshared_mlp_1}')\n",
    "        print(f'encode_unshared_mlp_3: {unshared_mlp_3}')\n",
    "        \n",
    "        updated_sharding[encode_unshared_mlp_1] = json.dumps(v)\n",
    "        updated_sharding[encode_unshared_mlp_3] = json.dumps(v)\n",
    "    \n",
    "updated_sharding_path = f'{save_dir}/0/state/_sharding'\n",
    "\n",
    "updated_sharding_path = epath.Path(updated_sharding_path)\n",
    "with updated_sharding_path.open('w') as f:\n",
    "    json.dump(updated_sharding, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5824db16-340b-4b31-bbe2-7eab1dbb2c4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "73"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# p = 'gs://llm_base_models_us-east5/v5p_256/7B/xm_45x7B_moe_1017/checkpoints/0/state/_METADATA'\n",
    "# # p = 'gs://llm_base_models_us-east5/v5p_256/7B/moe_test/checkpoints/0/state/_METADATA'\n",
    "# # p = 'gs://llm_base_models_us-east5/v5p_256/7B/moe_test/checkpoints/0/state/checkpoint'\n",
    "# p = epath.Path(p)\n",
    "\n",
    "# structure_path = p\n",
    "# msgpack = ocp.aggregate_handlers.MsgpackHandler(0)\n",
    "# structure = msgpack.deserialize(structure_path)\n",
    "\n",
    "# p = 'gs://llm_base_models_us-east5/v5p_256/7B/xm_45x7B_moe_1017/checkpoints/6/state/_METADATA'\n",
    "# # p = 'gs://llm_base_models_us-east5/v5p_256/7B/moe_test/checkpoints/0/state/_METADATA'\n",
    "\n",
    "# p = epath.Path(p)\n",
    "# with p.open('r') as f:\n",
    "#     meta = json.load(f)\n",
    "\n",
    "# def decode_base64(encoded_str):\n",
    "#     decoded_bytes = base64.b64decode(encoded_str)\n",
    "#     decoded_str = decoded_bytes.decode('utf-8')\n",
    "#     return decoded_str\n",
    "\n",
    "# ps = []\n",
    "# for k, v in updated_sharding.items():\n",
    "#     a = decode_base64(k)\n",
    "#     if 'opt_state' in a or 'step' in a: continue\n",
    "#     ps.append(a)\n",
    "#     print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e175c998-19f8-445c-80b2-2de3db8f9d0c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
