{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "189aae90-3a8e-4484-8fa8-e4d9ee4ee2ae",
   "metadata": {},
   "source": [
    "### 说明\n",
    "- 基于440000预训练的dense模型进行初始化，1个共享专家，8个非共享专家，每层都插入moe，共享专家直接复制dense的mlp，有mgate，添加了router_gate，并随机初始化\n",
    "- 其次，采用openmoe的实现，并设置容量因子为200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de3ffed5-3fae-41c4-8f30-738bb173436d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-03 10:04:31.993562: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:479] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-12-03 10:04:32.009004: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:10575] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-12-03 10:04:32.009031: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1442] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-03 10:04:32.730690: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Old structure file keys: dict_keys(['params'])\n",
      "New structure file keys: dict_keys(['params'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1733220279.349766   59234 gcs_resource.cc:109] Using default AdmissionQueue with limit 32\n",
      "I0000 00:00:1733220279.353753   59806 google_auth_provider.cc:180] Running on GCE, using service account 887571727717-compute@developer.gserviceaccount.com\n"
     ]
    }
   ],
   "source": [
    "# !pip install tensorflow==2.16.1\n",
    "# pip install numpy==1.26.4\n",
    "# 运行2遍\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import asyncio\n",
    "import argparse\n",
    "from collections import defaultdict\n",
    "import time\n",
    "import subprocess\n",
    "\n",
    "os.environ[\"JAX_PLATFORMS\"] = \"cpu\"\n",
    "from etils import epath\n",
    "import json\n",
    "import base64\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import jax.numpy as jnp\n",
    "import jax\n",
    "import orbax\n",
    "import orbax.checkpoint as ocp\n",
    "from etils import epath\n",
    "from jax.sharding import PartitionSpec as PS\n",
    "from flax.traverse_util import flatten_dict, unflatten_dict\n",
    "\n",
    "\n",
    "METADATA_FILE = '_METADATA'\n",
    "_CHECKPOINT_FILE = 'checkpoint'\n",
    "\n",
    "\n",
    "read_dir = 'gs://llm_base_models_europe-west4/v5p_256/7B/PileDCSlimLlama7B32Kx4x256x1v5p_0713/checkpoints/440000/state'\n",
    "save_dir = 'gs://llm_base_models_europe-west4/v5p_256/7B/xm_E8x7B_ShareWithMgate_UnshareWithMgate_AllCopy_1204/checkpoints/'\n",
    "\n",
    "read_dir = epath.Path(read_dir) \n",
    "save_dir = epath.Path(save_dir)\n",
    "\n",
    "metadata_path = read_dir / METADATA_FILE\n",
    "back_metadata_path = read_dir / f'{METADATA_FILE}.back'\n",
    "try:\n",
    "    metadata_path.rename(back_metadata_path)\n",
    "except:\n",
    "    pass\n",
    "metadata_path.unlink(missing_ok=True) # delete\n",
    "structure_path = read_dir / _CHECKPOINT_FILE\n",
    "msgpack = ocp.aggregate_handlers.MsgpackHandler(0)\n",
    "structure = msgpack.deserialize(structure_path)\n",
    "# backup original checkpoint fil\n",
    "back_structure_path = read_dir / 'checkpoint_back'\n",
    "back_structure = structure.copy()\n",
    "if not back_structure_path.exists():\n",
    "    asyncio.run(msgpack.serialize(back_structure_path, item=back_structure))\n",
    "print(f'Old structure file keys: {structure.keys()}')\n",
    "remove_keys = ['opt_state', 'step'] # select the weight name you don't want to load, all weight name: opt_state, step, params\n",
    "_ = [structure.pop(key) for key in remove_keys if key in structure]\n",
    "print(f'New structure file keys: {structure.keys()}')\n",
    "asyncio.run(msgpack.serialize(structure_path, item=structure))  # rewrite struct file\n",
    "\n",
    "# load model based struct, note: axes must same as training\n",
    "mesh_axes = ['data', 'stage', 'fsdp', 'fsdp_transpose', 'sequence', 'tensor', 'autoregressive']\n",
    "devices = np.asarray(jax.devices()).reshape([1] * len(mesh_axes))\n",
    "mesh = jax.sharding.Mesh(devices, mesh_axes)\n",
    "sharding = jax.sharding.NamedSharding(mesh, PS()) # Sharding is None because we use cpu to load weights\n",
    "weight_dtype = jnp.bfloat16 # set restore weights dtype\n",
    "restore_args = {}\n",
    "for k, v in flatten_dict(structure).items():\n",
    "    restore_args[k] =  ocp.ArrayRestoreArgs(restore_type=jax.Array, dtype=weight_dtype, sharding=sharding)\n",
    "restore_args = unflatten_dict(restore_args)\n",
    "ckptr = ocp.Checkpointer(ocp.PyTreeCheckpointHandler())\n",
    "w = ckptr.restore(read_dir, args=ocp.args.PyTreeRestore(restore_args=restore_args))\n",
    "structure_path = read_dir / _CHECKPOINT_FILE\n",
    "# rewrite struct file, otherwise occur error when continue training\n",
    "asyncio.run(msgpack.serialize(structure_path, item=back_structure))\n",
    "while 'params' in w:\n",
    "    w = w['params']\n",
    "xm3p5_w = {'.'.join(k): np.array(v) for k, v in flatten_dict(w).items()}\n",
    "\n",
    "try:\n",
    "    back_metadata_path.rename(metadata_path)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be5f1d28-e8a4-4ef1-ab52-c98d282d0824",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Configured `CheckpointManager` using deprecated legacy API. Please follow the instructions at https://orbax.readthedocs.io/en/latest/api_refactor.html to migrate by August 1st, 2024.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('decoder', 'decoder_norm', 'scale') (4096,) bfloat16\n",
      "('decoder', 'layers', 'mlp_0', 'mgate', 'kernel') (4096, 12, 44) bfloat16\n",
      "('decoder', 'layers', 'mlp_0', 'wi_0', 'kernel') (4096, 12, 5632) bfloat16\n",
      "('decoder', 'layers', 'mlp_0', 'wi_1', 'kernel') (4096, 12, 5632) bfloat16\n",
      "('decoder', 'layers', 'mlp_0', 'wo', 'kernel') (5632, 12, 4096) bfloat16\n",
      "('decoder', 'layers', 'mlp_1', 'mgate', 'kernel') (4096, 12, 44) bfloat16\n",
      "('decoder', 'layers', 'mlp_1', 'wi_0', 'kernel') (4096, 12, 5632) bfloat16\n",
      "('decoder', 'layers', 'mlp_1', 'wi_1', 'kernel') (4096, 12, 5632) bfloat16\n",
      "('decoder', 'layers', 'mlp_1', 'wo', 'kernel') (5632, 12, 4096) bfloat16\n",
      "('decoder', 'layers', 'mlp_2', 'mgate', 'kernel') (4096, 12, 44) bfloat16\n",
      "('decoder', 'layers', 'mlp_2', 'wi_0', 'kernel') (4096, 12, 5632) bfloat16\n",
      "('decoder', 'layers', 'mlp_2', 'wi_1', 'kernel') (4096, 12, 5632) bfloat16\n",
      "('decoder', 'layers', 'mlp_2', 'wo', 'kernel') (5632, 12, 4096) bfloat16\n",
      "('decoder', 'layers', 'mlp_3', 'mgate', 'kernel') (4096, 12, 44) bfloat16\n",
      "('decoder', 'layers', 'mlp_3', 'wi_0', 'kernel') (4096, 12, 5632) bfloat16\n",
      "('decoder', 'layers', 'mlp_3', 'wi_1', 'kernel') (4096, 12, 5632) bfloat16\n",
      "('decoder', 'layers', 'mlp_3', 'wo', 'kernel') (5632, 12, 4096) bfloat16\n",
      "('decoder', 'layers', 'post_self_attention_layer_norm_0', 'scale') (4096, 12) bfloat16\n",
      "('decoder', 'layers', 'post_self_attention_layer_norm_1', 'scale') (4096, 12) bfloat16\n",
      "('decoder', 'layers', 'post_self_attention_layer_norm_2', 'scale') (4096, 12) bfloat16\n",
      "('decoder', 'layers', 'post_self_attention_layer_norm_3', 'scale') (4096, 12) bfloat16\n",
      "('decoder', 'layers', 'pre_self_attention_layer_norm_0', 'scale') (4096, 12) bfloat16\n",
      "('decoder', 'layers', 'pre_self_attention_layer_norm_1', 'scale') (4096, 12) bfloat16\n",
      "('decoder', 'layers', 'pre_self_attention_layer_norm_2', 'scale') (4096, 12) bfloat16\n",
      "('decoder', 'layers', 'pre_self_attention_layer_norm_3', 'scale') (4096, 12) bfloat16\n",
      "('decoder', 'layers', 'self_attention_0', 'AttentionOp_0', 'dyn_w_proj', 'dd', 'kernel') (4096, 12, 1, 128) bfloat16\n",
      "('decoder', 'layers', 'self_attention_0', 'AttentionOp_0', 'dyn_w_proj', 'dw1', 'kernel') (4096, 12, 1, 4, 128) bfloat16\n",
      "('decoder', 'layers', 'self_attention_0', 'AttentionOp_0', 'dyn_w_proj', 'qkw') (1, 12, 4, 128, 4, 32) bfloat16\n",
      "('decoder', 'layers', 'self_attention_0', 'k_norm', 'scale') (128, 12) bfloat16\n",
      "('decoder', 'layers', 'self_attention_0', 'key', 'kernel') (4096, 12, 32, 128) bfloat16\n",
      "('decoder', 'layers', 'self_attention_0', 'out', 'kernel') (32, 12, 128, 4096) bfloat16\n",
      "('decoder', 'layers', 'self_attention_0', 'q_norm', 'scale') (128, 12) bfloat16\n",
      "('decoder', 'layers', 'self_attention_0', 'query', 'kernel') (4096, 12, 32, 128) bfloat16\n",
      "('decoder', 'layers', 'self_attention_0', 'value', 'kernel') (4096, 12, 32, 128) bfloat16\n",
      "('decoder', 'layers', 'self_attention_1', 'AttentionOp_0', 'dyn_w_proj', 'dd', 'kernel') (4096, 12, 1, 128) bfloat16\n",
      "('decoder', 'layers', 'self_attention_1', 'AttentionOp_0', 'dyn_w_proj', 'dw1', 'kernel') (4096, 12, 1, 4, 128) bfloat16\n",
      "('decoder', 'layers', 'self_attention_1', 'AttentionOp_0', 'dyn_w_proj', 'qkw') (1, 12, 4, 128, 4, 32) bfloat16\n",
      "('decoder', 'layers', 'self_attention_1', 'k_norm', 'scale') (128, 12) bfloat16\n",
      "('decoder', 'layers', 'self_attention_1', 'key', 'kernel') (4096, 12, 32, 128) bfloat16\n",
      "('decoder', 'layers', 'self_attention_1', 'out', 'kernel') (32, 12, 128, 4096) bfloat16\n",
      "('decoder', 'layers', 'self_attention_1', 'q_norm', 'scale') (128, 12) bfloat16\n",
      "('decoder', 'layers', 'self_attention_1', 'query', 'kernel') (4096, 12, 32, 128) bfloat16\n",
      "('decoder', 'layers', 'self_attention_1', 'value', 'kernel') (4096, 12, 32, 128) bfloat16\n",
      "('decoder', 'layers', 'self_attention_2', 'AttentionOp_0', 'dyn_w_proj', 'dd', 'kernel') (4096, 12, 1, 128) bfloat16\n",
      "('decoder', 'layers', 'self_attention_2', 'AttentionOp_0', 'dyn_w_proj', 'dw1', 'kernel') (4096, 12, 1, 4, 128) bfloat16\n",
      "('decoder', 'layers', 'self_attention_2', 'AttentionOp_0', 'dyn_w_proj', 'qkw') (1, 12, 4, 128, 4, 32) bfloat16\n",
      "('decoder', 'layers', 'self_attention_2', 'k_norm', 'scale') (128, 12) bfloat16\n",
      "('decoder', 'layers', 'self_attention_2', 'key', 'kernel') (4096, 12, 32, 128) bfloat16\n",
      "('decoder', 'layers', 'self_attention_2', 'out', 'kernel') (32, 12, 128, 4096) bfloat16\n",
      "('decoder', 'layers', 'self_attention_2', 'q_norm', 'scale') (128, 12) bfloat16\n",
      "('decoder', 'layers', 'self_attention_2', 'query', 'kernel') (4096, 12, 32, 128) bfloat16\n",
      "('decoder', 'layers', 'self_attention_2', 'value', 'kernel') (4096, 12, 32, 128) bfloat16\n",
      "('decoder', 'layers', 'self_attention_3', 'AttentionOp_0', 'dyn_w_proj', 'dd', 'kernel') (4096, 12, 1, 128) bfloat16\n",
      "('decoder', 'layers', 'self_attention_3', 'AttentionOp_0', 'dyn_w_proj', 'dw1', 'kernel') (4096, 12, 1, 4, 128) bfloat16\n",
      "('decoder', 'layers', 'self_attention_3', 'AttentionOp_0', 'dyn_w_proj', 'qkw') (1, 12, 4, 128, 4, 32) bfloat16\n",
      "('decoder', 'layers', 'self_attention_3', 'k_norm', 'scale') (128, 12) bfloat16\n",
      "('decoder', 'layers', 'self_attention_3', 'key', 'kernel') (4096, 12, 32, 128) bfloat16\n",
      "('decoder', 'layers', 'self_attention_3', 'out', 'kernel') (32, 12, 128, 4096) bfloat16\n",
      "('decoder', 'layers', 'self_attention_3', 'q_norm', 'scale') (128, 12) bfloat16\n",
      "('decoder', 'layers', 'self_attention_3', 'query', 'kernel') (4096, 12, 32, 128) bfloat16\n",
      "('decoder', 'layers', 'self_attention_3', 'value', 'kernel') (4096, 12, 32, 128) bfloat16\n",
      "('decoder', 'logits_dense', 'kernel') (4096, 152064) bfloat16\n",
      "('token_embedder', 'embedding') (152064, 4096) bfloat16\n",
      "('decoder', 'decoder_norm', 'scale') (4096,) bfloat16\n",
      "('decoder', 'layers', 'mlp_0', 'mgate', 'kernel') (4096, 12, 44) bfloat16\n",
      "('decoder', 'layers', 'mlp_0', 'wi_0', 'kernel') (4096, 12, 5632) bfloat16\n",
      "('decoder', 'layers', 'mlp_0', 'wi_1', 'kernel') (4096, 12, 5632) bfloat16\n",
      "('decoder', 'layers', 'mlp_0', 'wo', 'kernel') (5632, 12, 4096) bfloat16\n",
      "('decoder', 'layers', 'mlp_1', 'mgate', 'kernel') (4096, 12, 44) bfloat16\n",
      "('decoder', 'layers', 'mlp_1', 'wi_0', 'kernel') (4096, 12, 5632) bfloat16\n",
      "('decoder', 'layers', 'mlp_1', 'wi_1', 'kernel') (4096, 12, 5632) bfloat16\n",
      "('decoder', 'layers', 'mlp_1', 'wo', 'kernel') (5632, 12, 4096) bfloat16\n",
      "('decoder', 'layers', 'mlp_2', 'mgate', 'kernel') (4096, 12, 44) bfloat16\n",
      "('decoder', 'layers', 'mlp_2', 'wi_0', 'kernel') (4096, 12, 5632) bfloat16\n",
      "('decoder', 'layers', 'mlp_2', 'wi_1', 'kernel') (4096, 12, 5632) bfloat16\n",
      "('decoder', 'layers', 'mlp_2', 'wo', 'kernel') (5632, 12, 4096) bfloat16\n",
      "('decoder', 'layers', 'mlp_3', 'mgate', 'kernel') (4096, 12, 44) bfloat16\n",
      "('decoder', 'layers', 'mlp_3', 'wi_0', 'kernel') (4096, 12, 5632) bfloat16\n",
      "('decoder', 'layers', 'mlp_3', 'wi_1', 'kernel') (4096, 12, 5632) bfloat16\n",
      "('decoder', 'layers', 'mlp_3', 'wo', 'kernel') (5632, 12, 4096) bfloat16\n",
      "('decoder', 'layers', 'post_self_attention_layer_norm_0', 'scale') (4096, 12) bfloat16\n",
      "('decoder', 'layers', 'post_self_attention_layer_norm_1', 'scale') (4096, 12) bfloat16\n",
      "('decoder', 'layers', 'post_self_attention_layer_norm_2', 'scale') (4096, 12) bfloat16\n",
      "('decoder', 'layers', 'post_self_attention_layer_norm_3', 'scale') (4096, 12) bfloat16\n",
      "('decoder', 'layers', 'pre_self_attention_layer_norm_0', 'scale') (4096, 12) bfloat16\n",
      "('decoder', 'layers', 'pre_self_attention_layer_norm_1', 'scale') (4096, 12) bfloat16\n",
      "('decoder', 'layers', 'pre_self_attention_layer_norm_2', 'scale') (4096, 12) bfloat16\n",
      "('decoder', 'layers', 'pre_self_attention_layer_norm_3', 'scale') (4096, 12) bfloat16\n",
      "('decoder', 'layers', 'self_attention_0', 'AttentionOp_0', 'dyn_w_proj', 'dd', 'kernel') (4096, 12, 1, 128) bfloat16\n",
      "('decoder', 'layers', 'self_attention_0', 'AttentionOp_0', 'dyn_w_proj', 'dw1', 'kernel') (4096, 12, 1, 4, 128) bfloat16\n",
      "('decoder', 'layers', 'self_attention_0', 'AttentionOp_0', 'dyn_w_proj', 'qkw') (1, 12, 4, 128, 4, 32) bfloat16\n",
      "('decoder', 'layers', 'self_attention_0', 'k_norm', 'scale') (128, 12) bfloat16\n",
      "('decoder', 'layers', 'self_attention_0', 'key', 'kernel') (4096, 12, 32, 128) bfloat16\n",
      "('decoder', 'layers', 'self_attention_0', 'out', 'kernel') (32, 12, 128, 4096) bfloat16\n",
      "('decoder', 'layers', 'self_attention_0', 'q_norm', 'scale') (128, 12) bfloat16\n",
      "('decoder', 'layers', 'self_attention_0', 'query', 'kernel') (4096, 12, 32, 128) bfloat16\n",
      "('decoder', 'layers', 'self_attention_0', 'value', 'kernel') (4096, 12, 32, 128) bfloat16\n",
      "('decoder', 'layers', 'self_attention_1', 'AttentionOp_0', 'dyn_w_proj', 'dd', 'kernel') (4096, 12, 1, 128) bfloat16\n",
      "('decoder', 'layers', 'self_attention_1', 'AttentionOp_0', 'dyn_w_proj', 'dw1', 'kernel') (4096, 12, 1, 4, 128) bfloat16\n",
      "('decoder', 'layers', 'self_attention_1', 'AttentionOp_0', 'dyn_w_proj', 'qkw') (1, 12, 4, 128, 4, 32) bfloat16\n",
      "('decoder', 'layers', 'self_attention_1', 'k_norm', 'scale') (128, 12) bfloat16\n",
      "('decoder', 'layers', 'self_attention_1', 'key', 'kernel') (4096, 12, 32, 128) bfloat16\n",
      "('decoder', 'layers', 'self_attention_1', 'out', 'kernel') (32, 12, 128, 4096) bfloat16\n",
      "('decoder', 'layers', 'self_attention_1', 'q_norm', 'scale') (128, 12) bfloat16\n",
      "('decoder', 'layers', 'self_attention_1', 'query', 'kernel') (4096, 12, 32, 128) bfloat16\n",
      "('decoder', 'layers', 'self_attention_1', 'value', 'kernel') (4096, 12, 32, 128) bfloat16\n",
      "('decoder', 'layers', 'self_attention_2', 'AttentionOp_0', 'dyn_w_proj', 'dd', 'kernel') (4096, 12, 1, 128) bfloat16\n",
      "('decoder', 'layers', 'self_attention_2', 'AttentionOp_0', 'dyn_w_proj', 'dw1', 'kernel') (4096, 12, 1, 4, 128) bfloat16\n",
      "('decoder', 'layers', 'self_attention_2', 'AttentionOp_0', 'dyn_w_proj', 'qkw') (1, 12, 4, 128, 4, 32) bfloat16\n",
      "('decoder', 'layers', 'self_attention_2', 'k_norm', 'scale') (128, 12) bfloat16\n",
      "('decoder', 'layers', 'self_attention_2', 'key', 'kernel') (4096, 12, 32, 128) bfloat16\n",
      "('decoder', 'layers', 'self_attention_2', 'out', 'kernel') (32, 12, 128, 4096) bfloat16\n",
      "('decoder', 'layers', 'self_attention_2', 'q_norm', 'scale') (128, 12) bfloat16\n",
      "('decoder', 'layers', 'self_attention_2', 'query', 'kernel') (4096, 12, 32, 128) bfloat16\n",
      "('decoder', 'layers', 'self_attention_2', 'value', 'kernel') (4096, 12, 32, 128) bfloat16\n",
      "('decoder', 'layers', 'self_attention_3', 'AttentionOp_0', 'dyn_w_proj', 'dd', 'kernel') (4096, 12, 1, 128) bfloat16\n",
      "('decoder', 'layers', 'self_attention_3', 'AttentionOp_0', 'dyn_w_proj', 'dw1', 'kernel') (4096, 12, 1, 4, 128) bfloat16\n",
      "('decoder', 'layers', 'self_attention_3', 'AttentionOp_0', 'dyn_w_proj', 'qkw') (1, 12, 4, 128, 4, 32) bfloat16\n",
      "('decoder', 'layers', 'self_attention_3', 'k_norm', 'scale') (128, 12) bfloat16\n",
      "('decoder', 'layers', 'self_attention_3', 'key', 'kernel') (4096, 12, 32, 128) bfloat16\n",
      "('decoder', 'layers', 'self_attention_3', 'out', 'kernel') (32, 12, 128, 4096) bfloat16\n",
      "('decoder', 'layers', 'self_attention_3', 'q_norm', 'scale') (128, 12) bfloat16\n",
      "('decoder', 'layers', 'self_attention_3', 'query', 'kernel') (4096, 12, 32, 128) bfloat16\n",
      "('decoder', 'layers', 'self_attention_3', 'value', 'kernel') (4096, 12, 32, 128) bfloat16\n",
      "('decoder', 'logits_dense', 'kernel') (4096, 152064) bfloat16\n",
      "('token_embedder', 'embedding') (152064, 4096) bfloat16\n"
     ]
    }
   ],
   "source": [
    "## 基于dense模型保存和moe相同名字的参数\n",
    "def convert_to_jnp(params, remove_keys=[]):\n",
    "    convert_params = {}\n",
    "    for k, v in params.items():\n",
    "        r = 0\n",
    "        for remove_key in remove_keys:\n",
    "            if remove_key in k: \n",
    "                r = 1\n",
    "                break\n",
    "        if r: continue\n",
    "        k = tuple(k.split('.'))\n",
    "        convert_params[k] = v\n",
    "        # convert_params[k] = jnp.array(v).astype(jnp.bfloat16)\n",
    "    for k, v in convert_params.items():\n",
    "        print(k, v.shape, v.dtype)\n",
    "    return convert_params\n",
    "\n",
    "\n",
    "def save_params(step, save_dir, params):\n",
    "    item = {\n",
    "            'state': orbax.checkpoint.AsyncCheckpointer(orbax.checkpoint.PyTreeCheckpointHandler(use_ocdbt=False)),\n",
    "                    }\n",
    "    unflatten_params = unflatten_dict(params)\n",
    "    for k, v in params.items():\n",
    "        print(k, v.shape, v.dtype)\n",
    "    mngr = orbax.checkpoint.CheckpointManager(save_dir, item)\n",
    "    if 'params' not in unflatten_params: unflatten_params = {'params': unflatten_params}\n",
    "    mngr.save(step, items={'state': {'params': unflatten_params}})\n",
    "\n",
    "params_save_step = 0\n",
    "convert_params = convert_to_jnp(xm3p5_w, remove_keys=[])\n",
    "save_params(params_save_step, save_dir, convert_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b2990320-0a71-462d-af42-c33109d94a7b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "router_gate: decoder.layers.unshared_mlp_0.router_gate.kernel init_v: (4096, 12, 8)\n",
      "router_gate: decoder.layers.unshared_mlp_1.router_gate.kernel init_v: (4096, 12, 8)\n",
      "router_gate: decoder.layers.unshared_mlp_2.router_gate.kernel init_v: (4096, 12, 8)\n",
      "router_gate: decoder.layers.unshared_mlp_3.router_gate.kernel init_v: (4096, 12, 8)\n",
      "('decoder', 'layers', 'unshared_mlp_0', 'mgate') (8, 12, 4096, 44) -2.78125 -2.78125\n",
      "('decoder', 'layers', 'unshared_mlp_0', 'router_gate', 'kernel') (4096, 12, 8) -0.3125 0.131836\n",
      "('decoder', 'layers', 'unshared_mlp_0', 'wi_0') (8, 12, 4096, 5632) 4192 4192\n",
      "('decoder', 'layers', 'unshared_mlp_0', 'wi_1') (8, 12, 4096, 5632) 442 442\n",
      "('decoder', 'layers', 'unshared_mlp_0', 'wo') (8, 12, 5632, 4096) 42.5 42.5\n",
      "('decoder', 'layers', 'unshared_mlp_1', 'mgate') (8, 12, 4096, 44) 50 50\n",
      "('decoder', 'layers', 'unshared_mlp_1', 'router_gate', 'kernel') (4096, 12, 8) -0.248047 -0.21582\n",
      "('decoder', 'layers', 'unshared_mlp_1', 'wi_0') (8, 12, 4096, 5632) 5408 5408\n",
      "('decoder', 'layers', 'unshared_mlp_1', 'wi_1') (8, 12, 4096, 5632) -125 -125\n",
      "('decoder', 'layers', 'unshared_mlp_1', 'wo') (8, 12, 5632, 4096) 262 262\n",
      "('decoder', 'layers', 'unshared_mlp_2', 'mgate') (8, 12, 4096, 44) 29.125 29.125\n",
      "('decoder', 'layers', 'unshared_mlp_2', 'router_gate', 'kernel') (4096, 12, 8) -0.322266 0.101562\n",
      "('decoder', 'layers', 'unshared_mlp_2', 'wi_0') (8, 12, 4096, 5632) 3440 3440\n",
      "('decoder', 'layers', 'unshared_mlp_2', 'wi_1') (8, 12, 4096, 5632) -588 -588\n",
      "('decoder', 'layers', 'unshared_mlp_2', 'wo') (8, 12, 5632, 4096) 528 528\n",
      "('decoder', 'layers', 'unshared_mlp_3', 'mgate') (8, 12, 4096, 44) 5.71875 5.71875\n",
      "('decoder', 'layers', 'unshared_mlp_3', 'router_gate', 'kernel') (4096, 12, 8) -0.158203 -0.108887\n",
      "('decoder', 'layers', 'unshared_mlp_3', 'wi_0') (8, 12, 4096, 5632) 5888 5888\n",
      "('decoder', 'layers', 'unshared_mlp_3', 'wi_1') (8, 12, 4096, 5632) 60.75 60.75\n",
      "('decoder', 'layers', 'unshared_mlp_3', 'wo') (8, 12, 5632, 4096) -162 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Configured `CheckpointManager` using deprecated legacy API. Please follow the instructions at https://orbax.readthedocs.io/en/latest/api_refactor.html to migrate by August 1st, 2024.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-162\n",
      "('decoder', 'layers', 'unshared_mlp_0', 'mgate') (8, 12, 4096, 44) bfloat16\n",
      "('decoder', 'layers', 'unshared_mlp_0', 'router_gate', 'kernel') (4096, 12, 8) bfloat16\n",
      "('decoder', 'layers', 'unshared_mlp_0', 'wi_0') (8, 12, 4096, 5632) bfloat16\n",
      "('decoder', 'layers', 'unshared_mlp_0', 'wi_1') (8, 12, 4096, 5632) bfloat16\n",
      "('decoder', 'layers', 'unshared_mlp_0', 'wo') (8, 12, 5632, 4096) bfloat16\n",
      "('decoder', 'layers', 'unshared_mlp_1', 'mgate') (8, 12, 4096, 44) bfloat16\n",
      "('decoder', 'layers', 'unshared_mlp_1', 'router_gate', 'kernel') (4096, 12, 8) bfloat16\n",
      "('decoder', 'layers', 'unshared_mlp_1', 'wi_0') (8, 12, 4096, 5632) bfloat16\n",
      "('decoder', 'layers', 'unshared_mlp_1', 'wi_1') (8, 12, 4096, 5632) bfloat16\n",
      "('decoder', 'layers', 'unshared_mlp_1', 'wo') (8, 12, 5632, 4096) bfloat16\n",
      "('decoder', 'layers', 'unshared_mlp_2', 'mgate') (8, 12, 4096, 44) bfloat16\n",
      "('decoder', 'layers', 'unshared_mlp_2', 'router_gate', 'kernel') (4096, 12, 8) bfloat16\n",
      "('decoder', 'layers', 'unshared_mlp_2', 'wi_0') (8, 12, 4096, 5632) bfloat16\n",
      "('decoder', 'layers', 'unshared_mlp_2', 'wi_1') (8, 12, 4096, 5632) bfloat16\n",
      "('decoder', 'layers', 'unshared_mlp_2', 'wo') (8, 12, 5632, 4096) bfloat16\n",
      "('decoder', 'layers', 'unshared_mlp_3', 'mgate') (8, 12, 4096, 44) bfloat16\n",
      "('decoder', 'layers', 'unshared_mlp_3', 'router_gate', 'kernel') (4096, 12, 8) bfloat16\n",
      "('decoder', 'layers', 'unshared_mlp_3', 'wi_0') (8, 12, 4096, 5632) bfloat16\n",
      "('decoder', 'layers', 'unshared_mlp_3', 'wi_1') (8, 12, 4096, 5632) bfloat16\n",
      "('decoder', 'layers', 'unshared_mlp_3', 'wo') (8, 12, 5632, 4096) bfloat16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Configured `CheckpointManager` using deprecated legacy API. Please follow the instructions at https://orbax.readthedocs.io/en/latest/api_refactor.html to migrate by August 1st, 2024.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('decoder', 'decoder_norm', 'scale') (1,) bfloat16\n",
      "('decoder', 'layers', 'mlp_0', 'mgate', 'kernel') (1,) bfloat16\n",
      "('decoder', 'layers', 'unshared_mlp_0', 'mgate') (1,) bfloat16\n",
      "('decoder', 'layers', 'unshared_mlp_0', 'router_gate', 'kernel') (1,) bfloat16\n",
      "('decoder', 'layers', 'mlp_0', 'wi_0', 'kernel') (1,) bfloat16\n",
      "('decoder', 'layers', 'unshared_mlp_0', 'wi_0') (1,) bfloat16\n",
      "('decoder', 'layers', 'mlp_0', 'wi_1', 'kernel') (1,) bfloat16\n",
      "('decoder', 'layers', 'unshared_mlp_0', 'wi_1') (1,) bfloat16\n",
      "('decoder', 'layers', 'mlp_0', 'wo', 'kernel') (1,) bfloat16\n",
      "('decoder', 'layers', 'unshared_mlp_0', 'wo') (1,) bfloat16\n",
      "('decoder', 'layers', 'mlp_1', 'mgate', 'kernel') (1,) bfloat16\n",
      "('decoder', 'layers', 'unshared_mlp_1', 'mgate') (1,) bfloat16\n",
      "('decoder', 'layers', 'unshared_mlp_1', 'router_gate', 'kernel') (1,) bfloat16\n",
      "('decoder', 'layers', 'mlp_1', 'wi_0', 'kernel') (1,) bfloat16\n",
      "('decoder', 'layers', 'unshared_mlp_1', 'wi_0') (1,) bfloat16\n",
      "('decoder', 'layers', 'mlp_1', 'wi_1', 'kernel') (1,) bfloat16\n",
      "('decoder', 'layers', 'unshared_mlp_1', 'wi_1') (1,) bfloat16\n",
      "('decoder', 'layers', 'mlp_1', 'wo', 'kernel') (1,) bfloat16\n",
      "('decoder', 'layers', 'unshared_mlp_1', 'wo') (1,) bfloat16\n",
      "('decoder', 'layers', 'mlp_2', 'mgate', 'kernel') (1,) bfloat16\n",
      "('decoder', 'layers', 'unshared_mlp_2', 'mgate') (1,) bfloat16\n",
      "('decoder', 'layers', 'unshared_mlp_2', 'router_gate', 'kernel') (1,) bfloat16\n",
      "('decoder', 'layers', 'mlp_2', 'wi_0', 'kernel') (1,) bfloat16\n",
      "('decoder', 'layers', 'unshared_mlp_2', 'wi_0') (1,) bfloat16\n",
      "('decoder', 'layers', 'mlp_2', 'wi_1', 'kernel') (1,) bfloat16\n",
      "('decoder', 'layers', 'unshared_mlp_2', 'wi_1') (1,) bfloat16\n",
      "('decoder', 'layers', 'mlp_2', 'wo', 'kernel') (1,) bfloat16\n",
      "('decoder', 'layers', 'unshared_mlp_2', 'wo') (1,) bfloat16\n",
      "('decoder', 'layers', 'mlp_3', 'mgate', 'kernel') (1,) bfloat16\n",
      "('decoder', 'layers', 'unshared_mlp_3', 'mgate') (1,) bfloat16\n",
      "('decoder', 'layers', 'unshared_mlp_3', 'router_gate', 'kernel') (1,) bfloat16\n",
      "('decoder', 'layers', 'mlp_3', 'wi_0', 'kernel') (1,) bfloat16\n",
      "('decoder', 'layers', 'unshared_mlp_3', 'wi_0') (1,) bfloat16\n",
      "('decoder', 'layers', 'mlp_3', 'wi_1', 'kernel') (1,) bfloat16\n",
      "('decoder', 'layers', 'unshared_mlp_3', 'wi_1') (1,) bfloat16\n",
      "('decoder', 'layers', 'mlp_3', 'wo', 'kernel') (1,) bfloat16\n",
      "('decoder', 'layers', 'unshared_mlp_3', 'wo') (1,) bfloat16\n",
      "('decoder', 'layers', 'post_self_attention_layer_norm_0', 'scale') (1,) bfloat16\n",
      "('decoder', 'layers', 'post_self_attention_layer_norm_1', 'scale') (1,) bfloat16\n",
      "('decoder', 'layers', 'post_self_attention_layer_norm_2', 'scale') (1,) bfloat16\n",
      "('decoder', 'layers', 'post_self_attention_layer_norm_3', 'scale') (1,) bfloat16\n",
      "('decoder', 'layers', 'pre_self_attention_layer_norm_0', 'scale') (1,) bfloat16\n",
      "('decoder', 'layers', 'pre_self_attention_layer_norm_1', 'scale') (1,) bfloat16\n",
      "('decoder', 'layers', 'pre_self_attention_layer_norm_2', 'scale') (1,) bfloat16\n",
      "('decoder', 'layers', 'pre_self_attention_layer_norm_3', 'scale') (1,) bfloat16\n",
      "('decoder', 'layers', 'self_attention_0', 'AttentionOp_0', 'dyn_w_proj', 'dd', 'kernel') (1,) bfloat16\n",
      "('decoder', 'layers', 'self_attention_0', 'AttentionOp_0', 'dyn_w_proj', 'dw1', 'kernel') (1,) bfloat16\n",
      "('decoder', 'layers', 'self_attention_0', 'AttentionOp_0', 'dyn_w_proj', 'qkw') (1,) bfloat16\n",
      "('decoder', 'layers', 'self_attention_0', 'k_norm', 'scale') (1,) bfloat16\n",
      "('decoder', 'layers', 'self_attention_0', 'key', 'kernel') (1,) bfloat16\n",
      "('decoder', 'layers', 'self_attention_0', 'out', 'kernel') (1,) bfloat16\n",
      "('decoder', 'layers', 'self_attention_0', 'q_norm', 'scale') (1,) bfloat16\n",
      "('decoder', 'layers', 'self_attention_0', 'query', 'kernel') (1,) bfloat16\n",
      "('decoder', 'layers', 'self_attention_0', 'value', 'kernel') (1,) bfloat16\n",
      "('decoder', 'layers', 'self_attention_1', 'AttentionOp_0', 'dyn_w_proj', 'dd', 'kernel') (1,) bfloat16\n",
      "('decoder', 'layers', 'self_attention_1', 'AttentionOp_0', 'dyn_w_proj', 'dw1', 'kernel') (1,) bfloat16\n",
      "('decoder', 'layers', 'self_attention_1', 'AttentionOp_0', 'dyn_w_proj', 'qkw') (1,) bfloat16\n",
      "('decoder', 'layers', 'self_attention_1', 'k_norm', 'scale') (1,) bfloat16\n",
      "('decoder', 'layers', 'self_attention_1', 'key', 'kernel') (1,) bfloat16\n",
      "('decoder', 'layers', 'self_attention_1', 'out', 'kernel') (1,) bfloat16\n",
      "('decoder', 'layers', 'self_attention_1', 'q_norm', 'scale') (1,) bfloat16\n",
      "('decoder', 'layers', 'self_attention_1', 'query', 'kernel') (1,) bfloat16\n",
      "('decoder', 'layers', 'self_attention_1', 'value', 'kernel') (1,) bfloat16\n",
      "('decoder', 'layers', 'self_attention_2', 'AttentionOp_0', 'dyn_w_proj', 'dd', 'kernel') (1,) bfloat16\n",
      "('decoder', 'layers', 'self_attention_2', 'AttentionOp_0', 'dyn_w_proj', 'dw1', 'kernel') (1,) bfloat16\n",
      "('decoder', 'layers', 'self_attention_2', 'AttentionOp_0', 'dyn_w_proj', 'qkw') (1,) bfloat16\n",
      "('decoder', 'layers', 'self_attention_2', 'k_norm', 'scale') (1,) bfloat16\n",
      "('decoder', 'layers', 'self_attention_2', 'key', 'kernel') (1,) bfloat16\n",
      "('decoder', 'layers', 'self_attention_2', 'out', 'kernel') (1,) bfloat16\n",
      "('decoder', 'layers', 'self_attention_2', 'q_norm', 'scale') (1,) bfloat16\n",
      "('decoder', 'layers', 'self_attention_2', 'query', 'kernel') (1,) bfloat16\n",
      "('decoder', 'layers', 'self_attention_2', 'value', 'kernel') (1,) bfloat16\n",
      "('decoder', 'layers', 'self_attention_3', 'AttentionOp_0', 'dyn_w_proj', 'dd', 'kernel') (1,) bfloat16\n",
      "('decoder', 'layers', 'self_attention_3', 'AttentionOp_0', 'dyn_w_proj', 'dw1', 'kernel') (1,) bfloat16\n",
      "('decoder', 'layers', 'self_attention_3', 'AttentionOp_0', 'dyn_w_proj', 'qkw') (1,) bfloat16\n",
      "('decoder', 'layers', 'self_attention_3', 'k_norm', 'scale') (1,) bfloat16\n",
      "('decoder', 'layers', 'self_attention_3', 'key', 'kernel') (1,) bfloat16\n",
      "('decoder', 'layers', 'self_attention_3', 'out', 'kernel') (1,) bfloat16\n",
      "('decoder', 'layers', 'self_attention_3', 'q_norm', 'scale') (1,) bfloat16\n",
      "('decoder', 'layers', 'self_attention_3', 'query', 'kernel') (1,) bfloat16\n",
      "('decoder', 'layers', 'self_attention_3', 'value', 'kernel') (1,) bfloat16\n",
      "('decoder', 'logits_dense', 'kernel') (1,) bfloat16\n",
      "('token_embedder', 'embedding') (1,) bfloat16\n"
     ]
    }
   ],
   "source": [
    "def nd_dense_init(scale, mode, distribution):\n",
    "  \"\"\"Initializer with in_axis, out_axis set at call time.\"\"\"\n",
    "\n",
    "  def init_fn(key, shape, dtype, in_axis, out_axis):\n",
    "    fn = jax.nn.initializers.variance_scaling(scale, mode, distribution, in_axis, out_axis)\n",
    "    return fn(key, shape, dtype)\n",
    "\n",
    "  return init_fn\n",
    "\n",
    "init_func = nd_dense_init(1.0, \"fan_in\", \"truncated_normal\")\n",
    "\n",
    "\n",
    "## moe部分的参数保存，保存后在bucket后台人工进行转移\n",
    "start_time = time.time()\n",
    "unshared_experts = 8\n",
    "\n",
    "scale = 1\n",
    "mlp_dim = 5632 // scale\n",
    "model_dim = 4096 // scale\n",
    "# fp16_dtype = np.dtype('float16')\n",
    "# 4个子层\n",
    "moe_params = {}\n",
    "example_params = {}\n",
    "for k, v in xm3p5_w.items():\n",
    "    v = jnp.array(v).astype(jnp.bfloat16)\n",
    "    ev = jnp.array([100]).astype(jnp.bfloat16)\n",
    "    example_params[k] = ev\n",
    "    if 'decoder.layers.mlp_' in k:\n",
    "        mlp_inx = k.find('mlp_')\n",
    "        l = k[mlp_inx+4: mlp_inx+5]\n",
    "        # if int(l) % 2 != 0: continue\n",
    "        unshared_mlp = k.replace('decoder.layers.mlp_', 'decoder.layers.unshared_mlp_')\n",
    "        unshared_mlp = unshared_mlp.replace('.kernel', '')\n",
    "        copy_w = v.transpose(1, 0, 2)[None].repeat(unshared_experts, 0)\n",
    "        moe_params[unshared_mlp] = copy_w\n",
    "        example_params[unshared_mlp] = ev\n",
    "        if 'mgate' in unshared_mlp:\n",
    "            router_gate = unshared_mlp.replace('mgate', 'router_gate.kernel')\n",
    "            init_v = v[..., :unshared_experts]\n",
    "            # 为什么用这个初始化的时候，模型被load进cpu。取v的值的时候，被load进tpu，啥原因？\n",
    "            # init_v = init_func(key=jax.random.PRNGKey(9876), shape=v[...,:unshared_experts].shape, dtype=jnp.bfloat16, in_axis=0, out_axis=1)\n",
    "            print(f'router_gate: {router_gate} init_v: {init_v.shape}')\n",
    "            moe_params[router_gate] = init_v\n",
    "            example_params[router_gate] = ev\n",
    "            \n",
    "            \n",
    "moe_params = {tuple(k.split('.')): v for k, v in moe_params.items()}\n",
    "example_params = {tuple(k.split('.')): v for k,v in example_params.items()}\n",
    "\n",
    "for k, v in moe_params.items():\n",
    "    print(k, v.shape, v[0].sum(), v[1].sum())\n",
    "\n",
    "moe_save_step = params_save_step + 1\n",
    "example_step = params_save_step + 5\n",
    "\n",
    "save_params(moe_save_step, save_dir, moe_params)\n",
    "save_params(example_step, save_dir, example_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ebab4140-aba2-46d9-985d-9efe40bef446",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying gs://llm_base_models_europe-west4/v5p_256/7B/xm_E8x7B_ShareWithMgate_UnshareWithMgate_AllCopy_1204/checkpoints/5/state/_METADATA [Content-Type=application/octet-stream]...\n",
      "/ [1 files][ 34.0 KiB/ 34.0 KiB]                                                \n",
      "Operation completed over 1 objects/34.0 KiB.                                     \n",
      "Copying gs://llm_base_models_europe-west4/v5p_256/7B/xm_E8x7B_ShareWithMgate_UnshareWithMgate_AllCopy_1204/checkpoints/5/state/checkpoint...\n",
      "/ [1 files][  7.4 KiB/  7.4 KiB]                                                \n",
      "Operation completed over 1 objects/7.4 KiB.                                      \n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "source_dir = str(save_dir).rstrip('/')\n",
    "target_dir = str(save_dir).rstrip('/')\n",
    "\n",
    "command = f'gsutil cp {source_dir}/{example_step}/state/_METADATA {target_dir}/{params_save_step}/state/ '\n",
    "r = subprocess.run(command, stdout=subprocess.PIPE, shell=True)\n",
    "\n",
    "command = f'gsutil cp {source_dir}/{example_step}/state/checkpoint {target_dir}/{params_save_step}/state/ '\n",
    "r = subprocess.run(command, stdout=subprocess.PIPE, shell=True)\n",
    "\n",
    "command = f'gsutil -m cp -r {source_dir}/{moe_save_step}/state/params.params* {target_dir}/{params_save_step}/state/ '\n",
    "r = subprocess.run(command, stdout=subprocess.PIPE, shell=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a5a6438d-86f7-48a7-9cbc-073f605d101f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "core_nums: 16\n",
      "encode_unshared_mlp_1: params.params.decoder.layers.unshared_mlp_1.wi_0\n",
      "encode_unshared_mlp_3: params.params.decoder.layers.unshared_mlp_3.wi_0\n",
      "encode_unshared_mlp_1: params.params.decoder.layers.unshared_mlp_1.wi_1\n",
      "encode_unshared_mlp_3: params.params.decoder.layers.unshared_mlp_3.wi_1\n",
      "encode_unshared_mlp_1: params.params.decoder.layers.unshared_mlp_1.wo\n",
      "encode_unshared_mlp_3: params.params.decoder.layers.unshared_mlp_3.wo\n",
      "encode_unshared_mlp_1: params.params.decoder.layers.unshared_mlp_1.mgate\n",
      "encode_unshared_mlp_3: params.params.decoder.layers.unshared_mlp_3.mgate\n",
      "encode_unshared_mlp_1: params.params.decoder.layers.unshared_mlp_1.router_gate.kernel\n",
      "encode_unshared_mlp_3: params.params.decoder.layers.unshared_mlp_3.router_gate.kernel\n"
     ]
    }
   ],
   "source": [
    "# 基于tpu type 构建_sharding文件\n",
    "import base64\n",
    "\n",
    "def decode_base64(encoded_str):\n",
    "    decoded_bytes = base64.b64decode(encoded_str)\n",
    "    decoded_str = decoded_bytes.decode('utf-8')\n",
    "    return decoded_str\n",
    "\n",
    "def encode_base64(decoded_str):\n",
    "    # decoded_str = \"opt_state.mu.params.token_embedder.embedding\"\n",
    "    encoded_string = base64.b64encode(decoded_str.encode('utf-8')).decode('utf-8')\n",
    "    return encoded_string\n",
    "\n",
    "'''\n",
    "_sharding文件格式如下：\n",
    "{\n",
    "  b3B0X3N0YXRlLm11LnBhcmFtcy50b2tlbl9lbWJlZGRlci5lbWJlZGRpbmc=': {'sharding_type': 'NamedSharding',\n",
    "  'shape': [1, 1, 4, 1, 1, 1, 1],\n",
    "  'axis_names': ['data', 'stage', 'fsdp', 'fsdp_transpose', 'sequence', 'tensor','autoregressive'],\n",
    "  'partition_spec': [['tensor', 'autoregressive'], ['fsdp', 'fsdp_transpose', 'sequence']],\n",
    "   2: 4},\n",
    "   ...\n",
    "   }\n",
    "   '''\n",
    "# moe sharding\n",
    "_sharding_path = 'gs://llm_base_models_us-east5/v5p_256/7B/xm_45x7B_moe_1017/xm3p5_moe_params_no_opt_v5p_64_sharding.copy'\n",
    "_sharding_path = epath.Path(_sharding_path)\n",
    "# 读取已有的_sharding文件\n",
    "with _sharding_path.open('r') as f:\n",
    "    _sharding = json.load(f)\n",
    "\n",
    "tpu_type = 'v5p-32'\n",
    "core_nums = int(tpu_type.split('-')[-1])\n",
    "if 'v3' not in tpu_type:\n",
    "    core_nums = core_nums // 2\n",
    "print(f'core_nums: {core_nums}')\n",
    "updated_sharding = {}\n",
    "for k, v in _sharding.items():\n",
    "    v = json.loads(v)\n",
    "    v['shape'][2] = core_nums\n",
    "    base_k = decode_base64(k)\n",
    "    if 'opt_state' in base_k: continue\n",
    "    updated_sharding[k] = json.dumps(v)\n",
    "    if 'unshared_mlp_0' in base_k: # 因为已有的sharding文件是隔层moe，因此需要进行扩展\n",
    "        unshared_mlp_1 = base_k.replace('unshared_mlp_0', 'unshared_mlp_1')\n",
    "        unshared_mlp_3 = base_k.replace('unshared_mlp_0', 'unshared_mlp_3')\n",
    "        encode_unshared_mlp_1 = encode_base64(unshared_mlp_1)\n",
    "        encode_unshared_mlp_3 = encode_base64(unshared_mlp_3)\n",
    "        print(f'encode_unshared_mlp_1: {unshared_mlp_1}')\n",
    "        print(f'encode_unshared_mlp_3: {unshared_mlp_3}')\n",
    "        \n",
    "        updated_sharding[encode_unshared_mlp_1] = json.dumps(v)\n",
    "        updated_sharding[encode_unshared_mlp_3] = json.dumps(v)\n",
    "    \n",
    "updated_sharding_path = f'{save_dir}/{params_save_step}/state/_sharding'\n",
    "\n",
    "updated_sharding_path = epath.Path(updated_sharding_path)\n",
    "with updated_sharding_path.open('w') as f:\n",
    "    json.dump(updated_sharding, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "44eb6389-78db-4c73-be33-21915c42ea38",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step {\"sharding_type\": \"NamedSharding\", \"shape\": [1, 1, 16, 1, 1, 1, 1], \"axis_names\": [\"data\", \"stage\", \"fsdp\", \"fsdp_transpose\", \"sequence\", \"tensor\", \"autoregressive\"], \"partition_spec\": []}\n",
      "\n",
      "params.params.token_embedder.embedding {\"sharding_type\": \"NamedSharding\", \"shape\": [1, 1, 16, 1, 1, 1, 1], \"axis_names\": [\"data\", \"stage\", \"fsdp\", \"fsdp_transpose\", \"sequence\", \"tensor\", \"autoregressive\"], \"partition_spec\": [[\"tensor\", \"autoregressive\"], [\"fsdp\", \"fsdp_transpose\", \"sequence\"]]}\n",
      "\n",
      "params.params.decoder.decoder_norm.scale {\"sharding_type\": \"NamedSharding\", \"shape\": [1, 1, 16, 1, 1, 1, 1], \"axis_names\": [\"data\", \"stage\", \"fsdp\", \"fsdp_transpose\", \"sequence\", \"tensor\", \"autoregressive\"], \"partition_spec\": [\"tensor\"]}\n",
      "\n",
      "params.params.decoder.layers.unshared_mlp_0.wi_0 {\"sharding_type\": \"NamedSharding\", \"shape\": [1, 1, 16, 1, 1, 1, 1], \"axis_names\": [\"data\", \"stage\", \"fsdp\", \"fsdp_transpose\", \"sequence\", \"tensor\", \"autoregressive\"], \"partition_spec\": [null, \"stage\", [\"fsdp\", \"sequence\"], [\"fsdp_transpose\", \"tensor\", \"autoregressive\"]]}\n",
      "\n",
      "params.params.decoder.layers.unshared_mlp_1.wi_0 {\"sharding_type\": \"NamedSharding\", \"shape\": [1, 1, 16, 1, 1, 1, 1], \"axis_names\": [\"data\", \"stage\", \"fsdp\", \"fsdp_transpose\", \"sequence\", \"tensor\", \"autoregressive\"], \"partition_spec\": [null, \"stage\", [\"fsdp\", \"sequence\"], [\"fsdp_transpose\", \"tensor\", \"autoregressive\"]]}\n",
      "\n",
      "params.params.decoder.layers.unshared_mlp_3.wi_0 {\"sharding_type\": \"NamedSharding\", \"shape\": [1, 1, 16, 1, 1, 1, 1], \"axis_names\": [\"data\", \"stage\", \"fsdp\", \"fsdp_transpose\", \"sequence\", \"tensor\", \"autoregressive\"], \"partition_spec\": [null, \"stage\", [\"fsdp\", \"sequence\"], [\"fsdp_transpose\", \"tensor\", \"autoregressive\"]]}\n",
      "\n",
      "params.params.decoder.layers.unshared_mlp_0.wi_1 {\"sharding_type\": \"NamedSharding\", \"shape\": [1, 1, 16, 1, 1, 1, 1], \"axis_names\": [\"data\", \"stage\", \"fsdp\", \"fsdp_transpose\", \"sequence\", \"tensor\", \"autoregressive\"], \"partition_spec\": [null, \"stage\", [\"fsdp\", \"sequence\"], [\"fsdp_transpose\", \"tensor\", \"autoregressive\"]]}\n",
      "\n",
      "params.params.decoder.layers.unshared_mlp_1.wi_1 {\"sharding_type\": \"NamedSharding\", \"shape\": [1, 1, 16, 1, 1, 1, 1], \"axis_names\": [\"data\", \"stage\", \"fsdp\", \"fsdp_transpose\", \"sequence\", \"tensor\", \"autoregressive\"], \"partition_spec\": [null, \"stage\", [\"fsdp\", \"sequence\"], [\"fsdp_transpose\", \"tensor\", \"autoregressive\"]]}\n",
      "\n",
      "params.params.decoder.layers.unshared_mlp_3.wi_1 {\"sharding_type\": \"NamedSharding\", \"shape\": [1, 1, 16, 1, 1, 1, 1], \"axis_names\": [\"data\", \"stage\", \"fsdp\", \"fsdp_transpose\", \"sequence\", \"tensor\", \"autoregressive\"], \"partition_spec\": [null, \"stage\", [\"fsdp\", \"sequence\"], [\"fsdp_transpose\", \"tensor\", \"autoregressive\"]]}\n",
      "\n",
      "params.params.decoder.layers.unshared_mlp_0.wo {\"sharding_type\": \"NamedSharding\", \"shape\": [1, 1, 16, 1, 1, 1, 1], \"axis_names\": [\"data\", \"stage\", \"fsdp\", \"fsdp_transpose\", \"sequence\", \"tensor\", \"autoregressive\"], \"partition_spec\": [null, \"stage\", [\"fsdp_transpose\", \"tensor\", \"autoregressive\"], [\"fsdp\", \"sequence\"]]}\n",
      "\n",
      "params.params.decoder.layers.unshared_mlp_1.wo {\"sharding_type\": \"NamedSharding\", \"shape\": [1, 1, 16, 1, 1, 1, 1], \"axis_names\": [\"data\", \"stage\", \"fsdp\", \"fsdp_transpose\", \"sequence\", \"tensor\", \"autoregressive\"], \"partition_spec\": [null, \"stage\", [\"fsdp_transpose\", \"tensor\", \"autoregressive\"], [\"fsdp\", \"sequence\"]]}\n",
      "\n",
      "params.params.decoder.layers.unshared_mlp_3.wo {\"sharding_type\": \"NamedSharding\", \"shape\": [1, 1, 16, 1, 1, 1, 1], \"axis_names\": [\"data\", \"stage\", \"fsdp\", \"fsdp_transpose\", \"sequence\", \"tensor\", \"autoregressive\"], \"partition_spec\": [null, \"stage\", [\"fsdp_transpose\", \"tensor\", \"autoregressive\"], [\"fsdp\", \"sequence\"]]}\n",
      "\n",
      "params.params.decoder.layers.unshared_mlp_0.mgate {\"sharding_type\": \"NamedSharding\", \"shape\": [1, 1, 16, 1, 1, 1, 1], \"axis_names\": [\"data\", \"stage\", \"fsdp\", \"fsdp_transpose\", \"sequence\", \"tensor\", \"autoregressive\"], \"partition_spec\": [null, \"stage\", [\"fsdp\", \"sequence\"], [\"fsdp_transpose\", \"tensor\", \"autoregressive\"]]}\n",
      "\n",
      "params.params.decoder.layers.unshared_mlp_1.mgate {\"sharding_type\": \"NamedSharding\", \"shape\": [1, 1, 16, 1, 1, 1, 1], \"axis_names\": [\"data\", \"stage\", \"fsdp\", \"fsdp_transpose\", \"sequence\", \"tensor\", \"autoregressive\"], \"partition_spec\": [null, \"stage\", [\"fsdp\", \"sequence\"], [\"fsdp_transpose\", \"tensor\", \"autoregressive\"]]}\n",
      "\n",
      "params.params.decoder.layers.unshared_mlp_3.mgate {\"sharding_type\": \"NamedSharding\", \"shape\": [1, 1, 16, 1, 1, 1, 1], \"axis_names\": [\"data\", \"stage\", \"fsdp\", \"fsdp_transpose\", \"sequence\", \"tensor\", \"autoregressive\"], \"partition_spec\": [null, \"stage\", [\"fsdp\", \"sequence\"], [\"fsdp_transpose\", \"tensor\", \"autoregressive\"]]}\n",
      "\n",
      "params.params.decoder.layers.unshared_mlp_0.router_gate.kernel {\"sharding_type\": \"NamedSharding\", \"shape\": [1, 1, 16, 1, 1, 1, 1], \"axis_names\": [\"data\", \"stage\", \"fsdp\", \"fsdp_transpose\", \"sequence\", \"tensor\", \"autoregressive\"], \"partition_spec\": [[\"fsdp\", \"sequence\"], \"stage\", [\"fsdp_transpose\", \"tensor\", \"autoregressive\"]]}\n",
      "\n",
      "params.params.decoder.layers.unshared_mlp_1.router_gate.kernel {\"sharding_type\": \"NamedSharding\", \"shape\": [1, 1, 16, 1, 1, 1, 1], \"axis_names\": [\"data\", \"stage\", \"fsdp\", \"fsdp_transpose\", \"sequence\", \"tensor\", \"autoregressive\"], \"partition_spec\": [[\"fsdp\", \"sequence\"], \"stage\", [\"fsdp_transpose\", \"tensor\", \"autoregressive\"]]}\n",
      "\n",
      "params.params.decoder.layers.unshared_mlp_3.router_gate.kernel {\"sharding_type\": \"NamedSharding\", \"shape\": [1, 1, 16, 1, 1, 1, 1], \"axis_names\": [\"data\", \"stage\", \"fsdp\", \"fsdp_transpose\", \"sequence\", \"tensor\", \"autoregressive\"], \"partition_spec\": [[\"fsdp\", \"sequence\"], \"stage\", [\"fsdp_transpose\", \"tensor\", \"autoregressive\"]]}\n",
      "\n",
      "params.params.decoder.layers.unshared_mlp_2.wi_0 {\"sharding_type\": \"NamedSharding\", \"shape\": [1, 1, 16, 1, 1, 1, 1], \"axis_names\": [\"data\", \"stage\", \"fsdp\", \"fsdp_transpose\", \"sequence\", \"tensor\", \"autoregressive\"], \"partition_spec\": [null, \"stage\", [\"fsdp\", \"sequence\"], [\"fsdp_transpose\", \"tensor\", \"autoregressive\"]]}\n",
      "\n",
      "params.params.decoder.layers.unshared_mlp_2.wi_1 {\"sharding_type\": \"NamedSharding\", \"shape\": [1, 1, 16, 1, 1, 1, 1], \"axis_names\": [\"data\", \"stage\", \"fsdp\", \"fsdp_transpose\", \"sequence\", \"tensor\", \"autoregressive\"], \"partition_spec\": [null, \"stage\", [\"fsdp\", \"sequence\"], [\"fsdp_transpose\", \"tensor\", \"autoregressive\"]]}\n",
      "\n",
      "params.params.decoder.layers.unshared_mlp_2.wo {\"sharding_type\": \"NamedSharding\", \"shape\": [1, 1, 16, 1, 1, 1, 1], \"axis_names\": [\"data\", \"stage\", \"fsdp\", \"fsdp_transpose\", \"sequence\", \"tensor\", \"autoregressive\"], \"partition_spec\": [null, \"stage\", [\"fsdp_transpose\", \"tensor\", \"autoregressive\"], [\"fsdp\", \"sequence\"]]}\n",
      "\n",
      "params.params.decoder.layers.unshared_mlp_2.mgate {\"sharding_type\": \"NamedSharding\", \"shape\": [1, 1, 16, 1, 1, 1, 1], \"axis_names\": [\"data\", \"stage\", \"fsdp\", \"fsdp_transpose\", \"sequence\", \"tensor\", \"autoregressive\"], \"partition_spec\": [null, \"stage\", [\"fsdp\", \"sequence\"], [\"fsdp_transpose\", \"tensor\", \"autoregressive\"]]}\n",
      "\n",
      "params.params.decoder.layers.unshared_mlp_2.router_gate.kernel {\"sharding_type\": \"NamedSharding\", \"shape\": [1, 1, 16, 1, 1, 1, 1], \"axis_names\": [\"data\", \"stage\", \"fsdp\", \"fsdp_transpose\", \"sequence\", \"tensor\", \"autoregressive\"], \"partition_spec\": [[\"fsdp\", \"sequence\"], \"stage\", [\"fsdp_transpose\", \"tensor\", \"autoregressive\"]]}\n",
      "\n",
      "params.params.decoder.layers.mlp_0.wi_0.kernel {\"sharding_type\": \"NamedSharding\", \"shape\": [1, 1, 16, 1, 1, 1, 1], \"axis_names\": [\"data\", \"stage\", \"fsdp\", \"fsdp_transpose\", \"sequence\", \"tensor\", \"autoregressive\"], \"partition_spec\": [[\"fsdp\", \"sequence\"], \"stage\", [\"fsdp_transpose\", \"tensor\", \"autoregressive\"]]}\n",
      "\n",
      "params.params.decoder.layers.mlp_0.wi_1.kernel {\"sharding_type\": \"NamedSharding\", \"shape\": [1, 1, 16, 1, 1, 1, 1], \"axis_names\": [\"data\", \"stage\", \"fsdp\", \"fsdp_transpose\", \"sequence\", \"tensor\", \"autoregressive\"], \"partition_spec\": [[\"fsdp\", \"sequence\"], \"stage\", [\"fsdp_transpose\", \"tensor\", \"autoregressive\"]]}\n",
      "\n",
      "params.params.decoder.layers.mlp_0.wo.kernel {\"sharding_type\": \"NamedSharding\", \"shape\": [1, 1, 16, 1, 1, 1, 1], \"axis_names\": [\"data\", \"stage\", \"fsdp\", \"fsdp_transpose\", \"sequence\", \"tensor\", \"autoregressive\"], \"partition_spec\": [[\"fsdp_transpose\", \"tensor\", \"autoregressive\"], \"stage\", [\"fsdp\", \"sequence\"]]}\n",
      "\n",
      "params.params.decoder.layers.mlp_0.mgate.kernel {\"sharding_type\": \"NamedSharding\", \"shape\": [1, 1, 16, 1, 1, 1, 1], \"axis_names\": [\"data\", \"stage\", \"fsdp\", \"fsdp_transpose\", \"sequence\", \"tensor\", \"autoregressive\"], \"partition_spec\": [[\"fsdp\", \"sequence\"], \"stage\", [\"fsdp_transpose\", \"tensor\", \"autoregressive\"]]}\n",
      "\n",
      "params.params.decoder.layers.mlp_1.wi_0.kernel {\"sharding_type\": \"NamedSharding\", \"shape\": [1, 1, 16, 1, 1, 1, 1], \"axis_names\": [\"data\", \"stage\", \"fsdp\", \"fsdp_transpose\", \"sequence\", \"tensor\", \"autoregressive\"], \"partition_spec\": [[\"fsdp\", \"sequence\"], \"stage\", [\"fsdp_transpose\", \"tensor\", \"autoregressive\"]]}\n",
      "\n",
      "params.params.decoder.layers.mlp_1.wi_1.kernel {\"sharding_type\": \"NamedSharding\", \"shape\": [1, 1, 16, 1, 1, 1, 1], \"axis_names\": [\"data\", \"stage\", \"fsdp\", \"fsdp_transpose\", \"sequence\", \"tensor\", \"autoregressive\"], \"partition_spec\": [[\"fsdp\", \"sequence\"], \"stage\", [\"fsdp_transpose\", \"tensor\", \"autoregressive\"]]}\n",
      "\n",
      "params.params.decoder.layers.mlp_1.wo.kernel {\"sharding_type\": \"NamedSharding\", \"shape\": [1, 1, 16, 1, 1, 1, 1], \"axis_names\": [\"data\", \"stage\", \"fsdp\", \"fsdp_transpose\", \"sequence\", \"tensor\", \"autoregressive\"], \"partition_spec\": [[\"fsdp_transpose\", \"tensor\", \"autoregressive\"], \"stage\", [\"fsdp\", \"sequence\"]]}\n",
      "\n",
      "params.params.decoder.layers.mlp_1.mgate.kernel {\"sharding_type\": \"NamedSharding\", \"shape\": [1, 1, 16, 1, 1, 1, 1], \"axis_names\": [\"data\", \"stage\", \"fsdp\", \"fsdp_transpose\", \"sequence\", \"tensor\", \"autoregressive\"], \"partition_spec\": [[\"fsdp\", \"sequence\"], \"stage\", [\"fsdp_transpose\", \"tensor\", \"autoregressive\"]]}\n",
      "\n",
      "params.params.decoder.layers.mlp_2.wi_0.kernel {\"sharding_type\": \"NamedSharding\", \"shape\": [1, 1, 16, 1, 1, 1, 1], \"axis_names\": [\"data\", \"stage\", \"fsdp\", \"fsdp_transpose\", \"sequence\", \"tensor\", \"autoregressive\"], \"partition_spec\": [[\"fsdp\", \"sequence\"], \"stage\", [\"fsdp_transpose\", \"tensor\", \"autoregressive\"]]}\n",
      "\n",
      "params.params.decoder.layers.mlp_2.wi_1.kernel {\"sharding_type\": \"NamedSharding\", \"shape\": [1, 1, 16, 1, 1, 1, 1], \"axis_names\": [\"data\", \"stage\", \"fsdp\", \"fsdp_transpose\", \"sequence\", \"tensor\", \"autoregressive\"], \"partition_spec\": [[\"fsdp\", \"sequence\"], \"stage\", [\"fsdp_transpose\", \"tensor\", \"autoregressive\"]]}\n",
      "\n",
      "params.params.decoder.layers.mlp_2.wo.kernel {\"sharding_type\": \"NamedSharding\", \"shape\": [1, 1, 16, 1, 1, 1, 1], \"axis_names\": [\"data\", \"stage\", \"fsdp\", \"fsdp_transpose\", \"sequence\", \"tensor\", \"autoregressive\"], \"partition_spec\": [[\"fsdp_transpose\", \"tensor\", \"autoregressive\"], \"stage\", [\"fsdp\", \"sequence\"]]}\n",
      "\n",
      "params.params.decoder.layers.mlp_2.mgate.kernel {\"sharding_type\": \"NamedSharding\", \"shape\": [1, 1, 16, 1, 1, 1, 1], \"axis_names\": [\"data\", \"stage\", \"fsdp\", \"fsdp_transpose\", \"sequence\", \"tensor\", \"autoregressive\"], \"partition_spec\": [[\"fsdp\", \"sequence\"], \"stage\", [\"fsdp_transpose\", \"tensor\", \"autoregressive\"]]}\n",
      "\n",
      "params.params.decoder.layers.mlp_3.wi_0.kernel {\"sharding_type\": \"NamedSharding\", \"shape\": [1, 1, 16, 1, 1, 1, 1], \"axis_names\": [\"data\", \"stage\", \"fsdp\", \"fsdp_transpose\", \"sequence\", \"tensor\", \"autoregressive\"], \"partition_spec\": [[\"fsdp\", \"sequence\"], \"stage\", [\"fsdp_transpose\", \"tensor\", \"autoregressive\"]]}\n",
      "\n",
      "params.params.decoder.layers.mlp_3.wi_1.kernel {\"sharding_type\": \"NamedSharding\", \"shape\": [1, 1, 16, 1, 1, 1, 1], \"axis_names\": [\"data\", \"stage\", \"fsdp\", \"fsdp_transpose\", \"sequence\", \"tensor\", \"autoregressive\"], \"partition_spec\": [[\"fsdp\", \"sequence\"], \"stage\", [\"fsdp_transpose\", \"tensor\", \"autoregressive\"]]}\n",
      "\n",
      "params.params.decoder.layers.mlp_3.wo.kernel {\"sharding_type\": \"NamedSharding\", \"shape\": [1, 1, 16, 1, 1, 1, 1], \"axis_names\": [\"data\", \"stage\", \"fsdp\", \"fsdp_transpose\", \"sequence\", \"tensor\", \"autoregressive\"], \"partition_spec\": [[\"fsdp_transpose\", \"tensor\", \"autoregressive\"], \"stage\", [\"fsdp\", \"sequence\"]]}\n",
      "\n",
      "params.params.decoder.layers.mlp_3.mgate.kernel {\"sharding_type\": \"NamedSharding\", \"shape\": [1, 1, 16, 1, 1, 1, 1], \"axis_names\": [\"data\", \"stage\", \"fsdp\", \"fsdp_transpose\", \"sequence\", \"tensor\", \"autoregressive\"], \"partition_spec\": [[\"fsdp\", \"sequence\"], \"stage\", [\"fsdp_transpose\", \"tensor\", \"autoregressive\"]]}\n",
      "\n",
      "params.params.decoder.layers.post_self_attention_layer_norm_0.scale {\"sharding_type\": \"NamedSharding\", \"shape\": [1, 1, 16, 1, 1, 1, 1], \"axis_names\": [\"data\", \"stage\", \"fsdp\", \"fsdp_transpose\", \"sequence\", \"tensor\", \"autoregressive\"], \"partition_spec\": [[\"fsdp\", \"fsdp_transpose\", \"sequence\"], \"stage\"]}\n",
      "\n",
      "params.params.decoder.layers.post_self_attention_layer_norm_1.scale {\"sharding_type\": \"NamedSharding\", \"shape\": [1, 1, 16, 1, 1, 1, 1], \"axis_names\": [\"data\", \"stage\", \"fsdp\", \"fsdp_transpose\", \"sequence\", \"tensor\", \"autoregressive\"], \"partition_spec\": [[\"fsdp\", \"fsdp_transpose\", \"sequence\"], \"stage\"]}\n",
      "\n",
      "params.params.decoder.layers.post_self_attention_layer_norm_2.scale {\"sharding_type\": \"NamedSharding\", \"shape\": [1, 1, 16, 1, 1, 1, 1], \"axis_names\": [\"data\", \"stage\", \"fsdp\", \"fsdp_transpose\", \"sequence\", \"tensor\", \"autoregressive\"], \"partition_spec\": [[\"fsdp\", \"fsdp_transpose\", \"sequence\"], \"stage\"]}\n",
      "\n",
      "params.params.decoder.layers.post_self_attention_layer_norm_3.scale {\"sharding_type\": \"NamedSharding\", \"shape\": [1, 1, 16, 1, 1, 1, 1], \"axis_names\": [\"data\", \"stage\", \"fsdp\", \"fsdp_transpose\", \"sequence\", \"tensor\", \"autoregressive\"], \"partition_spec\": [[\"fsdp\", \"fsdp_transpose\", \"sequence\"], \"stage\"]}\n",
      "\n",
      "params.params.decoder.layers.pre_self_attention_layer_norm_0.scale {\"sharding_type\": \"NamedSharding\", \"shape\": [1, 1, 16, 1, 1, 1, 1], \"axis_names\": [\"data\", \"stage\", \"fsdp\", \"fsdp_transpose\", \"sequence\", \"tensor\", \"autoregressive\"], \"partition_spec\": [[\"fsdp\", \"fsdp_transpose\", \"sequence\"], \"stage\"]}\n",
      "\n",
      "params.params.decoder.layers.pre_self_attention_layer_norm_1.scale {\"sharding_type\": \"NamedSharding\", \"shape\": [1, 1, 16, 1, 1, 1, 1], \"axis_names\": [\"data\", \"stage\", \"fsdp\", \"fsdp_transpose\", \"sequence\", \"tensor\", \"autoregressive\"], \"partition_spec\": [[\"fsdp\", \"fsdp_transpose\", \"sequence\"], \"stage\"]}\n",
      "\n",
      "params.params.decoder.layers.pre_self_attention_layer_norm_2.scale {\"sharding_type\": \"NamedSharding\", \"shape\": [1, 1, 16, 1, 1, 1, 1], \"axis_names\": [\"data\", \"stage\", \"fsdp\", \"fsdp_transpose\", \"sequence\", \"tensor\", \"autoregressive\"], \"partition_spec\": [[\"fsdp\", \"fsdp_transpose\", \"sequence\"], \"stage\"]}\n",
      "\n",
      "params.params.decoder.layers.pre_self_attention_layer_norm_3.scale {\"sharding_type\": \"NamedSharding\", \"shape\": [1, 1, 16, 1, 1, 1, 1], \"axis_names\": [\"data\", \"stage\", \"fsdp\", \"fsdp_transpose\", \"sequence\", \"tensor\", \"autoregressive\"], \"partition_spec\": [[\"fsdp\", \"fsdp_transpose\", \"sequence\"], \"stage\"]}\n",
      "\n",
      "params.params.decoder.layers.self_attention_0.AttentionOp_0.dyn_w_proj.dd.kernel {\"sharding_type\": \"NamedSharding\", \"shape\": [1, 1, 16, 1, 1, 1, 1], \"axis_names\": [\"data\", \"stage\", \"fsdp\", \"fsdp_transpose\", \"sequence\", \"tensor\", \"autoregressive\"], \"partition_spec\": [[\"fsdp\", \"sequence\"], \"stage\", null, [\"fsdp_transpose\", \"tensor\", \"autoregressive\"]]}\n",
      "\n",
      "params.params.decoder.layers.self_attention_0.AttentionOp_0.dyn_w_proj.dw1.kernel {\"sharding_type\": \"NamedSharding\", \"shape\": [1, 1, 16, 1, 1, 1, 1], \"axis_names\": [\"data\", \"stage\", \"fsdp\", \"fsdp_transpose\", \"sequence\", \"tensor\", \"autoregressive\"], \"partition_spec\": [[\"fsdp\", \"sequence\"], \"stage\", null, null, [\"fsdp_transpose\", \"tensor\", \"autoregressive\"]]}\n",
      "\n",
      "params.params.decoder.layers.self_attention_0.AttentionOp_0.dyn_w_proj.qkw {\"sharding_type\": \"NamedSharding\", \"shape\": [1, 1, 16, 1, 1, 1, 1], \"axis_names\": [\"data\", \"stage\", \"fsdp\", \"fsdp_transpose\", \"sequence\", \"tensor\", \"autoregressive\"], \"partition_spec\": [null, \"stage\", null, null, null, null]}\n",
      "\n",
      "params.params.decoder.layers.self_attention_0.out.kernel {\"sharding_type\": \"NamedSharding\", \"shape\": [1, 1, 16, 1, 1, 1, 1], \"axis_names\": [\"data\", \"stage\", \"fsdp\", \"fsdp_transpose\", \"sequence\", \"tensor\", \"autoregressive\"], \"partition_spec\": [[\"tensor\", \"autoregressive\"], \"stage\", [], [\"fsdp\", \"fsdp_transpose\", \"sequence\"]]}\n",
      "\n",
      "params.params.decoder.layers.self_attention_0.k_norm.scale {\"sharding_type\": \"NamedSharding\", \"shape\": [1, 1, 16, 1, 1, 1, 1], \"axis_names\": [\"data\", \"stage\", \"fsdp\", \"fsdp_transpose\", \"sequence\", \"tensor\", \"autoregressive\"], \"partition_spec\": [\"tensor\", \"stage\"]}\n",
      "\n",
      "params.params.decoder.layers.self_attention_0.key.kernel {\"sharding_type\": \"NamedSharding\", \"shape\": [1, 1, 16, 1, 1, 1, 1], \"axis_names\": [\"data\", \"stage\", \"fsdp\", \"fsdp_transpose\", \"sequence\", \"tensor\", \"autoregressive\"], \"partition_spec\": [[\"fsdp\", \"fsdp_transpose\", \"sequence\"], \"stage\", [\"tensor\", \"autoregressive\"], []]}\n",
      "\n",
      "params.params.decoder.layers.self_attention_0.query.kernel {\"sharding_type\": \"NamedSharding\", \"shape\": [1, 1, 16, 1, 1, 1, 1], \"axis_names\": [\"data\", \"stage\", \"fsdp\", \"fsdp_transpose\", \"sequence\", \"tensor\", \"autoregressive\"], \"partition_spec\": [[\"fsdp\", \"fsdp_transpose\", \"sequence\"], \"stage\", [\"tensor\", \"autoregressive\"], []]}\n",
      "\n",
      "params.params.decoder.layers.self_attention_0.q_norm.scale {\"sharding_type\": \"NamedSharding\", \"shape\": [1, 1, 16, 1, 1, 1, 1], \"axis_names\": [\"data\", \"stage\", \"fsdp\", \"fsdp_transpose\", \"sequence\", \"tensor\", \"autoregressive\"], \"partition_spec\": [\"tensor\", \"stage\"]}\n",
      "\n",
      "params.params.decoder.layers.self_attention_0.value.kernel {\"sharding_type\": \"NamedSharding\", \"shape\": [1, 1, 16, 1, 1, 1, 1], \"axis_names\": [\"data\", \"stage\", \"fsdp\", \"fsdp_transpose\", \"sequence\", \"tensor\", \"autoregressive\"], \"partition_spec\": [[\"fsdp\", \"fsdp_transpose\", \"sequence\"], \"stage\", [\"tensor\", \"autoregressive\"], []]}\n",
      "\n",
      "params.params.decoder.layers.self_attention_1.AttentionOp_0.dyn_w_proj.dd.kernel {\"sharding_type\": \"NamedSharding\", \"shape\": [1, 1, 16, 1, 1, 1, 1], \"axis_names\": [\"data\", \"stage\", \"fsdp\", \"fsdp_transpose\", \"sequence\", \"tensor\", \"autoregressive\"], \"partition_spec\": [[\"fsdp\", \"sequence\"], \"stage\", null, [\"fsdp_transpose\", \"tensor\", \"autoregressive\"]]}\n",
      "\n",
      "params.params.decoder.layers.self_attention_1.AttentionOp_0.dyn_w_proj.dw1.kernel {\"sharding_type\": \"NamedSharding\", \"shape\": [1, 1, 16, 1, 1, 1, 1], \"axis_names\": [\"data\", \"stage\", \"fsdp\", \"fsdp_transpose\", \"sequence\", \"tensor\", \"autoregressive\"], \"partition_spec\": [[\"fsdp\", \"sequence\"], \"stage\", null, null, [\"fsdp_transpose\", \"tensor\", \"autoregressive\"]]}\n",
      "\n",
      "params.params.decoder.layers.self_attention_1.AttentionOp_0.dyn_w_proj.qkw {\"sharding_type\": \"NamedSharding\", \"shape\": [1, 1, 16, 1, 1, 1, 1], \"axis_names\": [\"data\", \"stage\", \"fsdp\", \"fsdp_transpose\", \"sequence\", \"tensor\", \"autoregressive\"], \"partition_spec\": [null, \"stage\", null, null, null, null]}\n",
      "\n",
      "params.params.decoder.layers.self_attention_1.out.kernel {\"sharding_type\": \"NamedSharding\", \"shape\": [1, 1, 16, 1, 1, 1, 1], \"axis_names\": [\"data\", \"stage\", \"fsdp\", \"fsdp_transpose\", \"sequence\", \"tensor\", \"autoregressive\"], \"partition_spec\": [[\"tensor\", \"autoregressive\"], \"stage\", [], [\"fsdp\", \"fsdp_transpose\", \"sequence\"]]}\n",
      "\n",
      "params.params.decoder.layers.self_attention_1.k_norm.scale {\"sharding_type\": \"NamedSharding\", \"shape\": [1, 1, 16, 1, 1, 1, 1], \"axis_names\": [\"data\", \"stage\", \"fsdp\", \"fsdp_transpose\", \"sequence\", \"tensor\", \"autoregressive\"], \"partition_spec\": [\"tensor\", \"stage\"]}\n",
      "\n",
      "params.params.decoder.layers.self_attention_1.key.kernel {\"sharding_type\": \"NamedSharding\", \"shape\": [1, 1, 16, 1, 1, 1, 1], \"axis_names\": [\"data\", \"stage\", \"fsdp\", \"fsdp_transpose\", \"sequence\", \"tensor\", \"autoregressive\"], \"partition_spec\": [[\"fsdp\", \"fsdp_transpose\", \"sequence\"], \"stage\", [\"tensor\", \"autoregressive\"], []]}\n",
      "\n",
      "params.params.decoder.layers.self_attention_1.query.kernel {\"sharding_type\": \"NamedSharding\", \"shape\": [1, 1, 16, 1, 1, 1, 1], \"axis_names\": [\"data\", \"stage\", \"fsdp\", \"fsdp_transpose\", \"sequence\", \"tensor\", \"autoregressive\"], \"partition_spec\": [[\"fsdp\", \"fsdp_transpose\", \"sequence\"], \"stage\", [\"tensor\", \"autoregressive\"], []]}\n",
      "\n",
      "params.params.decoder.layers.self_attention_1.q_norm.scale {\"sharding_type\": \"NamedSharding\", \"shape\": [1, 1, 16, 1, 1, 1, 1], \"axis_names\": [\"data\", \"stage\", \"fsdp\", \"fsdp_transpose\", \"sequence\", \"tensor\", \"autoregressive\"], \"partition_spec\": [\"tensor\", \"stage\"]}\n",
      "\n",
      "params.params.decoder.layers.self_attention_1.value.kernel {\"sharding_type\": \"NamedSharding\", \"shape\": [1, 1, 16, 1, 1, 1, 1], \"axis_names\": [\"data\", \"stage\", \"fsdp\", \"fsdp_transpose\", \"sequence\", \"tensor\", \"autoregressive\"], \"partition_spec\": [[\"fsdp\", \"fsdp_transpose\", \"sequence\"], \"stage\", [\"tensor\", \"autoregressive\"], []]}\n",
      "\n",
      "params.params.decoder.layers.self_attention_2.AttentionOp_0.dyn_w_proj.dd.kernel {\"sharding_type\": \"NamedSharding\", \"shape\": [1, 1, 16, 1, 1, 1, 1], \"axis_names\": [\"data\", \"stage\", \"fsdp\", \"fsdp_transpose\", \"sequence\", \"tensor\", \"autoregressive\"], \"partition_spec\": [[\"fsdp\", \"sequence\"], \"stage\", null, [\"fsdp_transpose\", \"tensor\", \"autoregressive\"]]}\n",
      "\n",
      "params.params.decoder.layers.self_attention_2.AttentionOp_0.dyn_w_proj.dw1.kernel {\"sharding_type\": \"NamedSharding\", \"shape\": [1, 1, 16, 1, 1, 1, 1], \"axis_names\": [\"data\", \"stage\", \"fsdp\", \"fsdp_transpose\", \"sequence\", \"tensor\", \"autoregressive\"], \"partition_spec\": [[\"fsdp\", \"sequence\"], \"stage\", null, null, [\"fsdp_transpose\", \"tensor\", \"autoregressive\"]]}\n",
      "\n",
      "params.params.decoder.layers.self_attention_2.AttentionOp_0.dyn_w_proj.qkw {\"sharding_type\": \"NamedSharding\", \"shape\": [1, 1, 16, 1, 1, 1, 1], \"axis_names\": [\"data\", \"stage\", \"fsdp\", \"fsdp_transpose\", \"sequence\", \"tensor\", \"autoregressive\"], \"partition_spec\": [null, \"stage\", null, null, null, null]}\n",
      "\n",
      "params.params.decoder.layers.self_attention_2.out.kernel {\"sharding_type\": \"NamedSharding\", \"shape\": [1, 1, 16, 1, 1, 1, 1], \"axis_names\": [\"data\", \"stage\", \"fsdp\", \"fsdp_transpose\", \"sequence\", \"tensor\", \"autoregressive\"], \"partition_spec\": [[\"tensor\", \"autoregressive\"], \"stage\", [], [\"fsdp\", \"fsdp_transpose\", \"sequence\"]]}\n",
      "\n",
      "params.params.decoder.layers.self_attention_2.k_norm.scale {\"sharding_type\": \"NamedSharding\", \"shape\": [1, 1, 16, 1, 1, 1, 1], \"axis_names\": [\"data\", \"stage\", \"fsdp\", \"fsdp_transpose\", \"sequence\", \"tensor\", \"autoregressive\"], \"partition_spec\": [\"tensor\", \"stage\"]}\n",
      "\n",
      "params.params.decoder.layers.self_attention_2.key.kernel {\"sharding_type\": \"NamedSharding\", \"shape\": [1, 1, 16, 1, 1, 1, 1], \"axis_names\": [\"data\", \"stage\", \"fsdp\", \"fsdp_transpose\", \"sequence\", \"tensor\", \"autoregressive\"], \"partition_spec\": [[\"fsdp\", \"fsdp_transpose\", \"sequence\"], \"stage\", [\"tensor\", \"autoregressive\"], []]}\n",
      "\n",
      "params.params.decoder.layers.self_attention_2.query.kernel {\"sharding_type\": \"NamedSharding\", \"shape\": [1, 1, 16, 1, 1, 1, 1], \"axis_names\": [\"data\", \"stage\", \"fsdp\", \"fsdp_transpose\", \"sequence\", \"tensor\", \"autoregressive\"], \"partition_spec\": [[\"fsdp\", \"fsdp_transpose\", \"sequence\"], \"stage\", [\"tensor\", \"autoregressive\"], []]}\n",
      "\n",
      "params.params.decoder.layers.self_attention_2.q_norm.scale {\"sharding_type\": \"NamedSharding\", \"shape\": [1, 1, 16, 1, 1, 1, 1], \"axis_names\": [\"data\", \"stage\", \"fsdp\", \"fsdp_transpose\", \"sequence\", \"tensor\", \"autoregressive\"], \"partition_spec\": [\"tensor\", \"stage\"]}\n",
      "\n",
      "params.params.decoder.layers.self_attention_2.value.kernel {\"sharding_type\": \"NamedSharding\", \"shape\": [1, 1, 16, 1, 1, 1, 1], \"axis_names\": [\"data\", \"stage\", \"fsdp\", \"fsdp_transpose\", \"sequence\", \"tensor\", \"autoregressive\"], \"partition_spec\": [[\"fsdp\", \"fsdp_transpose\", \"sequence\"], \"stage\", [\"tensor\", \"autoregressive\"], []]}\n",
      "\n",
      "params.params.decoder.layers.self_attention_3.AttentionOp_0.dyn_w_proj.dd.kernel {\"sharding_type\": \"NamedSharding\", \"shape\": [1, 1, 16, 1, 1, 1, 1], \"axis_names\": [\"data\", \"stage\", \"fsdp\", \"fsdp_transpose\", \"sequence\", \"tensor\", \"autoregressive\"], \"partition_spec\": [[\"fsdp\", \"sequence\"], \"stage\", null, [\"fsdp_transpose\", \"tensor\", \"autoregressive\"]]}\n",
      "\n",
      "params.params.decoder.layers.self_attention_3.AttentionOp_0.dyn_w_proj.dw1.kernel {\"sharding_type\": \"NamedSharding\", \"shape\": [1, 1, 16, 1, 1, 1, 1], \"axis_names\": [\"data\", \"stage\", \"fsdp\", \"fsdp_transpose\", \"sequence\", \"tensor\", \"autoregressive\"], \"partition_spec\": [[\"fsdp\", \"sequence\"], \"stage\", null, null, [\"fsdp_transpose\", \"tensor\", \"autoregressive\"]]}\n",
      "\n",
      "params.params.decoder.layers.self_attention_3.AttentionOp_0.dyn_w_proj.qkw {\"sharding_type\": \"NamedSharding\", \"shape\": [1, 1, 16, 1, 1, 1, 1], \"axis_names\": [\"data\", \"stage\", \"fsdp\", \"fsdp_transpose\", \"sequence\", \"tensor\", \"autoregressive\"], \"partition_spec\": [null, \"stage\", null, null, null, null]}\n",
      "\n",
      "params.params.decoder.layers.self_attention_3.out.kernel {\"sharding_type\": \"NamedSharding\", \"shape\": [1, 1, 16, 1, 1, 1, 1], \"axis_names\": [\"data\", \"stage\", \"fsdp\", \"fsdp_transpose\", \"sequence\", \"tensor\", \"autoregressive\"], \"partition_spec\": [[\"tensor\", \"autoregressive\"], \"stage\", [], [\"fsdp\", \"fsdp_transpose\", \"sequence\"]]}\n",
      "\n",
      "params.params.decoder.layers.self_attention_3.k_norm.scale {\"sharding_type\": \"NamedSharding\", \"shape\": [1, 1, 16, 1, 1, 1, 1], \"axis_names\": [\"data\", \"stage\", \"fsdp\", \"fsdp_transpose\", \"sequence\", \"tensor\", \"autoregressive\"], \"partition_spec\": [\"tensor\", \"stage\"]}\n",
      "\n",
      "params.params.decoder.layers.self_attention_3.key.kernel {\"sharding_type\": \"NamedSharding\", \"shape\": [1, 1, 16, 1, 1, 1, 1], \"axis_names\": [\"data\", \"stage\", \"fsdp\", \"fsdp_transpose\", \"sequence\", \"tensor\", \"autoregressive\"], \"partition_spec\": [[\"fsdp\", \"fsdp_transpose\", \"sequence\"], \"stage\", [\"tensor\", \"autoregressive\"], []]}\n",
      "\n",
      "params.params.decoder.layers.self_attention_3.query.kernel {\"sharding_type\": \"NamedSharding\", \"shape\": [1, 1, 16, 1, 1, 1, 1], \"axis_names\": [\"data\", \"stage\", \"fsdp\", \"fsdp_transpose\", \"sequence\", \"tensor\", \"autoregressive\"], \"partition_spec\": [[\"fsdp\", \"fsdp_transpose\", \"sequence\"], \"stage\", [\"tensor\", \"autoregressive\"], []]}\n",
      "\n",
      "params.params.decoder.layers.self_attention_3.q_norm.scale {\"sharding_type\": \"NamedSharding\", \"shape\": [1, 1, 16, 1, 1, 1, 1], \"axis_names\": [\"data\", \"stage\", \"fsdp\", \"fsdp_transpose\", \"sequence\", \"tensor\", \"autoregressive\"], \"partition_spec\": [\"tensor\", \"stage\"]}\n",
      "\n",
      "params.params.decoder.layers.self_attention_3.value.kernel {\"sharding_type\": \"NamedSharding\", \"shape\": [1, 1, 16, 1, 1, 1, 1], \"axis_names\": [\"data\", \"stage\", \"fsdp\", \"fsdp_transpose\", \"sequence\", \"tensor\", \"autoregressive\"], \"partition_spec\": [[\"fsdp\", \"fsdp_transpose\", \"sequence\"], \"stage\", [\"tensor\", \"autoregressive\"], []]}\n",
      "\n",
      "params.params.decoder.logits_dense.kernel {\"sharding_type\": \"NamedSharding\", \"shape\": [1, 1, 16, 1, 1, 1, 1], \"axis_names\": [\"data\", \"stage\", \"fsdp\", \"fsdp_transpose\", \"sequence\", \"tensor\", \"autoregressive\"], \"partition_spec\": [[\"fsdp\", \"fsdp_transpose\", \"sequence\"], [\"tensor\", \"autoregressive\"]]}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for k, v in updated_sharding.items():\n",
    "    base_k = decode_base64(k)\n",
    "    print(base_k, v)\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
