{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7703bf42-ac63-4182-beee-30e5a953b875",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-18 09:15:03.407736: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:479] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-07-18 09:15:03.425884: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:10575] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-07-18 09:15:03.425923: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1442] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-07-18 09:15:04.317402: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPECIAL_TOKENS: ((151643, '<|endoftext|>'), (151644, '<|im_start|>'), (151645, '<|im_end|>'), (151646, '<|extra_0|>'), (151647, '<|extra_1|>'), (151648, '<|extra_2|>'), (151649, '<|extra_3|>'), (151650, '<|extra_4|>'), (151651, '<|extra_5|>'), (151652, '<|extra_6|>'), (151653, '<|extra_7|>'), (151654, '<|extra_8|>'), (151655, '<|extra_9|>'), (151656, '<|extra_10|>'), (151657, '<|extra_11|>'), (151658, '<|extra_12|>'), (151659, '<|extra_13|>'), (151660, '<|extra_14|>'), (151661, '<|extra_15|>'), (151662, '<|extra_16|>'), (151663, '<|extra_17|>'), (151664, '<|extra_18|>'), (151665, '<|extra_19|>'), (151666, '<|extra_20|>'), (151667, '<|extra_21|>'), (151668, '<|extra_22|>'), (151669, '<|extra_23|>'), (151670, '<|extra_24|>'), (151671, '<|extra_25|>'), (151672, '<|extra_26|>'), (151673, '<|extra_27|>'), (151674, '<|extra_28|>'), (151675, '<|extra_29|>'), (151676, '<|extra_30|>'), (151677, '<|extra_31|>'), (151678, '<|extra_32|>'), (151679, '<|extra_33|>'), (151680, '<|extra_34|>'), (151681, '<|extra_35|>'), (151682, '<|extra_36|>'), (151683, '<|extra_37|>'), (151684, '<|extra_38|>'), (151685, '<|extra_39|>'), (151686, '<|extra_40|>'), (151687, '<|extra_41|>'), (151688, '<|extra_42|>'), (151689, '<|extra_43|>'), (151690, '<|extra_44|>'), (151691, '<|extra_45|>'), (151692, '<|extra_46|>'), (151693, '<|extra_47|>'), (151694, '<|extra_48|>'), (151695, '<|extra_49|>'), (151696, '<|extra_50|>'), (151697, '<|extra_51|>'), (151698, '<|extra_52|>'), (151699, '<|extra_53|>'), (151700, '<|extra_54|>'), (151701, '<|extra_55|>'), (151702, '<|extra_56|>'), (151703, '<|extra_57|>'), (151704, '<|extra_58|>'), (151705, '<|extra_59|>'), (151706, '<|extra_60|>'), (151707, '<|extra_61|>'), (151708, '<|extra_62|>'), (151709, '<|extra_63|>'), (151710, '<|extra_64|>'), (151711, '<|extra_65|>'), (151712, '<|extra_66|>'), (151713, '<|extra_67|>'), (151714, '<|extra_68|>'), (151715, '<|extra_69|>'), (151716, '<|extra_70|>'), (151717, '<|extra_71|>'), (151718, '<|extra_72|>'), (151719, '<|extra_73|>'), (151720, '<|extra_74|>'), (151721, '<|extra_75|>'), (151722, '<|extra_76|>'), (151723, '<|extra_77|>'), (151724, '<|extra_78|>'), (151725, '<|extra_79|>'), (151726, '<|extra_80|>'), (151727, '<|extra_81|>'), (151728, '<|extra_82|>'), (151729, '<|extra_83|>'), (151730, '<|extra_84|>'), (151731, '<|extra_85|>'), (151732, '<|extra_86|>'), (151733, '<|extra_87|>'), (151734, '<|extra_88|>'), (151735, '<|extra_89|>'), (151736, '<|extra_90|>'), (151737, '<|extra_91|>'), (151738, '<|extra_92|>'), (151739, '<|extra_93|>'), (151740, '<|extra_94|>'), (151741, '<|extra_95|>'), (151742, '<|extra_96|>'), (151743, '<|extra_97|>'), (151744, '<|extra_98|>'), (151745, '<|extra_99|>'), (151746, '<|extra_100|>'), (151747, '<|extra_101|>'), (151748, '<|extra_102|>'), (151749, '<|extra_103|>'), (151750, '<|extra_104|>'), (151751, '<|extra_105|>'), (151752, '<|extra_106|>'), (151753, '<|extra_107|>'), (151754, '<|extra_108|>'), (151755, '<|extra_109|>'), (151756, '<|extra_110|>'), (151757, '<|extra_111|>'), (151758, '<|extra_112|>'), (151759, '<|extra_113|>'), (151760, '<|extra_114|>'), (151761, '<|extra_115|>'), (151762, '<|extra_116|>'), (151763, '<|extra_117|>'), (151764, '<|extra_118|>'), (151765, '<|extra_119|>'), (151766, '<|extra_120|>'), (151767, '<|extra_121|>'), (151768, '<|extra_122|>'), (151769, '<|extra_123|>'), (151770, '<|extra_124|>'), (151771, '<|extra_125|>'), (151772, '<|extra_126|>'), (151773, '<|extra_127|>'), (151774, '<|extra_128|>'), (151775, '<|extra_129|>'), (151776, '<|extra_130|>'), (151777, '<|extra_131|>'), (151778, '<|extra_132|>'), (151779, '<|extra_133|>'), (151780, '<|extra_134|>'), (151781, '<|extra_135|>'), (151782, '<|extra_136|>'), (151783, '<|extra_137|>'), (151784, '<|extra_138|>'), (151785, '<|extra_139|>'), (151786, '<|extra_140|>'), (151787, '<|extra_141|>'), (151788, '<|extra_142|>'), (151789, '<|extra_143|>'), (151790, '<|extra_144|>'), (151791, '<|extra_145|>'), (151792, '<|extra_146|>'), (151793, '<|extra_147|>'), (151794, '<|extra_148|>'), (151795, '<|extra_149|>'), (151796, '<|extra_150|>'), (151797, '<|extra_151|>'), (151798, '<|extra_152|>'), (151799, '<|extra_153|>'), (151800, '<|extra_154|>'), (151801, '<|extra_155|>'), (151802, '<|extra_156|>'), (151803, '<|extra_157|>'), (151804, '<|extra_158|>'), (151805, '<|extra_159|>'), (151806, '<|extra_160|>'), (151807, '<|extra_161|>'), (151808, '<|extra_162|>'), (151809, '<|extra_163|>'), (151810, '<|extra_164|>'), (151811, '<|extra_165|>'), (151812, '<|extra_166|>'), (151813, '<|extra_167|>'), (151814, '<|extra_168|>'), (151815, '<|extra_169|>'), (151816, '<|extra_170|>'), (151817, '<|extra_171|>'), (151818, '<|extra_172|>'), (151819, '<|extra_173|>'), (151820, '<|extra_174|>'), (151821, '<|extra_175|>'), (151822, '<|extra_176|>'), (151823, '<|extra_177|>'), (151824, '<|extra_178|>'), (151825, '<|extra_179|>'), (151826, '<|extra_180|>'), (151827, '<|extra_181|>'), (151828, '<|extra_182|>'), (151829, '<|extra_183|>'), (151830, '<|extra_184|>'), (151831, '<|extra_185|>'), (151832, '<|extra_186|>'), (151833, '<|extra_187|>'), (151834, '<|extra_188|>'), (151835, '<|extra_189|>'), (151836, '<|extra_190|>'), (151837, '<|extra_191|>'), (151838, '<|extra_192|>'), (151839, '<|extra_193|>'), (151840, '<|extra_194|>'), (151841, '<|extra_195|>'), (151842, '<|extra_196|>'), (151843, '<|extra_197|>'), (151844, '<|extra_198|>'), (151845, '<|extra_199|>'), (151846, '<|extra_200|>'), (151847, '<|extra_201|>'), (151848, '<|extra_202|>'), (151849, '<|extra_203|>'), (151850, '<|extra_204|>'), (151851, '<repo_name>'), (151852, '<file_sep>'), (151853, '<translation_type>'), (151854, '<lang_zh>'), (151855, '<lang_zh-hant>'), (151856, '<lang_en>'), (151857, '<lang_ja>'), (151858, '<lang_ko>'), (151859, '<lang_pt>'), (151860, '<lang_es>'), (151861, '<lang_fr>'), (151862, '<lang_de>'), (151863, '<lang_ru>'), (151864, '<lang_th>'), (151865, '<lang_vi>'), (151866, '<lang_id>'), (151867, '<lang_ar>'), (151868, '<lang_it>'), (151869, '<lang_tr>'), (151870, '<lang_hi>'))\n",
      "Updating keys from env and command line: []\n",
      "Running Model: default\n",
      "Updating keys from model: []\n",
      "Attempting to initialize the jax distributed system...\n",
      "Jax distributed system initialized!\n",
      "System Information: Jax Version: 0.4.30\n",
      "System Information: Jaxlib Version: 0.4.30\n",
      "System Information: Jax Backend: PJRT C API\n",
      "TFRT TPU v3\n",
      "Built on Jun 17 2024 03:03:47 (1718618627) cl/643897370\n",
      "Not using emergency checkpoint, ignoring local_checkpoint_directory and local_checkpoint_period\n",
      "Config param adam_b1: 0.9\n",
      "Config param adam_b2: 0.95\n",
      "Config param adam_eps: 1e-08\n",
      "Config param adam_eps_root: 0.0\n",
      "Config param adam_weight_decay: 0.1\n",
      "Config param allow_split_physical_axes: False\n",
      "Config param ar_cache_axis_order: 1,2,0,3\n",
      "Config param async_checkpointing: True\n",
      "Config param attention: dot_product\n",
      "Config param autoregressive_decode_assert: \n",
      "Config param base_emb_dim: 4096\n",
      "Config param base_mlp_dim: 5632\n",
      "Config param base_num_decoder_layers: 48\n",
      "Config param base_num_kv_heads: 32\n",
      "Config param base_num_query_heads: 32\n",
      "Config param base_output_directory: \n",
      "Config param checkpoint_period: 50\n",
      "Config param collect_stack_trace: False\n",
      "Config param compile_topology: \n",
      "Config param compile_topology_num_slices: -1\n",
      "Config param compiled_trainstep_file: \n",
      "Config param compute_axis_order: 0,1,2,3\n",
      "Config param cosine_learning_rate_final_fraction: 0.1\n",
      "Config param data_sharding: (('data', 'stage', 'fsdp', 'fsdp_transpose', 'sequence', 'tensor', 'autoregressive'),)\n",
      "Config param data_shuffle_seed: 9876\n",
      "Config param dataset_path: \n",
      "Config param dataset_type: novel\n",
      "Config param dcn_autoregressive_parallelism: 1\n",
      "Config param dcn_data_parallelism: -1\n",
      "Config param dcn_fsdp_parallelism: 1\n",
      "Config param dcn_fsdp_transpose_parallelism: 1\n",
      "Config param dcn_pipeline_parallelism: 1\n",
      "Config param dcn_sequence_parallelism: 1\n",
      "Config param dcn_tensor_parallelism: 1\n",
      "Config param decode_sampling_nucleus_p: -1\n",
      "Config param decode_sampling_strategy: greedy\n",
      "Config param decode_sampling_temperature: 1.0\n",
      "Config param decode_sampling_top_k: 0\n",
      "Config param decoder_block: dcformer\n",
      "Config param dropout_rate: 0\n",
      "Config param dtype: bfloat16\n",
      "Config param emb_dim: 4096\n",
      "Config param enable_background_delete: True\n",
      "Config param enable_checkpoint_cloud_logger: False\n",
      "Config param enable_checkpoint_standard_logger: False\n",
      "Config param enable_checkpointing: True\n",
      "Config param enable_data_shuffling: False\n",
      "Config param enable_dropout: False\n",
      "Config param enable_emergency_checkpoint: False\n",
      "Config param enable_goodput_recording: False\n",
      "Config param enable_jax_profiler: False\n",
      "Config param enable_single_controller: False\n",
      "Config param enable_single_replica_ckpt_restoring: False\n",
      "Config param epoch: 1\n",
      "Config param eval_interval: 500\n",
      "Config param eval_loop_num_batches: 66\n",
      "Config param eval_per_device_batch_size: 1.0\n",
      "Config param eval_shuffle_buffer_size: 2000\n",
      "Config param eval_split: val_with_eos\n",
      "Config param expansion_factor_real_data: -1\n",
      "Config param force_unroll: False\n",
      "Config param fused_mlp: False\n",
      "Config param fused_qkv: False\n",
      "Config param gcs_metrics: False\n",
      "Config param global_batch_size_to_load: 8\n",
      "Config param global_batch_size_to_train_on: 8\n",
      "Config param global_parameter_scale: 1\n",
      "Config param gradient_clipping_threshold: 1.0\n",
      "Config param grain_worker_count: 4\n",
      "Config param hardware: tpu\n",
      "Config param head_dim: 128\n",
      "Config param ici_autoregressive_parallelism: 1\n",
      "Config param ici_data_parallelism: 1\n",
      "Config param ici_fsdp_parallelism: -1\n",
      "Config param ici_fsdp_transpose_parallelism: 1\n",
      "Config param ici_pipeline_parallelism: 1\n",
      "Config param ici_sequence_parallelism: 1\n",
      "Config param ici_tensor_parallelism: 1\n",
      "Config param inference_metadata_file: \n",
      "Config param inference_microbenchmark_log_file_path: \n",
      "Config param inference_microbenchmark_loop_iters: 10\n",
      "Config param inference_microbenchmark_prefill_lengths: 64,128,256,512,1024\n",
      "Config param inference_microbenchmark_stages: prefill,generate\n",
      "Config param init_weights_seed: 9876\n",
      "Config param iter_file_nums: 20\n",
      "Config param jax_cache_dir: ~/jax_cache\n",
      "Config param jax_profiler_port: 9999\n",
      "Config param keep_period: 500\n",
      "Config param key_wise: True\n",
      "Config param kv_quant_axis: heads_and_dkv\n",
      "Config param learning_rate: 0.0003\n",
      "Config param learning_rate_schedule_steps: 440000\n",
      "Config param load_from_prefill_dir: False\n",
      "Config param load_full_state_path: \n",
      "Config param load_parameters_path: \n",
      "Config param log_period: 10\n",
      "Config param logical_axis_rules: (('activation_batch', ('data', 'fsdp', 'fsdp_transpose')), ('activation_embed_and_logits_batch', ('stage', 'data', 'fsdp', 'fsdp_transpose')), ('activation_heads', ('tensor', 'sequence')), ('activation_kv_heads', ('tensor', 'sequence')), ('activation_length', 'sequence'), ('activation_embed', 'tensor'), ('activation_mlp', 'tensor'), ('activation_kv', 'tensor'), ('activation_kv_batch', ('data', 'fsdp', 'fsdp_transpose')), ('activation_kv_head_dim', 'tensor'), ('activation_vocab', ('tensor', 'sequence')), ('activation_vocab', 'tensor'), ('activation_vocab', 'sequence'), ('activation_stage', 'stage'), ('mlp', ('fsdp_transpose', 'tensor', 'autoregressive')), ('vocab', ('tensor', 'autoregressive')), ('embed', ('fsdp', 'fsdp_transpose', 'sequence')), ('embed', ('fsdp', 'sequence')), ('norm', 'tensor'), ('heads', ('tensor', 'autoregressive')), ('layers', 'stage'), ('kv', ()), ('kv_heads', ('tensor', 'autoregressive')), ('kv_head_dim', ()), ('cache_batch', ()), ('cache_heads', ('autoregressive', 'tensor')), ('cache_kv', ()), ('cache_sequence', ()))\n",
      "Config param logits_dot_in_fp32: False\n",
      "Config param logits_via_embedding: False\n",
      "Config param max_checkify: False\n",
      "Config param max_corpus_chars: 10000000\n",
      "Config param max_prefill_predict_length: 64\n",
      "Config param max_target_length: 32769\n",
      "Config param max_to_keep: 3\n",
      "Config param mesh_axes: ['data', 'stage', 'fsdp', 'fsdp_transpose', 'sequence', 'tensor', 'autoregressive']\n",
      "Config param metrics_file: \n",
      "Config param mgate: True\n",
      "Config param mgate_dim: 44\n",
      "Config param mlp_activations: ['silu', 'linear']\n",
      "Config param mlp_dim: 5632\n",
      "Config param model_name: default\n",
      "Config param normalization_layer_epsilon: 1e-05\n",
      "Config param normalize_embedding_logits: True\n",
      "Config param num_decoder_layers: 48\n",
      "Config param num_experts: 1\n",
      "Config param num_experts_per_tok: 1\n",
      "Config param num_kv_heads: 32\n",
      "Config param num_layers_per_block: 4\n",
      "Config param num_query_heads: 32\n",
      "Config param num_slices: 1\n",
      "Config param only_eval: False\n",
      "Config param opt_type: adam_pax\n",
      "Config param param_scan_axis: 1\n",
      "Config param per_device_batch_size: 1.0\n",
      "Config param post_compose: True\n",
      "Config param pre_compose: True\n",
      "Config param prefill_cache_axis_order: 1,2,0,3\n",
      "Config param prefill_cache_dir: \n",
      "Config param profiler: \n",
      "Config param profiler_steps: 5\n",
      "Config param prometheus_port: 0\n",
      "Config param prompt: I love to\n",
      "Config param qk_norm: True\n",
      "Config param quantization: \n",
      "Config param quantization_local_shard_count: 1\n",
      "Config param quantize_kvcache: False\n",
      "Config param query_chunk_size: 512\n",
      "Config param query_wise: True\n",
      "Config param record_internal_nn_metrics: 1\n",
      "Config param remat_policy: full\n",
      "Config param reset_for_eval: True\n",
      "Config param reshape_q: False\n",
      "Config param reuse_example_batch: 0\n",
      "Config param rope_max_timescale: 500000\n",
      "Config param rope_min_timescale: 1\n",
      "Config param run_name: None\n",
      "Config param save_config_to_gcs: False\n",
      "Config param scan_layers: True\n",
      "Config param set_mask_by_eos: True\n",
      "Config param skip_first_n_steps_for_profiler: 1\n",
      "Config param stack_trace_interval_seconds: 600\n",
      "Config param stack_trace_to_cloud: False\n",
      "Config param static_proj: False\n",
      "Config param steps: 10000000\n",
      "Config param target_eval_loss: 0.0\n",
      "Config param tokenizer_path: assets/tokenizer.llama2\n",
      "Config param train_shuffle_buffer_size: 200000\n",
      "Config param trainable_position_size: -1\n",
      "Config param training_num_batches_to_skip: None\n",
      "Config param upload_all_profiler_results: False\n",
      "Config param use_iota_embed: False\n",
      "Config param use_untrainable_positional_embedding: False\n",
      "Config param use_vertex_tensorboard: False\n",
      "Config param using_pipeline_parallelism: False\n",
      "Config param vertex_tensorboard_project: \n",
      "Config param vertex_tensorboard_region: \n",
      "Config param vocab_size: 152064\n",
      "Config param warmup_steps_fraction: 0.005\n",
      "Config param weight_dtype: float32\n",
      "Config param window_size: [256, 32768, 256, 256]\n",
      "Config param zero_loss: True\n",
      "Mesh: [[[[[[[TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)]]]]\n",
      "\n",
      "\n",
      "\n",
      "   [[[[TpuDevice(id=1, process_index=0, coords=(0,0,0), core_on_chip=1)]]]]\n",
      "\n",
      "\n",
      "\n",
      "   [[[[TpuDevice(id=2, process_index=0, coords=(1,0,0), core_on_chip=0)]]]]\n",
      "\n",
      "\n",
      "\n",
      "   [[[[TpuDevice(id=3, process_index=0, coords=(1,0,0), core_on_chip=1)]]]]\n",
      "\n",
      "\n",
      "\n",
      "   [[[[TpuDevice(id=4, process_index=0, coords=(0,1,0), core_on_chip=0)]]]]\n",
      "\n",
      "\n",
      "\n",
      "   [[[[TpuDevice(id=5, process_index=0, coords=(0,1,0), core_on_chip=1)]]]]\n",
      "\n",
      "\n",
      "\n",
      "   [[[[TpuDevice(id=6, process_index=0, coords=(1,1,0), core_on_chip=0)]]]]\n",
      "\n",
      "\n",
      "\n",
      "   [[[[TpuDevice(id=7, process_index=0, coords=(1,1,0), core_on_chip=1)]]]]]]] Num_devices: 8, shape (1, 1, 8, 1, 1, 1, 1) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Configured `CheckpointManager` using deprecated legacy API. Please follow the instructions at https://orbax.readthedocs.io/en/latest/api_refactor.html to migrate by August 1st, 2024.\n",
      "WARNING:absl:CheckpointMetadata file does not exist: gs://llm_base_models_us-east5/v5p_256/7B/PileDCSlimLlama7B32Kx4x256x1v5p_0705/checkpoints/441700/_CHECKPOINT_METADATA\n",
      "WARNING:absl:CheckpointMetadata file does not exist: gs://llm_base_models_us-east5/v5p_256/7B/PileDCSlimLlama7B32Kx4x256x1v5p_0705/checkpoints/441804/_CHECKPOINT_METADATA\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import yaml\n",
    "import json\n",
    "import base64\n",
    "from collections import defaultdict\n",
    "from typing import Tuple\n",
    "import functools\n",
    "\n",
    "sys.path.append('/home/lishengping/projects/maxtext/MaxText')\n",
    "os.environ['HARDWARE'] = 'tpu'\n",
    "\n",
    "from layers import models\n",
    "import max_utils\n",
    "import jax\n",
    "import orbax\n",
    "import jax.numpy as jnp\n",
    "from jax.sharding import Mesh\n",
    "from flax.traverse_util import flatten_dict, unflatten_dict\n",
    "from flax import linen as nn\n",
    "from transformers import AutoTokenizer\n",
    "from etils import epath\n",
    "\n",
    "import pyconfig\n",
    "from jax.sharding import PartitionSpec\n",
    "from flax.linen import partitioning as nn_partitioning\n",
    "\n",
    "\n",
    "TOKENIZER_PATH = '/home/lishengping/tokenizer'\n",
    "if not os.path.exists(TOKENIZER_PATH):\n",
    "    !gsutil cp -r gs://llm_base_models_us-east5/qwen/tokenizer /home/lishengping/\n",
    "tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_PATH, use_fast=True, trust_remote_code=True)\n",
    "\n",
    "read_dir = \"gs://llm_base_models/maxtext_align_pax_dc/maxtext_align2/checkpoints\"\n",
    "read_dir = \"gs://llm_base_models_us-east5/v5p_256/7B/PileDCSlimLlama7B32Kx4x256x1v5p_0716_test/checkpoints\"\n",
    "read_dir = \"gs://llm_base_models_us-east5/v5p_256/7B/PileDCSlimLlama7B32Kx4x256x1v5p_0705/checkpoints\"\n",
    "read_dir = epath.Path(read_dir)\n",
    "\n",
    "# config_name = '/home/lishengping/projects/maxtext/MaxText/configs/dcformer_pp_405m.yml'\n",
    "config_name = '/home/lishengping/projects/maxtext/MaxText/configs/dc_7b.yml'\n",
    "\n",
    "argv = [None, config_name]\n",
    "pyconfig.initialize(argv)\n",
    "config = pyconfig.config\n",
    "# validate_train_config(config)\n",
    "devices_array = max_utils.create_device_mesh(config)\n",
    "mesh = Mesh(devices_array, config.mesh_axes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda8d893-751a-40f8-9283-83be95d2bcd0",
   "metadata": {},
   "source": [
    "## 构建sharding和metadata信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "caee3b60-775c-4ead-b420-56b1622f8b29",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def decode_base64(encoded_str):\n",
    "    decoded_bytes = base64.b64decode(encoded_str)\n",
    "    decoded_str = decoded_bytes.decode('utf-8')\n",
    "    return decoded_str\n",
    "\n",
    "\n",
    "def mesh_shard_rules(mesh, rules, remove_keys=[]):\n",
    "    _sharding_dict = {}\n",
    "    for name, rule in rules.items():\n",
    "        if isinstance(rule, str):\n",
    "            rule = json.loads(rule)\n",
    "        name = decode_base64(name)\n",
    "        param_key = tuple(name.split('.'))\n",
    "        remove = any([1 if key in param_key else 0 for key in remove_keys])\n",
    "        if remove: continue\n",
    "        prule = [tuple(r) if isinstance(r, list) else r for r in rule['partition_spec'] ]\n",
    "        spec = jax.sharding.PartitionSpec(*prule)\n",
    "        _sharding_dict[param_key] = jax.sharding.NamedSharding(mesh, spec)\n",
    "    return _sharding_dict\n",
    "\n",
    "\n",
    "def rewrite_bucket_sharding(mesh, old_sharding, save_path):\n",
    "    cur_machine_sharding = {}\n",
    "    for k, v in old_sharding.items():\n",
    "        if isinstance(v, str):\n",
    "            v = json.loads(v)\n",
    "        v['shape'] = mesh.device_ids.shape\n",
    "        cur_machine_sharding[k] = v\n",
    "    save_path = epath.Path(save_path)\n",
    "    with save_path.open('w') as f:\n",
    "        json.dump(cur_machine_sharding, f)\n",
    "    \n",
    "load_step = 440000\n",
    "_sharding_path = read_dir / str(load_step) / 'state/_sharding'\n",
    "_metadata_path = read_dir / str(load_step) / 'state/_METADATA'\n",
    "\n",
    "# delete file or dir\n",
    "# _sharding_path.unlink()\n",
    "\n",
    "remove_keys = ['opt_state', 'step']\n",
    "if _sharding_path.exists():\n",
    "    with _sharding_path.open('r') as f:\n",
    "        _sharding_rules = json.load(f)\n",
    "    # 重写_sharding文件\n",
    "    rewrite_bucket_sharding(mesh, _sharding_rules, _sharding_path)\n",
    "    _sharding_dict = mesh_shard_rules(mesh, _sharding_rules, remove_keys=remove_keys)\n",
    "    _sharding_dict = unflatten_dict(_sharding_dict)\n",
    "elif _metadata_path.exists():\n",
    "    _metadata_dict = {}\n",
    "    with _metadata_path.open('r') as f:\n",
    "        _metadata = json.load(f)\n",
    "    for param_key in _metadata['tree_metadata']:\n",
    "        if isinstance(param_key, str): param_key = eval(param_key)\n",
    "        remove = any([1 if key in param_key else 0 for key in remove_keys])\n",
    "        if remove: continue\n",
    "        _metadata_dict[param_key] = jax.ShapeDtypeStruct(shape=(), dtype=jnp.float32)\n",
    "    _metadata_dict = unflatten_dict(_metadata_dict)\n",
    "    \n",
    "else:\n",
    "    _sharding_dict = None\n",
    "    _metadata_dict = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74a8f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # !pip install tensorflow==2.16.1\n",
    "# # pip install numpy==1.26.4\n",
    "# # 运行2遍\n",
    "# import json\n",
    "# import os\n",
    "# import sys\n",
    "# import asyncio\n",
    "# import argparse\n",
    "# from collections import defaultdict\n",
    "# import time\n",
    "\n",
    "# # os.environ[\"JAX_PLATFORMS\"] = \"cpu\"\n",
    "# from etils import epath\n",
    "# import json\n",
    "# import base64\n",
    "\n",
    "# import torch\n",
    "# import numpy as np\n",
    "# import jax.numpy as jnp\n",
    "# import jax\n",
    "# import orbax\n",
    "# import orbax.checkpoint as ocp\n",
    "# from etils import epath\n",
    "# from jax.sharding import PartitionSpec as PS\n",
    "# from flax.traverse_util import flatten_dict, unflatten_dict\n",
    "\n",
    "\n",
    "# METADATA_FILE = '_METADATA'\n",
    "# _CHECKPOINT_FILE = 'checkpoint'\n",
    "\n",
    "# read_dir = \"gs://llm_base_models_europe-west4/v5p_256/7B/xm_M8x7B_E8_UnshareWithMgate_ShareWithMlp_AllCopymlp_1201/checkpoints\"\n",
    "# read_dir = epath.Path(read_dir) \n",
    "# read_dir = read_dir / '0/state'\n",
    "\n",
    "# metadata_path = read_dir / METADATA_FILE\n",
    "# back_metadata_path = read_dir / f'{METADATA_FILE}.back'\n",
    "# try:\n",
    "#     metadata_path.rename(back_metadata_path)\n",
    "# except:\n",
    "#     pass\n",
    "# metadata_path.unlink(missing_ok=True) # delete\n",
    "# structure_path = read_dir / _CHECKPOINT_FILE\n",
    "# msgpack = ocp.aggregate_handlers.MsgpackHandler(0)\n",
    "# structure = msgpack.deserialize(structure_path)\n",
    "# # backup original checkpoint fil\n",
    "# back_structure_path = read_dir / 'checkpoint_back'\n",
    "# back_structure = structure.copy()\n",
    "# if not back_structure_path.exists():\n",
    "#     asyncio.run(msgpack.serialize(back_structure_path, item=back_structure))\n",
    "# print(f'Old structure file keys: {structure.keys()}')\n",
    "# remove_keys = ['opt_state', 'step'] # select the weight name you don't want to load, all weight name: opt_state, step, params\n",
    "# _ = [structure.pop(key) for key in remove_keys if key in structure]\n",
    "# print(f'New structure file keys: {structure.keys()}')\n",
    "# asyncio.run(msgpack.serialize(structure_path, item=structure))  # rewrite struct file\n",
    "\n",
    "# # load model based struct, note: axes must same as training\n",
    "# mesh_axes = ['data', 'stage', 'fsdp', 'fsdp_transpose', 'sequence', 'tensor', 'expert', 'autoregressive']\n",
    "# axes = [1] * len(mesh_axes)\n",
    "# axes[2] = 4\n",
    "# devices = np.asarray(jax.devices()).reshape(axes)\n",
    "# mesh = jax.sharding.Mesh(devices, mesh_axes)\n",
    "# sharding = jax.sharding.NamedSharding(mesh, PS()) # Sharding is None because we use cpu to load weights\n",
    "# weight_dtype = jnp.bfloat16 # set restore weights dtype\n",
    "# restore_args = {}\n",
    "# for k, v in flatten_dict(structure).items():\n",
    "#     restore_args[k] =  ocp.ArrayRestoreArgs(restore_type=jax.Array, dtype=weight_dtype, sharding=sharding)\n",
    "# restore_args = unflatten_dict(restore_args)\n",
    "\n",
    "# # 新加的\n",
    "# right_shard = flatten_dict(_sharding_dict)\n",
    "# new_restore_args = {}\n",
    "# for k, v in flatten_dict(restore_args).items():\n",
    "#     print(k, v)\n",
    "#     v.sharding = right_shard[k]\n",
    "#     new_restore_args[k] = v\n",
    "# restore_args = unflatten_dict(new_restore_args)\n",
    "\n",
    "\n",
    "# ckptr = ocp.Checkpointer(ocp.PyTreeCheckpointHandler())\n",
    "# w = ckptr.restore(read_dir, args=ocp.args.PyTreeRestore(restore_args=restore_args))\n",
    "# structure_path = read_dir / _CHECKPOINT_FILE\n",
    "# # rewrite struct file, otherwise occur error when continue training\n",
    "# asyncio.run(msgpack.serialize(structure_path, item=back_structure))\n",
    "# while 'params' in w:\n",
    "#     w = w['params']\n",
    "# # xm3p5_w = {'.'.join(k): np.array(v) for k, v in flatten_dict(w).items()}\n",
    "\n",
    "# # try:\n",
    "# #     back_metadata_path.rename(metadata_path)\n",
    "# # except:\n",
    "# #     pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "422a18c3-614f-4b24-8c64-46ead9ea44a5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1721296672.545915   40440 gcs_resource.cc:109] Using default AdmissionQueue with limit 32\n",
      "I0000 00:00:1721296672.550512   45282 google_auth_provider.cc:180] Running on GCE, using service account 887571727717-compute@developer.gserviceaccount.com\n",
      "tcmalloc: large alloc 1107296256 bytes == 0x97b4e000 @  0x7fd89e187680 0x7fd89e1a7ff4 0x7fd87770e500 0x7fd87770e592 0x7fd8764d0c20 0x7fd876d4e1ac 0x7fd876d51051 0x7fd877137623 0x7fd877139baf 0x7fd87790a1d0 0x7fd89e14d609 0x7fd89df16133\n",
      "tcmalloc: large alloc 2491416576 bytes == 0xb43ac4000 @  0x7fd89e187680 0x7fd89e1a7ff4 0x7fd87770e500 0x7fd87770e592 0x7fd8764d0c20 0x7fd876d4e1ac 0x7fd876d51051 0x7fd877137623 0x7fd877139baf 0x7fd87790a1d0 0x7fd89e14d609 0x7fd89df16133\n",
      "tcmalloc: large alloc 2491416576 bytes == 0xcdbd5e000 @  0x7fd89e187680 0x7fd89e1a7ff4 0x7fd87770e500 0x7fd87770e592 0x7fd8764d0c20 0x7fd876d4e1ac 0x7fd876d51051 0x7fd877137623 0x7fd877139baf 0x7fd87790a1d0 0x7fd89e14d609 0x7fd89df16133\n",
      "tcmalloc: large alloc 2491416576 bytes == 0x12ac0d2000 @  0x7fd89e187680 0x7fd89e1a7ff4 0x7fd87770e500 0x7fd87770e592 0x7fd8764d0c20 0x7fd876d4e1ac 0x7fd876d51051 0x7fd877137623 0x7fd877139baf 0x7fd87790a1d0 0x7fd89e14d609 0x7fd89df16133\n",
      "tcmalloc: large alloc 2491416576 bytes == 0x13a111e000 @  0x7fd89e187680 0x7fd89e1a7ff4 0x7fd87770e500 0x7fd87770e592 0x7fd8764d0c20 0x7fd876d4e1ac 0x7fd876d51051 0x7fd877137623 0x7fd877139baf 0x7fd87790a1d0 0x7fd89e14d609 0x7fd89df16133\n",
      "tcmalloc: large alloc 2491416576 bytes == 0x1537b46000 @  0x7fd89e187680 0x7fd89e1a7ff4 0x7fd87770e500 0x7fd87770e592 0x7fd8764d0c20 0x7fd876d4e1ac 0x7fd876d51051 0x7fd877137623 0x7fd877139baf 0x7fd87790a1d0 0x7fd89e14d609 0x7fd89df16133\n",
      "tcmalloc: large alloc 2491416576 bytes == 0x15ccc1e000 @  0x7fd89e187680 0x7fd89e1a7ff4 0x7fd87770e500 0x7fd87770e592 0x7fd8764d0c20 0x7fd876d4e1ac 0x7fd876d51051 0x7fd877137623 0x7fd877139baf 0x7fd87790a1d0 0x7fd89e14d609 0x7fd89df16133\n"
     ]
    }
   ],
   "source": [
    "# 如果不行就用上面的\n",
    "options = orbax.checkpoint.CheckpointManagerOptions()\n",
    "item = {\n",
    "    \"state\": orbax.checkpoint.Checkpointer(orbax.checkpoint.PyTreeCheckpointHandler(use_ocdbt=False))\n",
    "}\n",
    "max_mngr = orbax.checkpoint.CheckpointManager(read_dir, item, options)\n",
    "load_step = 441804\n",
    "if _sharding_dict is not None:\n",
    "    state = max_mngr.restore(load_step, items={\"state\": _sharding_dict})\n",
    "elif _metadata_dict is not None:\n",
    "    state = max_mngr.restore(load_step, items={\"state\": _metadata_dict})\n",
    "else:\n",
    "    state = max_mngr.restore(load_step, items=item)\n",
    "params = state['state']['params']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc55071-09fc-4e24-9eac-1720689ff9bf",
   "metadata": {},
   "source": [
    "## 模型初始化和参数shard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "67371963-b074-44a5-a92b-761941f26ed3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('params', 'decoder', 'decoder_norm', 'scale') (4096,)\n",
      "('params', 'decoder', 'layers', 'mlp_0', 'mgate', 'kernel') (4096, 12, 44)\n",
      "('params', 'decoder', 'layers', 'mlp_0', 'wi_0', 'kernel') (4096, 12, 5632)\n",
      "('params', 'decoder', 'layers', 'mlp_0', 'wi_1', 'kernel') (4096, 12, 5632)\n",
      "('params', 'decoder', 'layers', 'mlp_0', 'wo', 'kernel') (5632, 12, 4096)\n",
      "('params', 'decoder', 'layers', 'mlp_1', 'mgate', 'kernel') (4096, 12, 44)\n",
      "('params', 'decoder', 'layers', 'mlp_1', 'wi_0', 'kernel') (4096, 12, 5632)\n",
      "('params', 'decoder', 'layers', 'mlp_1', 'wi_1', 'kernel') (4096, 12, 5632)\n",
      "('params', 'decoder', 'layers', 'mlp_1', 'wo', 'kernel') (5632, 12, 4096)\n",
      "('params', 'decoder', 'layers', 'mlp_2', 'mgate', 'kernel') (4096, 12, 44)\n",
      "('params', 'decoder', 'layers', 'mlp_2', 'wi_0', 'kernel') (4096, 12, 5632)\n",
      "('params', 'decoder', 'layers', 'mlp_2', 'wi_1', 'kernel') (4096, 12, 5632)\n",
      "('params', 'decoder', 'layers', 'mlp_2', 'wo', 'kernel') (5632, 12, 4096)\n",
      "('params', 'decoder', 'layers', 'mlp_3', 'mgate', 'kernel') (4096, 12, 44)\n",
      "('params', 'decoder', 'layers', 'mlp_3', 'wi_0', 'kernel') (4096, 12, 5632)\n",
      "('params', 'decoder', 'layers', 'mlp_3', 'wi_1', 'kernel') (4096, 12, 5632)\n",
      "('params', 'decoder', 'layers', 'mlp_3', 'wo', 'kernel') (5632, 12, 4096)\n",
      "('params', 'decoder', 'layers', 'post_self_attention_layer_norm_0', 'scale') (4096, 12)\n",
      "('params', 'decoder', 'layers', 'post_self_attention_layer_norm_1', 'scale') (4096, 12)\n",
      "('params', 'decoder', 'layers', 'post_self_attention_layer_norm_2', 'scale') (4096, 12)\n",
      "('params', 'decoder', 'layers', 'post_self_attention_layer_norm_3', 'scale') (4096, 12)\n",
      "('params', 'decoder', 'layers', 'pre_self_attention_layer_norm_0', 'scale') (4096, 12)\n",
      "('params', 'decoder', 'layers', 'pre_self_attention_layer_norm_1', 'scale') (4096, 12)\n",
      "('params', 'decoder', 'layers', 'pre_self_attention_layer_norm_2', 'scale') (4096, 12)\n",
      "('params', 'decoder', 'layers', 'pre_self_attention_layer_norm_3', 'scale') (4096, 12)\n",
      "('params', 'decoder', 'layers', 'self_attention_0', 'AttentionOp_0', 'dyn_w_proj', 'dd', 'kernel') (4096, 12, 1, 128)\n",
      "('params', 'decoder', 'layers', 'self_attention_0', 'AttentionOp_0', 'dyn_w_proj', 'dw1', 'kernel') (4096, 12, 1, 4, 128)\n",
      "('params', 'decoder', 'layers', 'self_attention_0', 'AttentionOp_0', 'dyn_w_proj', 'qkw') (1, 12, 4, 128, 4, 32)\n",
      "('params', 'decoder', 'layers', 'self_attention_0', 'k_norm', 'scale') (128, 12)\n",
      "('params', 'decoder', 'layers', 'self_attention_0', 'key', 'kernel') (4096, 12, 32, 128)\n",
      "('params', 'decoder', 'layers', 'self_attention_0', 'out', 'kernel') (32, 12, 128, 4096)\n",
      "('params', 'decoder', 'layers', 'self_attention_0', 'q_norm', 'scale') (128, 12)\n",
      "('params', 'decoder', 'layers', 'self_attention_0', 'query', 'kernel') (4096, 12, 32, 128)\n",
      "('params', 'decoder', 'layers', 'self_attention_0', 'value', 'kernel') (4096, 12, 32, 128)\n",
      "('params', 'decoder', 'layers', 'self_attention_1', 'AttentionOp_0', 'dyn_w_proj', 'dd', 'kernel') (4096, 12, 1, 128)\n",
      "('params', 'decoder', 'layers', 'self_attention_1', 'AttentionOp_0', 'dyn_w_proj', 'dw1', 'kernel') (4096, 12, 1, 4, 128)\n",
      "('params', 'decoder', 'layers', 'self_attention_1', 'AttentionOp_0', 'dyn_w_proj', 'qkw') (1, 12, 4, 128, 4, 32)\n",
      "('params', 'decoder', 'layers', 'self_attention_1', 'k_norm', 'scale') (128, 12)\n",
      "('params', 'decoder', 'layers', 'self_attention_1', 'key', 'kernel') (4096, 12, 32, 128)\n",
      "('params', 'decoder', 'layers', 'self_attention_1', 'out', 'kernel') (32, 12, 128, 4096)\n",
      "('params', 'decoder', 'layers', 'self_attention_1', 'q_norm', 'scale') (128, 12)\n",
      "('params', 'decoder', 'layers', 'self_attention_1', 'query', 'kernel') (4096, 12, 32, 128)\n",
      "('params', 'decoder', 'layers', 'self_attention_1', 'value', 'kernel') (4096, 12, 32, 128)\n",
      "('params', 'decoder', 'layers', 'self_attention_2', 'AttentionOp_0', 'dyn_w_proj', 'dd', 'kernel') (4096, 12, 1, 128)\n",
      "('params', 'decoder', 'layers', 'self_attention_2', 'AttentionOp_0', 'dyn_w_proj', 'dw1', 'kernel') (4096, 12, 1, 4, 128)\n",
      "('params', 'decoder', 'layers', 'self_attention_2', 'AttentionOp_0', 'dyn_w_proj', 'qkw') (1, 12, 4, 128, 4, 32)\n",
      "('params', 'decoder', 'layers', 'self_attention_2', 'k_norm', 'scale') (128, 12)\n",
      "('params', 'decoder', 'layers', 'self_attention_2', 'key', 'kernel') (4096, 12, 32, 128)\n",
      "('params', 'decoder', 'layers', 'self_attention_2', 'out', 'kernel') (32, 12, 128, 4096)\n",
      "('params', 'decoder', 'layers', 'self_attention_2', 'q_norm', 'scale') (128, 12)\n",
      "('params', 'decoder', 'layers', 'self_attention_2', 'query', 'kernel') (4096, 12, 32, 128)\n",
      "('params', 'decoder', 'layers', 'self_attention_2', 'value', 'kernel') (4096, 12, 32, 128)\n",
      "('params', 'decoder', 'layers', 'self_attention_3', 'AttentionOp_0', 'dyn_w_proj', 'dd', 'kernel') (4096, 12, 1, 128)\n",
      "('params', 'decoder', 'layers', 'self_attention_3', 'AttentionOp_0', 'dyn_w_proj', 'dw1', 'kernel') (4096, 12, 1, 4, 128)\n",
      "('params', 'decoder', 'layers', 'self_attention_3', 'AttentionOp_0', 'dyn_w_proj', 'qkw') (1, 12, 4, 128, 4, 32)\n",
      "('params', 'decoder', 'layers', 'self_attention_3', 'k_norm', 'scale') (128, 12)\n",
      "('params', 'decoder', 'layers', 'self_attention_3', 'key', 'kernel') (4096, 12, 32, 128)\n",
      "('params', 'decoder', 'layers', 'self_attention_3', 'out', 'kernel') (32, 12, 128, 4096)\n",
      "('params', 'decoder', 'layers', 'self_attention_3', 'q_norm', 'scale') (128, 12)\n",
      "('params', 'decoder', 'layers', 'self_attention_3', 'query', 'kernel') (4096, 12, 32, 128)\n",
      "('params', 'decoder', 'layers', 'self_attention_3', 'value', 'kernel') (4096, 12, 32, 128)\n",
      "('params', 'decoder', 'logits_dense', 'kernel') (4096, 152064)\n",
      "('params', 'token_embedder', 'embedding') (152064, 4096)\n",
      "devices: {TpuDevice(id=4, process_index=0, coords=(0,1,0), core_on_chip=0), TpuDevice(id=6, process_index=0, coords=(1,1,0), core_on_chip=0), TpuDevice(id=7, process_index=0, coords=(1,1,0), core_on_chip=1), TpuDevice(id=2, process_index=0, coords=(1,0,0), core_on_chip=0), TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0), TpuDevice(id=5, process_index=0, coords=(0,1,0), core_on_chip=1), TpuDevice(id=3, process_index=0, coords=(1,0,0), core_on_chip=1), TpuDevice(id=1, process_index=0, coords=(0,0,0), core_on_chip=1)}\n",
      "Mesh: [[[[[[[TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)]]]]\n",
      "\n",
      "\n",
      "\n",
      "   [[[[TpuDevice(id=1, process_index=0, coords=(0,0,0), core_on_chip=1)]]]]\n",
      "\n",
      "\n",
      "\n",
      "   [[[[TpuDevice(id=2, process_index=0, coords=(1,0,0), core_on_chip=0)]]]]\n",
      "\n",
      "\n",
      "\n",
      "   [[[[TpuDevice(id=3, process_index=0, coords=(1,0,0), core_on_chip=1)]]]]\n",
      "\n",
      "\n",
      "\n",
      "   [[[[TpuDevice(id=4, process_index=0, coords=(0,1,0), core_on_chip=0)]]]]\n",
      "\n",
      "\n",
      "\n",
      "   [[[[TpuDevice(id=5, process_index=0, coords=(0,1,0), core_on_chip=1)]]]]\n",
      "\n",
      "\n",
      "\n",
      "   [[[[TpuDevice(id=6, process_index=0, coords=(1,1,0), core_on_chip=0)]]]]\n",
      "\n",
      "\n",
      "\n",
      "   [[[[TpuDevice(id=7, process_index=0, coords=(1,1,0), core_on_chip=1)]]]]]]] Num_devices: 8, shape (1, 1, 8, 1, 1, 1, 1) \n"
     ]
    }
   ],
   "source": [
    "assert _sharding_dict is not None\n",
    "\n",
    "@functools.partial(jax.jit, in_shardings=None, out_shardings=_sharding_dict['params'])\n",
    "def shard_to_tpu(x):\n",
    "    return x\n",
    "tpu_params = shard_to_tpu(params)\n",
    "flat_params = flatten_dict(tpu_params)\n",
    "for k, v in flat_params.items():\n",
    "    print(k, v.shape)\n",
    "print(f'devices: {v.devices()}')\n",
    "\n",
    "quant = None\n",
    "\n",
    "Transformer = models.Transformer\n",
    "model = Transformer(config, mesh, quant=quant)\n",
    "is_train = False\n",
    "rng1, aqt_rng = jax.random.split(jax.random.key(9876))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29d4ee25-7fb1-44db-a87a-386ae242b655",
   "metadata": {},
   "source": [
    "## 加载数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "c4752069-35e7-43df-968d-3533a6e65e41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train file: 4000,  test file: 1\n",
      "example[name]: Tensor(\"strided_slice:0\", shape=(None,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import argparse\n",
    "import socket\n",
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "import tensorflow as tf\n",
    "import jax\n",
    "import numpy as np\n",
    "\n",
    "import math\n",
    "from typing import Dict, List, Optional\n",
    "\n",
    "from google.cloud import storage\n",
    "\n",
    "\n",
    "seq_len = 4097\n",
    "\n",
    "def extract_v3p5_longdata_files(dataset_path):  # lsp\n",
    "    random.seed(9876)\n",
    "    client = storage.Client()\n",
    "    #v3: us-east1-d -> common_datasets, v4: us-central2-b -> common_datasets_us-central2-b\n",
    "    path = dataset_path.replace('gs://', '')\n",
    "    path_parts = path.split('/')\n",
    "    bucket_name = path_parts[0]\n",
    "    directory_path = '/'.join(path_parts[1:])\n",
    "    directory_path = directory_path if directory_path.endswith('/') else directory_path + '/'\n",
    "    train_files, valid_files = [], []\n",
    "    train_long_files, train_short_files = [], []\n",
    "    for blob in client.list_blobs(bucket_name, prefix=directory_path):\n",
    "        path = f'gs://{os.path.join(bucket_name, blob.name)}'\n",
    "        if 'valid' in path:\n",
    "            valid_files.append(path)\n",
    "        else:\n",
    "            if '.long' in path:\n",
    "                train_long_files.append(path)\n",
    "            else:\n",
    "                train_short_files.append(path)\n",
    "    # file size short：long = 1.5: 1, 为了保证short的token: long = 3: 7, 因此 short 取 (1 / 1.5) * (3 / 7) = 2 / 7\n",
    "    short_k = min(3 * len(train_long_files) // 14, len(train_short_files))\n",
    "    selected_short_files = random.sample(train_short_files, k=short_k)\n",
    "    train_files = selected_short_files + train_long_files\n",
    "    print(f'selected_short_files: {len(selected_short_files)} train_long_files: {len(train_long_files)}')\n",
    "    random.shuffle(train_files)\n",
    "    print(f'first 10 train files: {train_files[:10]}')\n",
    "    valid_files = sorted(valid_files)\n",
    "    print(f'valid_files: {valid_files}')\n",
    "    return train_files, valid_files\n",
    "\n",
    "\n",
    "def extract_v3p5_data_files(dataset_path):\n",
    "    client = storage.Client()\n",
    "    path = dataset_path.replace('gs://', '')\n",
    "    path_parts = path.split('/')\n",
    "    bucket_name = path_parts[0]\n",
    "    directory_path = '/'.join(path_parts[1:])\n",
    "    directory_path = directory_path if directory_path.endswith('/') else directory_path + '/'\n",
    "    # logging.info(f'bucket_name = {bucket_name}, directory_path = {directory_path}')\n",
    "    train_files, valid_files = [], []\n",
    "    for blob in client.list_blobs(bucket_name, prefix=directory_path):\n",
    "        path = f'gs://{os.path.join(bucket_name, blob.name)}'\n",
    "        if 'valid' in path:\n",
    "            valid_files.append(path)\n",
    "        else:\n",
    "            train_files.append(path)\n",
    "    train_files = sorted(train_files)\n",
    "    valid_files = sorted(valid_files)\n",
    "    print(f'Train file: {len(train_files)},  test file: {len(valid_files)}')\n",
    "    return train_files, valid_files\n",
    "    \n",
    "\n",
    "def _parse_function(example_proto):\n",
    "    feature_desc = {key: tf.io.VarLenFeature(tf.int64) for key in task_features}\n",
    "    example = tf.io.parse_single_example(example_proto, feature_desc)\n",
    "    for name in list(example.keys()):\n",
    "        t = example[name]\n",
    "        if t.dtype == tf.int64:\n",
    "            t = tf.cast(t, dtype=tf.int32)\n",
    "        example[name] = tf.sparse.to_dense(t, default_value=0)[: seq_len]\n",
    "        print(f'example[name]: {example[name]}')\n",
    "    return example\n",
    "\n",
    "task_features = {'input_ids': None}\n",
    "train_seed = 1234\n",
    "num_infeed_hosts = 1\n",
    "shuffle_buffer_size = None\n",
    "pad_id = 0\n",
    "batch_size = 8\n",
    "\n",
    "fname = ['gs://jax_llm_data/xiaomeng/sft_target/tfrecord_len2k/en.test.continue_write.tfrecord']\n",
    "datadir = 'gs://jax_llm_data_us-east5/xiaomeng/v3.5/tfids_4k_32k_0622/valid_tfrecord'\n",
    "# train_files, eval_files = extract_v3p5_longdata_files(datadir)\n",
    "\n",
    "datadir = 'gs://jax_llm_data_us-east5/xiaomeng/v3.5/tfids0527'\n",
    "train_files, eval_files = extract_v3p5_data_files(datadir)\n",
    "\n",
    "\n",
    "fname = eval_files\n",
    "\n",
    "# fname = ['gs://jax_llm_data/xiaomeng/sft_target/tfrecord_len2k/en.test.continue_write.tfrecord']\n",
    "tf.random.set_seed(train_seed)\n",
    "ds = tf.data.Dataset.from_tensor_slices(fname)\n",
    "ds = ds.apply(tf.data.TFRecordDataset)\n",
    "# shard host data\n",
    "ds = ds.shard(num_infeed_hosts, 0)\n",
    "ds = ds.map(_parse_function, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "if shuffle_buffer_size is not None:\n",
    "    ds = ds.shuffle(buffer_size=shuffle_buffer_size)\n",
    "padded_shapes = {key: seq_len for key in task_features}\n",
    "padding_values = {key: pad_id for key in task_features}\n",
    "ds = ds.padded_batch(\n",
    "    batch_size=np.prod(batch_size),\n",
    "    padded_shapes=padded_shapes,\n",
    "    padding_values=padding_values,\n",
    "    drop_remainder=True,\n",
    ")\n",
    "# ds = ds.map(self.convert)\n",
    "# ds = ds.prefetch(tf.data.AUTOTUNE)\n",
    "iter_ds = ds.as_numpy_iterator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82339bd9-a86e-4ec2-a5c4-d88d452b5af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_data_sharding(features, shard_names):\n",
    "    shard_names = ('fsdp', None)\n",
    "    data_sharding = {}\n",
    "    for k in features:\n",
    "        spec = jax.sharding.PartitionSpec(*shard_names)\n",
    "        data_sharding[k] = jax.sharding.NamedSharding(mesh, spec)\n",
    "    return data_sharding\n",
    "\n",
    "data_features = ['inputs', 'inputs_position', 'inputs_segmentation', 'targets']\n",
    "data_shard_names = ('data', None)\n",
    "data_sharding = build_data_sharding(data_features, data_shard_names)\n",
    "\n",
    "@functools.partial(jax.jit, in_shardings=(data_sharding, _sharding_dict['params'], ), out_shardings=None)\n",
    "def model_forward(data, params):\n",
    "    logits, intermediate_outputs = model.apply(\n",
    "          params,\n",
    "          data[\"inputs\"],\n",
    "          data[\"inputs_position\"],\n",
    "          decoder_segment_ids=data[\"inputs_segmentation\"],\n",
    "          enable_dropout=config.enable_dropout if is_train else False,\n",
    "          rngs={\"dropout\": rng1, \"params\": aqt_rng},\n",
    "          mutable=\"intermediates\",\n",
    "      )\n",
    "    one_hot_targets = jax.nn.one_hot(data[\"targets\"], config.vocab_size)\n",
    "    xent, _ = max_utils.cross_entropy_with_logits(logits, one_hot_targets, 0.0)\n",
    "    xent = nn.with_logical_constraint(xent, (\"activation_embed_and_logits_batch\", \"activation_length\"))\n",
    "    return xent    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "3f791e55-240b-41bc-8cd5-3a68ec877b2b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids: (8, 4097)\n",
      "loss shape: (8, 4096) mean: 2.4977970123291016\n"
     ]
    }
   ],
   "source": [
    "x = next(iter_ds)\n",
    "input_ids = x['input_ids']\n",
    "print(f'input_ids: {input_ids.shape}')\n",
    "data = {}\n",
    "data['inputs'] = input_ids[:, :-1]\n",
    "pos = jnp.arange(data['inputs'].shape[1]).reshape(1, -1)\n",
    "data[\"inputs_position\"] = jnp.broadcast_to(pos, (batch_size, pos.shape[-1]))\n",
    "data[\"inputs_segmentation\"] = jnp.ones_like(data['inputs'])\n",
    "data[\"targets\"] = input_ids[:, 1:]\n",
    "data = {k: v[:, :] for k, v in data.items()}\n",
    "\n",
    "# loss compute\n",
    "loss = model_forward(data, tpu_params)\n",
    "print(f'loss shape: {loss.shape} mean: {loss.mean()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfeb615c-6f37-461d-9efc-ab64a5794146",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
